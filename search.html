<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Manthan's Blog</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Manthan’s Blog" />
<meta name="author" content="Riccardo Graziosi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Findings, thoughts and observations from the eye of a Software Engineer." />
<meta property="og:description" content="Findings, thoughts and observations from the eye of a Software Engineer." />
<link rel="canonical" href="/www/search.html" />
<meta property="og:url" content="/www/search.html" />
<meta property="og:site_name" content="Manthan’s Blog" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Manthan’s Blog" />
<script type="application/ld+json">
{"description":"Findings, thoughts and observations from the eye of a Software Engineer.","url":"/www/search.html","@type":"WebPage","headline":"Manthan’s Blog","author":{"@type":"Person","name":"Riccardo Graziosi"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="/www/feed.xml" title="Manthan's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/www/logo.png" />
  <link rel="stylesheet" href="/www/assets/css/main.css" />
</head><body a="auto">
    <main class="page-content" aria-label="Content">
      <div class="w"><a href="/www/">..</a><div style="text-align: center;">
  <form action="/search.html" method="get" style="display: inline-block;">
    <!-- <label for="search-box">Search</label> -->
    <input type="text" id="search-box" name="query" placeholder="type...">
    <input type="submit" value="search">
  </form>
</div>
<div id="search-results" style="list-style-type: none;"></div>

<script>
  window.store = {
    
      "2020-10-15-one-pot-potato-curry-recipe": {
        "title": "One Pot Potato Curry Recipe",
        "author": "Manthan Dave",
        "category": "",
        "content": "IntroductionThe Gujarati potato curry is a regular for quite a large number of people living in India. It’s popular because it has a perfect balance of being cheap and easy to prepare and being delicious. It only has three core ingredients: Potatoes, tomatoes and salt. However, the recipe in this post has a little more kick than that.AppliancesPressure cooker (or Instant Pot Duo)You might be able to make this curry without a pressure cooker but I’ve never done it that way so can’t help you there.Please note that the ratios below are scoped and relative to other ingredients/spices within their own section. So 5x salt and 5x potatoes are not the same 😂Core Ingredients1x medium onion3x medium tomatoes5x medium potatoes0.1x cooking oilCore Spices5x Salt1.5x Turmeric1x Fresh ginger / Ginger powder0.5x Asafoeteda1x SugarA note on the core ingredients: The core ingredients need to balance each other. The general ratio for onions/tomatoes/potatoes that I go for is 1:3:5. Obviously you can change this as you like. If you like it to be more tangy, add more tomatoes. If you want it to be richer, add more onions. Although, you can also make it richer by adding half grated onions and half medium chopped onions without adding extra onions.As for the spices, I like a little bit more turmeric, although most people I know only add 1x turmeric. Also, the ratio of salt is entirely subjective. I don’t like my curry to be too salty, but sometimes if I am having it with rice I’ll step it up a little bit since the rice will be absorbing a lot of the flavour. If you do choose to follow that, make sure you don’t add salt to the water when boiling rice (does not apply to steamed rice). Sugar is sort of optional but I always like to add it in a bit. It adds to the richness and balances the salt.Optional Spices3x Fresh/Ready chopped garlic0.5x Chilli powderOptional Ingredients3x Finely chopped green chillis2x Medium PeppersA note on optional spices and ingredients: Garlic brings a lot of flavour to the curry but not everyone likes it. If you really like garlic, crush it and then chop it finely. As for the chilli powder, it does two things: 1) Adds a burst of colour to the curry. 2) Adds that kick. If you’re like me and like the colour more than the kick, add Kashmiri Red Chilli Powder. It’s not spicy at all but adds that iconic red curry colour.Chopped green chillis are great to add a bit of a kick. The kick from green chillis is different than the one that you get from red chilli powder. It’s a sort of kick that comes with a unique fresh chilli smell that you don’t get from red chilli powder. Although, if you don’t like the kick but want the smell, my mum has a trick where she cuts a chilli vertically, removes the core and the seeds and adds them to the curry. Once curry is ready you can take them out. Peppers are for taste and additional texture.MethodWash the potatoes, do not peel themChop onions, tomatoes, potatoes to small-medium piecesAdd oil to the pressure cooker and set to medium heatOnce oil is heated, add the chopped onions - let them simmer for 2-3 minutesAdd chopped tomatoes - let them simmer for 3-4 minutesAdd chopped potatoes - let everything in the pot simmer for 2-3 minutesAdd all the spices, mix it all up and let everything simmer for 3-4 minutesAdd water enough for the potatoes to just about peek above the waterlineClose the pressure cooker and cook for 10 minutes (Instant Pot Duo: High pressure + Medium heat)When the pressure cooker goes off after 10 minutes, leave it off the heat for 5 more minutes. Patience, my friend, is key.Curry is ready!A note on method: Wash the potatoes before chopping if you want slightly richer curry. If you wash after, some of the starch will be washed away and you’ll get a lighter curry. Also, the size of the potato chunks will affect the cooking time. Medium chunks, slightly bigger than a 6 sided die is optimal. Larger pieces will require longer time and smaller pieces will be shorter. Although if its too small it might turn into mush.Peanut oil, if you can find it is the best. Next best is cotton-seed oil. After that is the sunflower. Last is olive oil. Don’t ask, it is what it is and it does make a difference. If you’re worried about cholesterol/health, maybe workout harder that day or walk extra 5000 steps 😊. You can always reduce the amount of oil that you add too.[jetpack_subscription_form show_subscribers_total=\"false\" button_on_newline=\"false\" submit_button_text=\"Subscribe\" custom_font_size=\"16\" custom_border_radius=\"0\" custom_border_weight=\"1\" custom_padding=\"15\" custom_spacing=\"10\" submit_button_classes=\"\" email_field_classes=\"\" show_only_email_and_button=\"true\"]You can change the amount of water you add depending on how thick you want the curry. I like it runny. If you find yours too runny, just burn off excess water after. Having said that, if you’re using a pressure cooker, you need to add enough water for the steam to build up and be under pressure so don’t skimp on it too much! My advice would be to start with the level principle I mentioned, see how you like it and keep reducing it slightly until you find your balance.Also pre-boiling the water will significantly reduce the cooking time. As for the peppers and chillis, I like to add them just before closing the lid of the pressure cooker. This way it keeps all the smell inside as the curry is cooking, enhancing the flavour. If you add it to the oil earlier, it’ll smell great but the curry won’t be at its best.Thank you for taking time to read through my recipe. I didn’t just want to drop in the ingredients and method, I wanted to make sure you know why certain things are the way they are so that you can build on it and make it better.Let me know how you get on!",
        "url": "/2020/10/15/one-pot-potato-curry-recipe/"
      }
      ,
    
      "2020-10-08-on-riding-in-my-first-ever-virtual-group-ride-on-zwift": {
        "title": "On riding in my first ever virtual group ride on Zwift",
        "author": "Manthan Dave",
        "category": "",
        "content": "Read this post on my new blog 👉 https://gears.substack.com/p/on-riding-in-my-first-ever-virtualIntroductionI’ve been Zwifting for about 4 months now. It’s a fantastic platform for indoor bike workouts. All you need is a bike trainer or rather anything that can provide information regarding how much power you are generating on your bike. You can read up all the info on Zwift’s website regarding what you need to get going.I started my subscription on Zwift when I bought my first bike trainer. The story regarding this is a little weird but in the end, I subscribed to a 1-month free trial and then kept my subscription as I was getting quite a lot out of it. And why wouldn’t I? It’s better than staring at the wall while grinding on the pedals.Zwift is a platform for indoor virtual bike rides. It has a social aspect to it too, where you can meet up with friends and ride virtually. You can kit out your bike, buy new bikes, gear etc to make your experience better. I have a pretty standard carbon bike on Zwift but I can always switch to a sturdy steel bike if I wanted to make it more difficult for myself.There is always something going on in Zwift. If you check the events calendar, there is always some group ride or race, or a group workout happening every half an hour or an hour, 24 hours, every single day. It’s astonishing how many people are on it at any given time. It’s always in the thousands! That in itself is pretty motivating. Every single time I open the Zwift app on my laptop, it’ll show me something like 4,000-6,000 people riding in Watopia (Zwift’s virtual world) or some virtual location. I’d just imagine that many people spread across the world, in their shed, garage or just in a living room pushing on the pedals, heart pumping, riding in the virtual world. That’s a lot of power and a lot of sweat!So after having been on the platform for 4 months, I decided to sign up for a group ride. Now the timing of my group ride was pretty bad. On the day before I rode the Road to Sky route. For those of you who don’t know, the highest mountain in Zwift is modelled after Alpe d'Huez and is called Alpe du Zwift. Mountains in cycling are categorised in different categories, from Cat 4 (easiest) to Cat 1 (hardest). This mountain is a HC category climb, HC = Hors catégorie which means beyond categorisation, i.e. its so steep they can’t even categorise it. From a pure numbers perspective, the average gradient is 8.5% for 10.7 miles, although every single time I look up at the screen its always between 9-10%.Needless to say, riding up this mountain, even though its virtual is hard work. My legs would be begging me not to do the ride from the moment I wake up on the day. It’s more mental than the physical effort to do the ride but the physical definitely matters. However, what I did was even stupider than this. Instead of resting for a day after the ride up Alpe du Zwift like I should, I signed up to the Zwift Academy Group Ride on the very next day.On the next day, as if my brain was trying to sabotage this whole thing, I completely forgot I was supposed to be doing a group ride. Although I was smart (or dumb depending on the way you look at it) and I had set an alarm that would go off 30 minutes before the ride. Either way, I was totally unprepared.*buzz buzz* *buzz buzz*As soon as the alarm went off my whole body realised what was about to happen. The plot had been foiled and it was going to be 1 hour of suffering on the bike, definitely worse than the previous day because tiredness and fatigue from the ride up the mountain had already set in.[jetpack_subscription_form show_subscribers_total=\"false\" button_on_newline=\"false\" custom_font_size=\"16\" custom_border_radius=\"0\" custom_border_weight=\"1\" custom_padding=\"15\" custom_spacing=\"10\" submit_button_classes=\"\" email_field_classes=\"\" show_only_email_and_button=\"true\"]I quickly made a cup of tea to get that caffeine boost. I’d have made coffee but that’d have taken about 15-20 minutes to prepare. I prepared my bike setup - fished out the trainer, attached the bike, set up my laptop, connected my headphones and prepared my electrolyte drink. 4 minutes to go. I stepped into the shower to freshen up. When I got out, I had 2 minutes left. Changed into the cycling clothes and got on my bike. 1 minute late.Now it took me a minute to figure out how to join the event from the Zwift home screen. I was more awkward than I anticipated. I thought it’d be a big red button on the home screen that would’ve said “Join Event” but instead I had to do a normal ride and then while loading the “World” Zwift would ask if I wanted to join the event.When I clicked “Join Event” Zwift dropped me straight in the middle of a peloton, in some 430th position out of 493 total riders. I had about 5 seconds to get my cycling shoes on and start pedalling. Tick tock. I quickly slipped into my shoes and without tightening them I jumped on the bike and started pedalling. Nice! On every single upstroke, I’d rotate the lace tightening thing on my shoe to get it tighter. I eventually got it in after 30 or so tries.I felt fine. The group’s pace was 2.3-2.5 w/kg but some of the riders were pushing 3.1 w/kg. So I started to follow them. Big mistake. It was here I realised I hadn’t connected my Garmin. *click* *click* *click*. Ok, its connected and I’m good to go. I started pushing it. 3.1 w/kg. 3.5 w/kg. Really pushing it to 3.7 w/kg. My position in the race kept changing. 410/493. 390/493. 362/493. Keeping this pace was hard so I slowed for a bit. It felt like only a few seconds but a few minutes had passed. I started pushing it again to a maximum 3.5 w/kg. I was flying up those flats at 27-29 miles an hour. I didn’t even know it and I when I checked, I was at 270/493. Woot! I eased off slightly to recover for a bit. I was only about 10 minutes in.It was here that the fatigue from the previous day started to creep in. It wasn’t like hitting a wall, but more like someone stacking bricks on my rear pannier while I was cycling, slowly slowing me down. Suddenly a few guys passed me. I slipped from 270th to 280th. What’s going on? I checked, I was doing 2.3 w/kg. Ugh not good, need to keep pushing. For some reason this time it was hard to keep up 3.5 w/kg so I settled for 3.2 w/kg instead. After passing a few riders, I was at 272th.When I started the race, with the adrenaline rush that I had, I thought I could make it to the top 100. Now, that wasn’t looking so good. Keeping position within 270s was proving to be hard work. My heart rate was at maximum to begin with, no point in checking that. I had no idea what else I could be doing. Looking at my power chart, I wasn’t doing too good there either. I started to see signs where I kept dropping power. I’d do 3.2 w/kg and it’ll slowly taper to 3.0 w/kg. Slowly getting slowed down. But the mind is a powerful tool and I kept pushing.The slowly slowing down got me back to 300th position. I could really feel the fatigue. The torn muscle fibres, tired heart petitioning for me to stop. I was willing to slow down but mentally I was set. I’d rather pass out on this bike than give up this race. Even though it's virtual there are 493 other cyclists riding. If they can do it, I should be able to as well. Sure not all of them rode up the Alpe on the day before but some of them surely did some sort of hard workout. If they can do it, why can’t I?Two-third of the way through the race. I kept bobbing back and forth between 300th and 320th position. I was stuck in a group of riders and I was struggling to break out. I could see the next group ahead but could not conjure up enough energy to break away. I did a few attacks, ride up to 3.1 w/kg, almost get to the next group, and then slowly go down to 2.8-2.9 w/kg because the body couldn’t keep up. Eventually, I found the pace I could rely on. It was 2.7 w/kg. I could sustain this pace for at least until the end of the race. I wasn’t too far. I just had to HODL the watts. 5 minutes to go, I ramped down slightly, just a sliver, hoping it was enough to build some recovery, enough for me to break out at the end. I slipped to 312th/493.After grinding for four and a half minutes of pure suffering, my time had come. 30 seconds to go, maybe too soon? I didn’t care. I attacked. I conjured up the watts with all the energy I had. HR was screaming at 185 bpm. I looked down. My trainer indicated red - I was pushing it hard. I looked up. I was advancing in the peloton. 310th, 309th. 308th. I finally broke away from the group. I saw a rider trying to pass me from the left. Not today. The clock ticked to 0. I finished 308th.ConclusionI know it wasn’t a race, just a group ride so the results don’t really matter. But the position data made me competitive and it mattered to me. I’m sure even if I didn’t have the position data I would’ve found something else to be competitive with. But the best thing about this ride was that if I didn’t have it scheduled, if I wasn’t riding with 493 strangers on the internet, I would’ve done nothing today. I would have watched Netflix, probably done some shopping and found some other topic to write about on this blog. The social aspect of this platform made me go out of my comfort zone to push myself beyond my limits.Now whether this is objectively good for my fitness or not is a different question. Should I have done an intense group ride after riding all effort up a flipping mountain? Probably not. But did I do it? Yes! Do I regret it? Hell no. Now I can order the greasiest burger or the fattest pizza and not feel bad about it at all. Although I’ll never do that, probably will cook something healthier to support my recovery.I hope you enjoyed this post. What did you think? Do you like working out? Have you done something stupid like I did? Let me know in the comments!",
        "url": "/2020/10/08/on-riding-in-my-first-ever-virtual-group-ride-on-zwift/"
      }
      ,
    
      "2020-10-02-on-working-from-home": {
        "title": "On working from home",
        "author": "Manthan Dave",
        "category": "",
        "content": "Read this post on my new blog 👉 https://gears.substack.com/p/on-working-from-homeSo it took a pandemic to force realisation on employers with corporate staff how their workforce could operate remotely. Believe it or not, at one of the companies that I worked for in the past, I did ask whether I could work from home ONE day a week and was denied that opportunity. There were people in my org who were working from home regularly once or twice in a week so I found it weird why I was denied. When I pointed and asked why can they do it? The unofficial answer was that they had kids and lived far away so it was ok for them. Ok, so I can work from home ONE day a week if I got married, had kids and lived in the middle of nowhere and I can’t now because I’m single and I live in the city? Sounds like a bad deal to me 😂Welp, here we are in the middle of a pandemic and now everyone is working from home. I wonder what the situation is like at that company. Are people forced to have kids so that it’s ok for them to work from home? I can only wonder.Anyways, my current company has been pretty generous in allowing for such things. My manager (and the company in general) has a hands-off approach - as long as the work gets done, it doesn’t matter where I do it from. While I absolutely LOVE working in the office, working from home definitely has its perks.🏢❤️ Reasons why I love working from the officeMy company’s office is based in London. It’s a brand spanking new office on the 16th floor, managed by some of the best people I know. Incidentally, I live in London too, and not in the middle of nowhere. Yes, it is expensive to live in London but we’ll get to how I manage that in another post.☕ The coffee machineThe coffee machine in my office is out of this world. I’ve been told its very expensive, costs several arms and legs but it is really worth the expense. It makes all the usual drinks, plus the best hot chocolate ever. However, just when you think its pretty good, it amazes you with its Bluetooth capabilities. It comes with an app that lets you create a custom drink. You can choose the level of foam, the intensity of the coffee shot, amount of milk etc. It also has a steamer that you can use to steam your mug before getting coffee in it so that it doesn’t cool down the coffee too much.But this is not the only reason why I love the coffee machine. Its the conversations and the random collisions that happen around this wonderful machine. I work in Software Engineering, as a Software Engineer so I’m quite further away from the end-user of the product that I develop. But when I am at the coffee machine, I get a chance to meet some of the brightest minds who do work directly with the end-user of my product. They tell me things that I wouldn’t even know to ask. Things like what the customers are doing outside of what the product offers. Where the industry is going. What is going on in the customer success org. These are invaluable insights.In addition to this, I am able to offer what it is that I am doing, where the Software Engineering org is going. Where the general tech is going. What is possible, what is not possible to do in the current product. How easy it is to implement something - this is surprisingly important because sometimes non-technical people think something is really hard to implement in the software but a lot of times its not, its just a simple code change that can unlock whole host of capabilities for our customers!🥪 The lunchesI live on my own, and no this is not going to be a sad story with a slow violin 🎻 playing in the background while I harp about how difficult it is to live on your own in a big city. Actually it’s pretty great but again we’ll cover that in another post.I love cooking because it gives me a good break from working, and when I think really hard about it, subconsciously it gives me a chance to be the computer, to simply follow instructions from a recipe book or memory than having to think too much about it. That aside, I also love to eat! For all the high-intensity workouts that I do, at a subconscious level, I do them so that I can enjoy food afterwards. Working at an office in Central London means that there are always some cool lunch spots around. Think of a cuisine and it’s most likely within 10 minutes walking distance. On my first month, I’m pretty sure I didn’t eat two same cuisines in the same week.What’s better than the food are the lunch time conversations. These range from random non-work related conversations about Brexit or some new game that’s coming out in the week, to something specific that’s happening in a little country that no one has heard of. I can’t search on google for stuff like this. A lot of times during these lunch conversations I’d make a note to self in my head and then google the crap out of it on the weekends, going head first into a giant rabbit hole. Knowledge is awesome.🏢👎 Downsides of working from the office💸 ExpensiveYes, those sweet sweet Vietnamese curries that I devour for lunch add up over time. Are they worth it? 100% but when I do the math, they do add up. I’m eating in Central London, it is going to be expensive.Also travelling to work is expensive too. Not too expensive compared to the normal London standards, I am definitely not taking a black cab to work every day, but expensive compared to what the travel costs might be if I was doing the same commute in a smaller city or town. The London underground is pretty efficient at getting people to work on time - on most days; however, the monthly subscription isn’t cheap. Now I don’t remember how much I used to pay, maybe £150-£200 a month? I switched from taking the tube to cycling to work almost two years ago but it was definitely an expense that I thought I could cut at the time.🚃 TravelGetting to work is fun. On some days I feel like a real grown-up with my laptop bag and headphones on, walking to my nearby train station. Hurr Durr look at me, I’m an adult going to work 😂What is not fun is getting squished in London underground carriage under some giant 6 foot something person’s armpit when it’s already stuffy inside because it has been raining all morning. Also, the whole queuing to get into a carriage is pretty tiring. Watching the tube arrive, only to see it jam-packed with people already on it, hoping that group in the middle might get off at this stop so that I can find a 5”x5” space to stand.While I did want to highlight the travel aspect, as not everyone cycles to work, I’ve not had such issues since I started cycling to work. In fact, these issues have turned into fuel for me to continue cycling to work. On days when I feel like not riding my bike, I just imagine that picture of me in the tube, cramped in a tiniest of spaces, and suddenly my legs don’t feel tired anymore.👋 InterruptionsNow this is not the case all the time. My colleagues are pretty well disciplined in that I don’t get random taps on my shoulders all the time. The typical “hey I want your attention but don’t want to disturb you” is someone’s head coming up the horizon like a meercat, hoping I would notice. Occassionally someone would tap on my shoulder or wave in front of my eyes, but usually these methods are reserved for when my attention is really needed.The issue with interruptions is that it breaks being in The Zone. When I am in The Zone, my mind is usually holding the context of what I am working on. The context of the application, the customer use case, the testing, the code framework that I am using, all the changes I need to make, in sequence, tracking errors and the mental list to fix them, and in what order. This is just scratching the surface. Ask any Software Engineer and they’ll tell you how important The Zone is in gettting stuff done. When someone taps on my shoulder, all this, the entire mental universe of things comes crashing down and vanishes.Surpirisingly, this is fine and in the subsequent times I get interrupted, it gets easier to re-create the state of mind that is being in The Zone. However, it gets tiring and causes mental fatigue that builds up over the day, until at the point when one’s too tired to do anything.🏡❤️ Reasons why I love working from homeI live in Greenwich, a borough pretty close to Central London. My neighbourhood is pretty quiet, mostly residential with a few shops and nice restaurants at a stone’s throw. I moved into my current place only last year so I am still pretty new to the area. Since it’s only been a year, I am still setting up my place. It’s sort of good that I was forced to work from home because it has made me rethink how I should organise my living space. Initially, when I set out planning the home furnishings, my priorities were different - they were designed around not being at home all the time. Now it’s different because I need my home to be able to support me if I have to work from it all the time.🌌 The ZoneWorking from home is completely different compared to working from the office. Sure I’m doing the same work but its the environment that makes it different. Living alone 🎻, my flat is very quiet. It makes it easy for me to slip into The Zone where hours go by like seconds and work gets done quicker than ever. It’s a mental state of absolute focus. I usually get into it when I have a clearly defined end goal. Like designing or implementing a feature from end to end. It is great to start my day, pick up a task, get in The Zone and then end the day having either done the task completely or made significant progress on it.Interruptions that break being in The Zone when I am working from home just aren’t there. Occasional delivery guy delivering something or a neighbour needing a chat are things I can usually work around. And since they are not that frequent the mental fatigue build up over time is pretty small.💰 SavingsI’d be lying if I said this wasn’t true: working from home has saved me some money. Not as much as I thought it did, the main large expenses are still there but some of the medium to small ones have disappeared. For example, I don’t buy lunches anymore, I cook at home most of the time. So while my monthly grocery expense has gone up, the lunch expense going down has added a decent boost. Apart from that, there aren’t many savings that I have made. Also, the cost of not having to buy lunch is offset by me ordering food for delivery on few of the days. This has increased the cost because the food delivered is more expensive than the cost of me walking up to a place to buy lunch.🛋 ComfortNow this could vary depending on your setup - for me, it is more comfortable working from home. My chair, table, monitor, the whole setup is tailored to my liking. Sure it might not be objectively the best but I’ve designed it to be comfortable to my liking. Occassionally, if I get bored, I move over to the sofa with my work laptop and start working from there. Or work from the balcony during summer.Having said that, the comfort of working from home came at a cost, both financially and physically. I have suffered from RSI in the past so it took me a while to adjust everything to my liking. While I did this, my RSI got triggered a bunch of times when I suffered from wrist pain, only to figure out that the source was the height of the chair or not having a separate keyboard to work.🏡👎 Downsides of working from home🤫/👋 Too quiet / too many interruptionsThis one’s a bit of a double-edged sword. Since I live on my own, I find my surroundings to be too quiet. Maybe its because I grew up in a busy home but I like ambient sounds during the day. When I grew up, my dad conducted most of his business from home and my mum was a full-time housewife so while they ensured quiet surroundings when I was studying, there were always few ambient sounds of my mom cooking or my dad opening/folding newspaper to read etc. Here in my London flat, sometimes I find it to be too quiet. Not that I want to live in a noisy neighbourhood but maybe I’m now used to those ambient sounds of people being around me.I know some people who are living with their families are finding it difficult to work from home. Partly due to interruptions from their kids who were at home all day when schools got closed and also from their partner / rest of the family members. It can be difficult if there isn’t a separation of space, between a dedicated working space and normal “living space”.⚡️ Over workingWhile being in The Zone has it’s perks, this is one of the downsides. I’ve had days when I’ve forgotten to take both lunch and dinner. I’d get out of The Zone at 9pm only when I realise I’ve been struggling to see because the monitor is too bright and I’ve not turned on the lights. Also I am hungry, why am I so hungry?!It’s not just being in The Zone. It’s also to do with not having to travel. When I am working in the office, having to get home is almost a driver for me to wrap things up and call it a day. That physical separation of 6 mile bike ride gives me good time to start unwinding. At home there is no such thing so it is easier to keep working. I sometimes find myself starting work at 8am and finishing at 10pm! It’s not a problem for me personally because I love what I do but it’s not good for the long term health and longevity. I know this sort of behaviour causes burnout and it’s not good for anyone.🏃‍♂️ Workout disciplineMaybe this is just me that has this issue. Since I bought myself an indoor bike trainer, working out when working from home is a little more challenging than going out for a bike ride. Also since I have lost that morning and evening bike ride to/from work, I sort of feel extra pressure to step up my indoor bike workouts. Usually when I am working from the office, I’d get 40-ish minutes of free workout every day, just riding my bike to work. Then I’d go to a nearby gym and workout from there for an hour. That would make total of 8+ hour work week just working out. Pretty good!Now that I am working from home, I’ve lost that ride to work workout time. Sure its not super intense but it still burns more calories than me walking within my flat. The indoor workout time is about 1 hour a day which would make it 5 hours spent working out during the work week. Nowhere close to the previous 8+ hours.On top of this riding bike indoors is less fun. I use Zwift so its definitely more fun than staring at a blank wall but its not as much fun as riding outdoors. However, it is a whole another level of intensity. On London roads, I can never ride continuously for an hour with an average speed of 22 miles an hour on a bike. In Zwift, I can. It is very good for building fitness very quickly. However this step up in intensity and a step down in time spent working out during the work week is hard to put in numbers and therefore hard to compare to see whether I am doing enough.ConclusionSo there you have it. My thoughts so far on working from home. It has its ups and downs. While I do wish the office would open sooner rather than later, I wouldn’t attend the office if the COVID restrictions are in place, limiting interactions, no access to kitchen etc. Because thats why I go to the office in the first place. I can do my work anywhere, I don’t need to be present in the office to do it. But it is the interactions and conversations that I value so much more than anything else.If the restrictions were to open, I’d prefer a mixed appoach, where I work from home on days when I need to get certain things done, and go into the office for most of the other days. Although, not sure where I would draw the line of going and not going to the office. Maybe we’ll discuss that in another post.What are your thoughts so far? How do you feel working from home? Would you rather be in office? What are your hacks to get more out of your work from home life? Please write a comment below. I’d love to hear about it.",
        "url": "/2020/10/02/on-working-from-home/"
      }
      ,
    
      "2020-09-23-on-quitting-social-media": {
        "title": "On quitting Social Media",
        "author": "Manthan Dave",
        "category": "",
        "content": "Social media has made the world smaller. It has brought great minds together, resulting in many unexpected connections and collaborations that would not have happened otherwise. It has brought great content from people's personal lives in the palm of one's hand. Birthdays, pregnancies, celebrations, you name it. Sadly it does not discriminate and has also brought hypersocial people closer too. People who like to post anything and everything on social media. Had a morning coffee? Got to post that on Facebook. Went to walk by the river? There's a filter, a caption and a hashtag for that! More power to them but it adds time to the lives of those who are connected to them. If you're following one person and spend let's say 5 seconds (probably more) looking at their one post, for 200 friends (on average its probably way more) each posting an average of 3 posts a day (include posts, sharing of links and sharing of other posts), you'd be spending a whopping 3000 seconds per day. That amounts to 50 minutes of your life spent every day on following people. And this number does not even account for all the notifications for likes, comments, likes for the comments, shares and direct messages to personal or group chats!Imagine all the time saved if you just quit social media! I've seen YouTubers bragging about quitting social media, who quit because a large portion of their 200k+ followers commented this as the next thing to do on their video. Apart from the irony and the typical: I had more time to do X, Y and Z things, there doesn't seem to be anything super game changer. I mean, I'm hanging on to the edge of my seat waiting for someone to quit social media and solve world hunger and cure cancer. Any day now!Quitting the Instant lifeSo I thought I'd try it. Not go completely cold turkey - I live in the UK on my own while the rest of my family lives in India. Social media is one of the best ways for me to connect with them. But I sought for a way out of the Instant life. The life of constant Instant notifications where every single like, comment, like on comment, share popped up on my phone as if it is the most important thing for me to focus on.So about a two and a half years ago, probably more, I deleted the Facebook app from my phone. Within a week I came close to reinstalling it. The only reason why I couldn't was that my old phone didn't have the space available. It was a really old phone with 16GB internal memory, 9 of which was used by the operating system and other built-in system apps. Buying a new phone just so I could reinstall Facebook seemed absurd so I kept it uninstalled and moved on with my life.The next app to go was Snapchat. At this point I wasn't thinking about reinstalling Facebook, just looking for my next victim of an app to delete and remove from my life. I still had the Facebook Messenger and Whatsapp installed - mainly because I used those for video calls to my family back in India. But over time I did tune notification settings, mainly around the group chats. Some people who get added in group chats feel like they have to contribute every day by saying or sharing something. This is fine and it is great to have someone like that but getting an audible notification on my phone for a post about how Coca Cola breaks down your internal organs just like how it cleans a rusty nail is not what I would call urgent. Besides, I found that if there was anything important, I usually get a direct message anyway. Also, I can always catch up at the end of the day when I have time on all that was going on in the group chats. There is definitely some good stuff in there, it's just not urgent.Time went by and I uninstalled Instagram. Same thing. I found myself spending hours catching up on the \"Stories\" and photos. Either my life isn't as interesting or I don't know how to work the platform but I never had much to post.Where's my free time?At some point, I looked back and thought - I have uninstalled all these apps, where is my free time? Where is my 1 hour every day that is freed up? Well turns out, the reality is that time tends to \"fill up\" in any given space, just like air and gases fill up any and all available space. Those blog posts and youtube videos do not tell you this. I am not solving world hunger, nor am I curing cancer. Why not? - I asked myself.Well, the mind is a funny thing. Procrastination is an art form that our minds have perfected. Have an hour to kill? Oh, what's on TV? No TV? How about the newspaper? No newspaper? What about the book? No book? Staring at a blank wall it is!Procrastinators are going to procrastinate no matter what. I found myself doing this all the time. After spending some time dissecting this, I found that the 1 hour of expected free time was filled up thusly:Slightly longer video calls with familySome more time in prepping my workout setupListening to audiobooksCleaningWalkingTending to plantsLonger time workingLonger time cooking...Of course, this is a very dynamic list, sometimes the time is used up in catching up with friends on a 1:1 basis which is even better. Sometimes it's used in walking to a place than taking a bus. There are all sorts of use cases. But the point is that it's not like a single 1-hour block that I have left in my day. I have plenty of things that I shouldn't do but I'd rather do than do what I am supposed to do. Quitting social media did not change this at all. It merely removed one aspect. Plenty of others very quickly gobbled up that space.So, if you're thinking, GOD! I can be so much better at whatever it is that I am doing if I just quit social media, just wait and see. Unless you really want to do what it is that you're trying to do, it won't happen no matter how much time you have left in the world. If you're looking to catch up on a side project, try taking a week off of work. See what that does to you. Do you make strides? Or do you procrastinate and fill that completely free week up with filler stuff? People complain about time all the time! If I just had X hours more, I could do Y and Z. Earth could have 30 hours a day instead of 24 and people would still be complaining about lack of time. It is not the lack of time that is the problem, it is the lack of initiative and discipline.",
        "url": "/2020/09/23/on-quitting-social-media/"
      }
      ,
    
      "2020-09-14-basic-garbage-collection-explained": {
        "title": "Basic Garbage Collection Explained",
        "author": "Manthan Dave",
        "category": "",
        "content": "Imagine you are inside a pub, having a few drinks with your mates. Eventually, as time passes, your table starts to fill up with empty glasses. You have two options to reduce the clutter: You clean up the table yourself by picking up the glasses and dropping them at the barYou wait for one of the servers to come by and clean it up for you.There is a third option where you raise your neck like a meerkat and try to make eye contact with one of the servers, hoping for them to come by your table wherein you order more drinks and in the process they clean up the table for you. We'll discuss this option later.Cleaning the glasses by yourself is hard but a sure way to clean things up. You know who's still drinking and which glasses are clear so you can be precise. However, it is hard work because you have to be constantly mindful of the utilisation and have to keep doing it.There is a better way.If the pub you're at has servers, one of them can come by and clear up the glasses. However, this approach is less precise because while they make sure they only clear empty glasses, sometimes they misjudge and can accidentally clear up ones that have that some drink left. Also the frequency of the cleanup would have to be fine tuned - although experienced servers and staff have this nailed down pretty well. If they clear up too frequently (imagine a server grabbing the empty glass out of your hand as soon as you're done) it would be rude and would leave you out of a drink, but if they do it too late, glasses will pile up. Either extreme would result in annoyed patrons who will might leave to find another pub with better staff.Manual GCWhen programming using languages that don't have \"managed memory management\", the former is the case. When the program runs, it will create variables and objects that take up the memory space. If the memory space is not \"managed\", the programmer has to make sure to put instructions in code to clear up the memory. This could be seen as a boon from performance perspective because the programmer would know when to and when not to clear the memory. Additionally, it gives programmer more control over what is running, i.e. there is no garbage collector process or thread present which could interfere with system's performance. At the same time, this could be problematic because they might forget to clear some parts which could eventually fill up the memory over time. Also, going back to the pub analogy, if you're at the pub having a nice chat with your mates, it can get annoying and time consuming on your part to have to remember to clear up the table from time to time. If you forget and glasses keep piling up to a point when the table can no longer hold more glasses, adding one more glass onto the pile could make them all crash onto the floor, ruining everybody's evening. The program, in the same way could crash if the programmer either forgets to clear up some objects or references or clears up objects or references that are still in use.If the memory space is not \"managed\", the programmer has to make sure to put instructions in code to clear up the memorySome languages, like Java for instance, have built in garbage collection. What this means is, instead of the programmer having to clear up the memory manually, there is a small, low priority thread running in the background that watches the lifecycle of the objects and variables that are created, making sure that unused ones get cleared up. This approach is great from programming perspective because as a programmer, I don't have to micro-manage the memory. It frees me up to do other things like writing elegant code and focusing more on the business logic. However, as we established earlier, this is not very precise because while the garbage collector is watching the objects and taking notes, it takes up a small, tiny amount of CPU power and memory. In the grand scale of things, this is small, however, arguably those resources could be better spent in processing more requests.Serial GCNow when it comes to clearing up the memory, the garbage collector can take different approaches. A simplistic approach is to simply freeze the program and clear up all objects and variables that are no longer referenced. While this is ok for small applications, it is not ideal because for that time, the program is unresponsive. Usually with small programs, this time is in micro or even nano seconds so it won't matter, however with large applications using gigabytes of memory, this time could be a couple of seconds. This is actually how garbage collection algorithm used to work in the old days and you can still use it now using the `-XX:+UseSerialGC` flag when running your Java application.If that wasn't clear, lets go back to the pub analogy. Having Serial GC is like having one server in the entire pub who goes around and clears up the unused glasses off of people's tables. But instead of doing it silently and courteously, they'll just climb up on top of table and shout \"EVERYBODY FREEZE\". Everybody freezes like a statue making it easy for them to go around and collect the empty glasses. When they're done, they'll shout \"CONTINUE\" and everybody would resume like nothing happened. Now, even if everybody is pretending, this could get annoying if the pub doesn't have sufficient glasses and have to call \"FREEZE!\" a lot (read low available memory) or if the pub is really busy and everybody is drinking really fast (read highly variable memory utilisation).A simplistic approach is to simply freeze the program and clear up all objects and variables that are no longer referenced.This act of shouting \"FREEZE!\" and \"CONTINUE!\", in technical terms is called a Stop The World (STW) action. If required, JVM will call STW to pause all threads and objects from processing anything, resuming only when garbage collection is done.CMS GCGoing back to the pub analogy, there is another way to clear up the tables of unused glasses. When the pub is busy, instead of picking up and carrying the glasses at once, the server will watch the tables and mentally mark the glasses that are empty. If the glasses are not touched on say the third mark, they are collected. Simple right? In terms of garbage collection in Java, this approach is called the Concurrent Mark and Sweep (CMS).&nbsp;A CMS garbage collector has two primary functions. Mark and Sweep. Marking is action where references to an object are marked. When an object no longer has any references, it is \"sweeped\". CMS is a concurrent algorithm, taking full advantage of the modern multi core processors.G1 GCLike a couple of servers in a small pub, CMS works effectively upto a certain point. When heap sizes increase beyond 4GB, the efficiency that it brings starts to taper off. Relating back to the pub analogy, servers going around the whole pub marking and clearing glasses off the tables works efficiently up to a point. When the pub is too large say it has multiple levels or is large enough to hold say &gt; 2000 people, going around the whole pub becomes tiring for the servers.This is where the G1 Garbage Collector (G1GC) comes in. The basic premise of it's algorithm is to divide up the heap space into sections such that each garbage collection thread has a small enough section to manage. Think of this as each server in a pub having its own dedicated area to manage, like it usually is in the real world! Here since each server is operating in their own area, it is not only easier to manage the objects but more importantly it reduces overall overhead. ConclusionGarbage collection is pretty interesting topic. There are several built in as well as custom algorithms available. The Java virtual machine allows for a lot of customisation not only in the garbage collection approach but also in tuning of the garbage collection algorithm itself. However, it is a bit of a black art in the sense that there is no silver bullet and each approach is unique to many things like the supporting hardware, application use case, application architecture, underlying system settings and features etc. Having said that though, it is important to note that there is no inherently bad garbage collection algorithm. Its all relative.For instance, while the default garbage collection algorithm is the concurrent mark sweep, it is a bad choice if you are running single core machine (although I think it switches from cms automatically if the number of CPUs==1) and if your application is sort of a time-driven batch application. In that case, the serial GC is perfectly a fine choice despite the STW freezes. Why? Well because the batch application can do its thing, using reasonable amount of memory and when its done, who cares if the application freezes? Again, use case is king here.In this post I've tried to simplify the concepts around garbage collection in Java to make it easy for everyone to understand. Understand that the choice of an optimal algorithm isn't as clear cut as it seems and it might take some effort to actually find the right GC algorithm for your application. I'll cover this some other time, but please do remember that even the best garbage collector algorithms are no replacement for bad code or bad architecture! Choosing a wrong algorithm could wreck the application performance as much as the right algorithm that could enhance it. Usually there are certain markers that should tell you if the garbage collector is working against you and this is the point at which you should start your analysis. Having said that though, the default CMS is perfectly acceptable in most cases and you probably don't need to spend two sprints analysing your application.",
        "url": "/2020/09/14/basic-garbage-collection-explained/"
      }
      ,
    
      "2020-09-06-on-consenting-to-cookies": {
        "title": "On consenting to cookies",
        "author": "Manthan Dave",
        "category": "",
        "content": "Alright lets talk about cookies a second. Remember the EU Cookie Directive? All those pop ups asking to allow cookies are a direct result of that cookie directive.State of thingsBuzzfeed.com cookie pop upYes it is a little bit annoying, especially if you're visiting a website only momentarily to look something up and the pop up covers your entire screen preventing access. To be honest, I didn't pay much attention to them. In fact, I didn't even read what the pop up said, instead my mind would seek a way out of it by looking for a highlighted button with words like Accept/Allow/OK/Dismiss just so that I could get to the content that I was trying to view. In all fairness, this was the case because I didn't visit the site expecting to agree to some contract but to view the content of the website. As an Engineer, I know what the cookie directive says. I know that I shouldn't agree to it unless I really do but in most cases, like most normal people would treat terms and conditions, my instinct is to find a way out of it. So lets look at what the cookie directive actually says:“Member States shall ensure that the storing of information, or the gaining of access to information already stored, in the terminal equipment of a subscriber or user is only allowed on condition that the subscriber or user concerned has given his or her consent, having been provided with clear and comprehensive information, in accordance with Directive 95/46/EC, inter alia, about the purposes of the processing. This shall not prevent any technical storage or access for the sole purpose of carrying out the transmission of a communication over an electronic communications network, or as strictly necessary in order for the provider of an information society service explicitly requested by the subscriber or user to provide the service.;”https://cookiepedia.co.uk/eu-cookie-lawSo to comply with this, websites started putting up those annoying pop ups so that they can provide \"clarity\" into whatever use case they have of storing cookies on my computer. How do they do it? Well, usually, these things have two action buttons, Allow All or Reject Cookies. I don't know about you but it feels like a red pill / blue pill moment to me.Blue pill / Red pillThere is however a third option. It usually resembles \"Manage settings\", \"more options\" or in cases when they really don't want you to click on it, \"Learn more\". When you click on this option, the pop up usually changes into something a bit more helpful.Buzzfeed.com cookie settings pop upNow that makes sense - this is what BuzzFeed wants to do with my cookies. Note that everything is \"Off\" by default. This is what is supposed to be the default, but if you remember the previous screen, the primary button, designed to draw your attention and make you click was the Agree and Exit button. What would happen if you clicked that? Well I checked and in this case, it stored 34 cookies in my browser - in addition to you permitting all the above use cases. Whoa thats a lot of cookies. No wonder my computer is fat! What if you clicked Disagree and Exit button? 4 cookies. Thats it. If you clicked More Options, here again, the primary button is Agree and Exit, seems like they really want to store cookies in your browser! Alas, if you click that, your browser will have 34 cookies again, same result as if you never had clicked on More options in the first place. Although, if you clicked Save and exit, the number of cookies stored are only 4. Here's a small list to help you with the math.Agree &amp; Exit: 34 cookiesMore Options -&gt; Agree &amp; Exit: 34 cookiesDisagree &amp; Exit: 4 cookiesMore Options -&gt; Save &amp; Exit: 4 cookiesNow BuzzFeed is actually one of the good websites so they have a direct option to Disagree &amp; Exit which effectivelly turns off other cookies. Here's a website called Healthline.com who don't provide this option:Healthline.com cookie pop upWhen you click Manage Settings, you get a nice page with all fancy cookie use cases turned off by default:Healthline.com cookie settingsHad you agreed to the first pop up and clicked on \"Accept and continue to site\" you'd have consented healthline.com and its partners to do whatever they want to do under \"Special purposes\" and \"Special features\" (+ all the other use cases they have outlined) and received 12 cookies stored in your web browser. On the other hand, if you go into \"Manage settings\" and click \"Save settings\", it'll take you to anon.healthline.com - which is an ad free version of the website. It still stores 7 cookies but they are not storing any third party or marketing cookies on your web browser.And of course, this is assuming the websites are actually complying to the cookie law and aren't doing naughty things anyway. Of course there's no real way of knowing that. Websites could store cookies in your browser to track you and still send your information to the third party, via a serverside route without you knowing it happened at all.ReflectionThe thing about this that is striking to me is that for most people, the EU cookie directive has become more pain than something that actually works. From a consumer standpoint, it is merely an annoying pop up that gives bad user experience. Users just want to find the quickest way out of it and the website developers are happy to provide that - using the same dopamine driven design psychology that they use to get consumers to \"Buy\", \"Like\" and \"Share\" things.As an Engineer, this is annoying on several levels. It interrupts the user experience flow. It breaks immersive experience. It is annoying to implement. And even after all that, it doesn't give users what they deserve because they accept everything without reading anything. I'll explore the potential solutions some other time, but for now, the only real approach is to \"take control\" by reviewing the cookie consent and proceeding in an informed manner. Whether you're on a well known recipe website just cheekily reading that korma recipe or sifting through tons of web pages trying to figure out whether aliens really made the pyramids, take a few seconds of your time to review what you are consenting to. Or better yet, if you're only browsing to check something quickly, use something like Firefox Focus (or incognito/private mode in your favourite web browser), which effectively gives you a disposable browser where when you're done with your session, you can wipe the slate clean.If you want to explore the wonderful world of websites and the cookies they work with, you can try https://www.cookieserve.com. Type in a URL and it'll explain which cookies serve what purpose. Its not bullet proof but is good at catching google and facebook cookies if a website is sneakily using them.You could also use browser plugins that prevent tracking cookies from being stored. I use uBlock Origin (chrome/firefox) which is a general tracking blocker. Additionally, if you use a modern web browser like Mozilla Firefox, it comes with tracking protection baked in and turned on. Not sure about Google Chrome, it probably doesn't since Google's revenue depends on Ads. I've heard of an alternative called Chromium - which is supposed to be the core open source version of Google Chrome without the Google stuff but I'm not too sure about it so can't strongly recommend.",
        "url": "/2020/09/06/on-consenting-to-cookies/"
      }
      ,
    
      "2020-08-03-whos-using-my-macbook-camera": {
        "title": "Who&amp;#8217;s using my macbook camera?",
        "author": "Manthan Dave",
        "category": "",
        "content": "So the other day I was working away on my Macbook Pro and suddenly, out of the blue, the light next to my camera came on. Now, this is a new macbook so I don't have my little sliding window sticker that blocks my webcam yet. Even if I did have it, I'd still be equally worried. So I started digging. The only apps I had open at the time were:FirefoxBlueJeans (video conferencing software)Intellij IDEAVisual Studio CodeiTermBasically the Software Engineer's toolkit.After a little digging on the Internet, I found a few commands that could help me. I have conveniently combined them all into this single command:lsof | grep -e \"AppleCamera\" -e \"iSight\" -e \"VDC\"This gave  me the following:➜  ~ lsof | grep -e \"AppleCamera\" -e \"iSight\" -e \"VDC\"firefox   1956 mdave  txt       REG                1,5      424176 1152921500312438057 /System/Library/Frameworks/CoreMediaIO.framework/Versions/A/Resources/VDC.plugin/Contents/MacOS/VDCHmm so its Firefox? Weird because usually its very good with permissions. Most of my tabs were DuckDuckGo, StackOverflow, Jira, Confluence, Gmail and a few other \"trusty\" company web pages. This couldn't be it?I did some digging around in settings but no luck. Finally, I brought down the hammer and took away the permissions to Camera (and Microphone for safety) from the System Preferences. And viola!➜  ~ lsof | grep -e \"AppleCamera\" -e \"iSight\" -e \"VDC\"firefox   1956 mdave  txt       REG                1,5      424176 1152921500312438057 /System/Library/Frameworks/CoreMediaIO.framework/Versions/A/Resources/VDC.plugin/Contents/MacOS/VDC➜  ~ lsof | grep -e \"AppleCamera\" -e \"iSight\" -e \"VDC\"➜  ~No more naughty webcam access! I guess if I need it again, I'll just give it permissions explicitly through the System Preferences. Or perhaps I can side-install another firefox browser when I need to grant it permissions to view my webcam - the number of websites that I use that need this permission are very small in number and I can live with using a dedicated web browser for it.",
        "url": "/2020/08/03/whos-using-my-macbook-camera/"
      }
      ,
    
      "2020-05-04-mac-command-to-view-last-login-wake-sleep-charge-times": {
        "title": "Mac command to view last login/wake/sleep/charge times",
        "author": "Manthan Dave",
        "category": "",
        "content": "The pmset utility on mac is quite useful for viewing system logs. If you type:pmset -g logyou'll be able to see some cool system logs. However, as a timesaver, here's a no-bullshit command to view the following times:When mac went into sleepWhen mac came out of sleepCharge levelsStandby timespmset -g log|grep -e \" Sleep  \" -e \" Wake  \"Enjoy!",
        "url": "/2020/05/04/mac-command-to-view-last-login-wake-sleep-charge-times/"
      }
      ,
    
      "2018-08-08-bulk-edit-filenames-using-shell": {
        "title": "Bulk edit filenames using shell",
        "author": "Manthan Dave",
        "category": "",
        "content": "            for f in *-defaults.properties; do mv “$f” “$(echo “$f”      sed s/-defaults//)”; done      ",
        "url": "/2018/08/08/bulk-edit-filenames-using-shell/"
      }
      ,
    
      "2018-06-27-useful-unix-file-editing-commands": {
        "title": "Useful unix file editing commands",
        "author": "Manthan Dave",
        "category": "",
        "content": "Find and replace text matching a regular expression in a single filesed -E -i '' 's/(something\\\\-[\\\\da-zA-Z]+)/ToReplace/g' path/to/file.txtThe above command searches for the (something\\-[\\da-zA-Z]+) regular expression and replaces it whole (because of the parenthesis which means to select the text matching the expression within) with ToReplace. The g in the end indicates that the operation will be applied to all matches in the file as supplied in path/to/file.txt argument. The -i parameter along with '' suggests the sed command to perform the edit on the file itself, without creating a new copy.Find and replace text matching a regular expression in files matching namefind /base/directory -name \"*.txt\" -type f | xargs -n 1 sed -E -i '' 's/hello/hi/g'The above command finds file within /base/directory whose names match *.txt format. Later we combine the output of this with xargs which appends each line of output (path to each matching file) to the following sed command. The -n 1 argument to the xargs command tells it to supply each line of argument one by one to the sed command.Find and replace text matching a regular expression in files whose contents match a regular expressiongrep -rlE 'TextToFindInFiles' $(find /base/directory -name '*.txt' -type f) | xargs -n 1 sed -E -i '' 's/hi/hello/g'In the above command we use the basic find command to get a list of files that we want to do the search in. Then we use grep to recursively find in those files. Now in the above command we don’t really need the -r flag for grep because the find command will list full paths to those files, but we would need it if we were doing find on a relative path instead (like .). The -l flag for grep here will only list the paths to the files that it found having the content TextToFindInFiles and not the actual matching contents like it usually prints. This list of the files is then outputted to the xargs command which then subsequently runs the sed command. ",
        "url": "/2018/06/27/useful-unix-file-editing-commands/"
      }
      ,
    
      "2018-03-12-documenting-project-information-with-maven": {
        "title": "Documenting project information with Maven",
        "author": "Manthan Dave",
        "category": "",
        "content": "In most projects I’ve worked on, the project information is captured in some sort of README.md file. While this has its strengths, such as ability to write free form text and embed images, it doesn’t quite fit with the project because when the artifact is produced during the build time, the read me is not checked into the artifact repository along with it. So how do I know who the contributors were for that specific artifact version? How do I know where the project was hosted at the time?Well, today I learned that maven provides ability to capture some of those attributes and some more! I like this because it means that when the artifact gets checked in (along with its pom.xml) the details about the project are captured for that version, frozen in time.Great thing about these attibutes is that when you run mvn site they are used in the project information html page that maven produces. Technically, then you can upload this to any statically hosted site location, all versioned up and ready to be explored.Ok so lets begin exploring these attributes. The first three attributes are the basic ones:&lt;name&gt;Sample API (HTTP)&lt;/name&gt;&lt;description&gt;HTTP API endpoint&lt;/description&gt;&lt;url&gt;https://github.com/manthanhd/sample-api&lt;/url&gt;For people working in corporations and companies, the following might be useful, especially when open sourcing the project:&lt;organization&gt;  &lt;name&gt;Company Name Inc&lt;/name&gt;  &lt;url&gt;https://www.company.org&lt;/url&gt;&lt;/organization&gt;Additionally, the licenses tag can be used to describe some licensing information. How many times have you come across a project that has changed the licensing information half way through versions? This is a life saver (at least legally)!&lt;licenses&gt;  &lt;license&gt;    &lt;name&gt;MIT License&lt;/name&gt;    &lt;url&gt;https://opensource.org/licenses/MIT&lt;/url&gt;  &lt;/license&gt;&lt;/licenses&gt;Some source control information is also useful, however, whats more useful is knowing where to go in order to raise issues. In most companies, people use source control to store code but then use an alternative mechanism like Jira or Trello to mange issues. The scm and issueManagement tags are useful here in clarifying such information:&lt;scm&gt;  &lt;url&gt;https://username@company-bitbucket.org:orgname/sample-api&lt;/url&gt;  &lt;connection&gt;scm:git:git://username@company-bitbucket.org:orgname/sample-api.git&lt;/connection&gt;  &lt;developerConnection&gt;scm:git:git@company-bitbucket.org:orgname/sample-api.git&lt;/developerConnection&gt;&lt;/scm&gt;&lt;issueManagement&gt;  &lt;url&gt;https://some-jira.companyname.org/newissue&lt;/url&gt;  &lt;system&gt;Private Jira powered issue management&lt;/system&gt;&lt;/issueManagement&gt;Developer information on projects is very useful. Companies don’t usually like this because, well, developers are supposed to be expendable, however, in my humble opinion, it is useful to list developers who were working on the project at the time. This way the versions can be tracked, at least to the lead working on it at the time. Use it thusly:&lt;developers&gt;  &lt;developer&gt;    &lt;name&gt;John Doe&lt;/name&gt;    &lt;email&gt;john.doe@company.com&lt;/email&gt;    &lt;roles&gt;      &lt;role&gt;Founder&lt;/role&gt;      &lt;role&gt;Chief Engineer&lt;/role&gt;    &lt;/roles&gt;  &lt;/developer&gt;&lt;/developers&gt;For contributors, people who have worked on the project, although not necessarily part of it exclusively, use:&lt;contributors&gt;  &lt;contributor&gt;    &lt;name&gt;Joe Bloggs&lt;/name&gt;    &lt;email&gt;joe.bloggs@company.com&lt;/email&gt;    &lt;roles&gt;      &lt;role&gt;Occasional Committer&lt;/role&gt;    &lt;/roles&gt;  &lt;/contributor&gt;&lt;/contributors&gt;Lastly, it is useful to capture some information about pipeline that was used to create the project. For this, maven has ciManagement tag:&lt;ciManagement&gt;  &lt;url&gt;https://company-jenkins.org:orgname/sample-api/pipeline&lt;/url&gt;  &lt;system&gt;Some Jenkins based pipeline&lt;/system&gt;&lt;/ciManagement&gt;The above is just a small example of what you can do with these attributes, there are many more sub-attributes that you can explore. Hope this is a good introduction and kicks off some ideas of what you can do with this information.",
        "url": "/2018/03/12/documenting-project-information-with-maven/"
      }
      ,
    
      "2018-03-10-finding-classes-in-jar-war-files": {
        "title": "Finding classes in jar/war files",
        "author": "Manthan Dave",
        "category": "",
        "content": "Recently I was out looking for classes that were present in my Web Application Archive (WAR) file. Why? Well, I was out combating Jar Hell. As I am allergic to doing things manually, here’s a small command I constructed to help me find classes within jar files:find . -type f -name \"*.jar\" -exec sh -c \"jar -tf {} | grep \"SomeClass.class\" &amp;&amp; echo ^ found in {} file\" \\;The above command prints out class file as well as the name of the jar file that class is present in. If you’re looking just for the path where the classes are present, you can just run:find . -type f -name \"*.jar\" | xargs -n 1 jar -tf | grep \"SomeClass.class\" | less ",
        "url": "/2018/03/10/finding-classes-in-jar-war-files/"
      }
      ,
    
      "2018-03-08-update-all-pom-versions-at-once-using-maven-versions": {
        "title": "Update all pom versions at once using maven versions",
        "author": "Manthan Dave",
        "category": "",
        "content": "Most of the projects I work on are multi-module projects so updating versions of pom files manually is a bit pain. As always, here’s a command you can run that will update all pom versions in one go:mvn versions:set -DnewVersion=2.0-SNAPSHOTMake sure all your modules are discoverable. You can do this by enabling all your profiles in case some of your sub-modules are not visible from the main pom.mvn -P profile1,profile2 versions:set -DnewVersion=2.0-SNAPSHOTSource: https://stackoverflow.com/a/5726412",
        "url": "/2018/03/08/update-all-pom-versions-at-once-using-maven-versions/"
      }
      ,
    
      "2018-03-01-an-introduction-to-mocks-spies-and-fakes-in-unit-testing": {
        "title": "An introduction to Mocks, Spies and Fakes in unit testing",
        "author": "Manthan Dave",
        "category": "",
        "content": "When we’re unit testing, in addition to knowing the boundary conditions, it is important to know what classification our dependencies fall into, in testing terms. These classifications are: \tMocks \tSpies \tFakesMocksAs the name suggests, a mock is a proxy or placeholder value that can be programmed to respond just like a real object would under any or matched conditions. A mock should be used where the real object dependency we have is very complicated to deal with during testing. This could be (but is not limited to): \tThird party interactions (database/queue/api/files) \tClasses that do really complicated stuff that take a lot of time \tThings that work with random \tTime based workMocking dependencies is a three phased approach: \tCreate the mock of the dependency based on the class or interface. \t“Arm the mock” by preparing it work request/responses based on any or matched interactions \tSet the mock on the test subjectCreating the mock could be the easiest or the hardest thing of all of the three because it depends on what you’re mocking and how you’re mocking it. If the test subject is relying on a class directly rather than an interface, mocking could become a bit trickier depending on how that class was implemented. This is because when you’re mocking a class, it has to first create a new instance of that class (by calling its constructor and by extension constructing all its ancestors). If at any one point, if one of the ancestors is doing something funky with one of its dependencies within the constructor, you’ll have to mock that out. This requires some level of knowledge or understanding of the entire class hierarchy and this is why use of interfaces for your dependencies is preferred.When mocking an interface, Mockito uses JDK Proxy (as opposed to CGLib when mocking a class) which plays much more nicely with everything else. Also, because interfaces cannot be instantiated, there is no hierarchy to follow and the JDK Proxy wraps around it without any problems (or worry about dependencies). Although note that I have yet to personally test how this works with Java 9 which allows interfaces to have implementation code in them.Arming the mock is relatively straight forward and involves three steps: \tKnowing which part of class to “arm” Which method are you mocking? \tKnowing what to return (or do) What should that method return/do? \tKnowing what to match Under which parameters should this mock work?In most mocking frameworks, you can mock methods and have them either call the real thing (if its a spy; covered in next point) or have them return a custom value. In addition to this, you can specify the circumstances under which the mock should do what you’ve configured it to do. This is where the matchers come in. I am not going to cover how the matchers work in this post as that is a whole thing of its own, you can find out more here:https://static.javadoc.io/org.mockito/mockito-all/1.9.0/org/mockito/Matchers.htmlOnce you’ve armed the mock, it is time to make it available to the test subject. Now, ideally, this should be done in a way that the test subject isn’t even aware of the mocking, in a most non-intrusive way. Here are a couple of ways you can do this in decreasing order of intrusion: \tSetter methods with default access modifier \tConstructor args \tInversion of control - field level dependency Injection magicObviously its great to aim for point three but sometimes its harder to get DI involved in a project that has never has it so my suggestion here would be to work backwards from that.When testing, while mocks are great at controlling the behaviours of the dependencies that your implementation is relying on, it is also great to verify whether or not the dependency is used in certain flows. In mockito for example, here you can ask Mockito to verify whether a particular mocked method was called, how many times it was called and even checking in certain cases to ensure that it wasn’t called!SpiesSpies are like mocks, the only difference being that there is a real object under there. This is not the “real” object that CGLib wraps around, it is the one that you create to then set a spy on. Generally, spies are used in cases where you want to track the interactions that your test subject is making. This is nasty in my opinion and should only be used where you have to mock one of the test subject’s own methods (because they are being called from one of its other methods) or when you want to verify that a certain method is called.Why is it nasty? Well it is because technically after setting a spy on the test subject object, it is no longer the real test subject. In addition to that spying on the test subject makes it easy to accidentally leave a mock on one of the methods of the test subject’s spy without realising and have the test(s) accidentally pass!Spying generally involves three steps: \tCreate real object \tCreate Spy around the real object \tSet (or use) the spyFakesFaking is not mocking. Fakes are real objects but they have fake values in them. My suggestion here is to fake POJOs and other “simple” classes. The general rule of thumb that I think everybody should follow is that if the external dependency is more work to mock than fake then fake. Faking should extend, but not be limited to: \tData structures (Map, Queue, List) \tPOJOs \tBuilder classesLike mocks, using a fake involves two simple steps: \tCreate the fake with required values \tSet the fakeAs always, be mindful of the inheritance hierarchy here to ensure it doesn’t become unreliable and make our lives harder than it should be. An unreliable fake is a marker for something that should really be mocked.Of course, all the three classifications above can be used in conjunction with each other. For example, recently I had to spy on the test subject, mock one of its methods to return a mock object which then in turn returns a fake object when one of its methods is called!",
        "url": "/2018/03/01/an-introduction-to-mocks-spies-and-fakes-in-unit-testing/"
      }
      ,
    
      "2018-02-23-finding-biggest-files-and-folders-on-linux-unix-systems": {
        "title": "Finding biggest files and folders on Linux/Unix systems",
        "author": "Manthan Dave",
        "category": "",
        "content": "I was happily doing some builds on my Jenkins slave server at home and then suddenly boom! it broke. It took all the queued up builds with it because they all started failing. When I looked, it was a disk space issue.Normally my builds don’t take up much disk space so I started investigating. I needed to find files that were occupying largest size on the disk. After some trial and error, following command came to rescue:find . -type f -exec du -sh {} \\; -print | sort -nThis was great, but then I wanted to find folders that were the biggest. No problem!find . -type d -exec du -sh {} \\; -print | sort -nNotice the -type d above which differentiates it from the first command. Also note that the above command outputting the disk use by folder also includes subfolders, so generally the largest ones will be the top level folders and as you go up the list (its ascending in order by default) you’ll find the subfolders with their sizes. You can always pipe the whole thing through grep to only look for the folders you want like so:find . -type f -exec du -sh {} \\; -print | sort -n | grep -i /targetWhile this is great and all, sometimes you just want to go upto certain depth within the file tree. Say no more!find . -type f -maxdepth 2 -exec du -sh {} \\; -print | sort -n | grep -i /targetNotice the -maxdepth 2 flag which sets the max depth to 2.However, you could be one of those people who don’t like the find command at all. Maybe a past feud, or just a dislike. Well, the du command has your back!du -m -d 2 * --all | sort -nThe -m flag makes it print file sizes in megabytes, -d 2 sets max depth to 2 and --all tells it to work with files as well as directories. Its actually quite comprehensive because using some clever flags like -I to provide a mask for files and directories to ignore and -L to follow symbolic links (they are not followed by default) you can get quite a lot out of it. Also, a quick note before ending this post, you can switch the file size block from -m for megabytes (example above) to -g for gigabytes or even -k for kilobytes. You can use -h for human readable where it will automatically choose the closest block size but this will confuse the sort because it doesn’t quite take the size character in account and only sorts things using the numeric values.Above commands have been tested on mac where they were installed as part of GNU CoreUtils homebrew package.",
        "url": "/2018/02/23/finding-biggest-files-and-folders-on-linux-unix-systems/"
      }
      ,
    
      "2018-01-01-download-java-cryptography-extension-jce-jars-using-curl": {
        "title": "Download Java Cryptography Extension (JCE) jars using curl",
        "author": "Manthan Dave",
        "category": "",
        "content": "Normally, in order to acquire the JCE jars, you have to: \tGo to google \tSearch JCE Jars \tNavigate to oracle website from search results \tAccept some agreement agreeing to sell your soul to Oracle \tFinally download the jars.While this is great, it doesn’t work well for when you want JCE jars in your docker container. I mean, yeah you can get it by downloading it into a static directory so that it is available to docker during build time but to be honest, that is a bit lame. Lamer than just fetching it from a URL.Now, I can’t remember the exact source but after much head banging and googling, I found the following command to download Java Cryptography Extension jars on the fly using curl.curl -q -L -C - -b \"oraclelicense=accept-securebackup-cookie\" -o /tmp/jce_policy-8.zip -O http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip \\&lt;br&gt;    &amp;&amp; unzip -oj -d /usr/lib/jvm/java-8-openjdk-amd64/jre/lib/security /tmp/jce_policy-8.zip \\*/\\*.jar \\&lt;br&gt;    &amp;&amp; rm /tmp/jce_policy-8.zipAt this point in time, I don’t have links to other versions of this. I needed it for Java 8 and I found it for Java 8. Regardless, I can’t imagine why it wouldn’t work if you’d just change the version from 8 to 7 in the above command.",
        "url": "/2018/01/01/download-java-cryptography-extension-jce-jars-using-curl/"
      }
      ,
    
      "2017-12-24-how-to-delete-all-entries-from-java-jks-keystore": {
        "title": "How to delete all entries from Java JKS Keystore",
        "author": "Manthan Dave",
        "category": "",
        "content": "I had to deal with this recently. After much trial and error, here’s the command that you can use to wipe your Java JKS Keystore of all its entries:keytool -list -keystore ${KEYSTORE} -storepass ${KEYSTORE_PASS} -rfc | grep -Po \"Alias name: \\K([A-Za-z0-9\\s_-]+)\" | xargs -n 1 -I {} keytool -delete -alias {} -keystore ${KEYSTORE} -storepass ${KEYSTORE_PASS}Here, the variable KEYSTOREis the path to your Java keystore and the variable KEYSTORE_PASS is the keystore’s password. If you are not comfortable in using the keystore password plain text in command line, I’d suggest you use an alternative version using a file containing keystore password or name of an environment variable instead. This will hide the password from appearing in shell history. You can do this by suffixing the -storepass argument with :file or :env resulting in it effectively becoming -storepass:file &lt;path/to/file&gt; or -storepass:env &lt;ENV_NAME_WITHOUT_$. Here are some examples:keytool -list -keystore ${KEYSTORE} -storepass:file ${KEYSTORE_PASS_FILE} -rfc | grep -Po \"Alias name: \\K([A-Za-z0-9\\s_-]+)\" | xargs -n 1 -I {} keytool -delete -alias {} -keystore ${KEYSTORE} -storepass:file ${KEYSTORE_PASS_FILE}In the above, notice how the ${KEYSTORE_PASS} environment variable has changed to ${KEYSTORE_PASS_FILE}. Use this to provide a path to the file containing your keystore password.keytool -list -keystore ${KEYSTORE} -storepass:env ${KEYSTORE_PASS_ENV} -rfc | grep -Po \"Alias name: \\K([A-Za-z0-9\\s_-]+)\" | xargs -n 1 -I {} keytool -delete -alias {} -keystore ${KEYSTORE} -storepass:env ${KEYSTORE_PASS_ENV}Similar to previous, this one has been slightly modified to use the -storepass:env flag with ${KEYSTORE_PASS_ENV} environment variable instead.",
        "url": "/2017/12/24/how-to-delete-all-entries-from-java-jks-keystore/"
      }
      ,
    
      "2017-09-01-generating-random-bytes-in-java": {
        "title": "Generating random bytes in Java",
        "author": "Manthan Dave",
        "category": "",
        "content": "I recently needed to generate a bit of randomness in Java in order to produce a secret. Java comes built in with Random and SecureRandom classes which can help you do this properly but as with all things, there are multiple ways of doing things.Two of these stood out to me.Generate a long random number as String, convert to hex and then convert it to bytes.SecureRandom random = new SecureRandom();byte[] randomBytes = Long.toHexString(random.nextLong()).getBytes();You could potentially improve this method by not just converting the long number to hex string but also maybe base64 encoding it. You could potentially go further by adding additional entropy to it by filling in random bytes in random indices.But as it stands above, the benefit of this method is that it is very quick to run. This may vary based on your random seed but I have found the above method to consistently produce byte arrays of size 14-16. This might be good enough in most cases - especially due to the fact that its fast, but in times where you might need very high amounts of entropy, the second approach might suit you.My personal discontent with this method is that the bytes it produces will be chunked by the length of each individual hex character due to the fact that it is coming from a hex string. This, arguably is not very random. Arguably because although every individual hex characters are themselves in random order, the bytes generated off of those will have identifiable chunks representing each hex character. The second approach resolves this problem in a simpler way.Another issue I have with this approach is that the total size is limited to the maximum value a Long can support (263-1). The size of Long data structure limits the length of hex string that gets generated which in turn limits total number of bytes produced.The approach below resolves most of these issues.Generate a array of random size composed of bytes and then let SecureRandom fill in those bytes.SecureRandom random = new SecureRandom();int byteArraySize = Math.abs(random.nextInt());byte[] randomBytes = new byte[byteArraySize];random.nextBytes(randomBytes);Here, we’re creating a byte array of random size and then using secure random to fill that byte array in with random bytes. Its simple and elegant. However, as with all things, this has pros and cons of its own.In my experiments, I have found the range of the byte array to be truly random. In a test that I ran, once it created a byte array of size 8890 while in another time, it created an array so large, I lost my patience and had to quit the process.This poses a problem where your program could take a long time to generate your secret. Furthermore, it could potentially even go out of stack memory if the array becomes too large.You can resolve this issue by setting a bound to the random.nextInt which is being used to determine the size of the byte array. I cannot tell you what size here is most optimal because it really depends on the capabilities of the processor you’re running on, your stack size as well as the algorithm you’re using to initialise SecureRandom. An implementation limiting the size of the byte array may look like following:SecureRandom random = new SecureRandom();int byteArraySize = Math.abs(random.nextInt(20));byte[] randomBytes = new byte[byteArraySize];random.nextBytes(randomBytes);Here the size of the byte array will be between 0 and 20.Also, please note that without the Math.abs the value for byteArraySize could be negative. If you initialise your array with a negative number for size, you will get NegativeArraySizeException.",
        "url": "/2017/09/01/generating-random-bytes-in-java/"
      }
      ,
    
      "2017-08-17-making-executable-jar-using-maven": {
        "title": "Making executable jar using maven",
        "author": "Manthan Dave",
        "category": "",
        "content": "I was trying something out the other day and wanted to write a really simple application. So I created a simple application backed by Maven.Now I could run the jar file that maven built using the standard -e flag that lets java know the entry point but all that is too main stream. I wanted maven to handle that for me.After doing some googling, I found a plugin provided by codehaus. This one allowed me to run the application through maven. As you can see below, the configuration is quite simple.&lt;build&gt;..  &lt;plugin&gt;    &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;    &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;    &lt;version&gt;1.2.1&lt;/version&gt;    &lt;configuration&gt;      &lt;mainClass&gt;com.manthanhd.cli.RunCli&lt;/mainClass&gt;    &lt;/configuration&gt;  &lt;/plugin&gt;..&lt;/build&gt;Make sure you update the value inside mainClass with fully qualified name of your class that you want to run. This class must have a public static void main method in it.Once you are happy with the configuration, you can run your application by executing the following in your terminalmvn exec:javaWhile this is great, I cannot run the jar on its own on a server somewhere. If I wanted to, I’d have to get the source code with maven and then run it using the above command. Thats sub-optimal. So I went googling again.Finally, I found this wonderful plugin provided by our friends at Apache. This is the maven-jar-plugin. This is a standard jar plugin but one of its features is ability to specify a mainClass attribute - just like the codehaus plugin. But unlike the codehaus plugin, I can run the jar as a standalone application without needing to pass in any other flags or parameters indicating the main class.Here’s the maven build plugin configuration to use the maven-jar-plugin.&lt;build&gt;...  &lt;plugin&gt;    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;    &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt;    &lt;configuration&gt;      &lt;archive&gt;          &lt;manifest&gt;              &lt;addClasspath&gt;true&lt;/addClasspath&gt;              &lt;mainClass&gt;com.manthanhd.cli.RunCli&lt;/mainClass&gt;          &lt;/manifest&gt;      &lt;/archive&gt;    &lt;/configuration&gt;  &lt;/plugin&gt;...&lt;/build&gt;As it is standard with maven, package up your application using:mvn clean installIf the build was successful, just run your jar file using standard java -jar path/to/app.jar command. In my case, I ran:java -jar ./target/my-awesome-standalone-app.jar ",
        "url": "/2017/08/17/making-executable-jar-using-maven/"
      }
      ,
    
      "2017-08-14-aws-pipeline-plugin-for-jenkins-2-x": {
        "title": "AWS Pipeline Plugin for Jenkins 2.x",
        "author": "Manthan Dave",
        "category": "",
        "content": "I found this really cool plugin last week so I thought I’d make a post out of it.I heavily use the “AssumeRole” capability within my application. Previously, this is what I used:/usr/bin/env aws sts assume-role --region ${AWS_DEFAULT_REGION} --role-arn ${CROSS_ACCOUNT_ROLE} --role-session-name \"AssumedSession\" &gt; /tmp/tmp.txtexport AWS_ACCESS_KEY_ID=$(grep -Po '(?&lt;=\"AccessKeyId\": \")[^\"]*' /tmp/session.txt | tr -d '\\n')export AWS_SECRET_ACCESS_KEY=$(grep -Po '(?&lt;=\"SecretAccessKey\": \")[^\"]*' /tmp/session.txt | tr -d '\\n')export AWS_SESSION_TOKEN=$(grep -Po '(?&lt;=\"SessionToken\": \")[^\"]*' /tmp/session.txt | tr -d '\\n')export TOKEN_ISSUED_SECONDS=${SECONDS}rm /tmp/session.txtAs you can see, the code above is using shell commands to achieve the assume role awesomeness. This works well when your jenkins job has a shell command step but not when you have a groovy pipeline defined in your Jenkins.I struggled with it initially - one of the ideas I had was to upload the assume script somewhere like S3 and then pull it down and run it when I wanted to run commands under the assume-role. However, this felt a bit cumbersome.Next thing in my mind was to write a groovy plugin that can do this for me. However, rather than reinventing the wheel, I started looking for existing solutions. Finally, I found the aws-pipeline-plugin.Its a neat little plugin that allows you to do a bunch of basic stuff that you might want to do on AWS. Assume role is one of them.So now with the new plugin, my code reduced to:withAWS(role: roleToAssume, roleAccount: myAwsAccountId) {    sh \"aws s3 cp file.txt s3://${bucketName}${uploadPath}\"}Here, I’ve got a couple of variables but their names should be self-explanatory of their purpose. The general idea is that anything you write within that withAWS block will get executed under the role specified in role variable.",
        "url": "/2017/08/14/aws-pipeline-plugin-for-jenkins-2-x/"
      }
      ,
    
      "2017-07-25-tunnelling-into-your-chef-kitchen-vagrant-instance": {
        "title": "Tunnelling into your Chef Kitchen Vagrant instance",
        "author": "Manthan Dave",
        "category": "",
        "content": "So you’ve just done this:kitchen create default-centos-72And now something has gone wrong on the box and you need to tunnel through to check something on your instance (say mysql database). Whatever the port may be, you need to change your current directory to where the vagrant file is. This can be achieved by running the following command relative to where your kitchen file is:cd .kitchen/kitchen-vagrant/kitchen-default-centos72List the directory and you will be able to see a Vagrantfile in it. Once you are there, run:vagrant ssh -- -L 3306:localhost:3306Here I’m using mysql port 3306, you can replace it with whatever port you want to tunnel through.If you want to tunnel multiple ports, run:vagrant ssh -- -L 3306:localhost:3306 -L 27017:localhost:27017Repeating -L &lt;BIND_ADDRESS&gt; multiple times.",
        "url": "/2017/07/25/tunnelling-into-your-chef-kitchen-vagrant-instance/"
      }
      ,
    
      "2017-05-11-fixing-ubuntu-desktops-wifi-disconnect-issue-on-wake": {
        "title": "Fixing ubuntu desktop&amp;#8217;s wifi disconnect issue on wake",
        "author": "Manthan Dave",
        "category": "",
        "content": "For about more than half the time, when I wake my ubuntu laptop from sleep, I will lose wifi. When I say lose, I mean I lose the wlan0 interface completely.Now this is a problem because I quite like having wifi and normally there seems to be no way to turn it back on from that little wifi menu from the top right corner.After doing some googling, I found a crude yet simple solution. Restart the network-manager service by running the following:sudo service network-manager restartRight before writing this article, I had to run that command and now it all works buttery smooth. Maybe there is a more permanent fix, I will keep an eye out for it and when I can find some time, will dig in to some logs somewhere to actually find the issue but for now, this is satisfactory (meh) to my current needs.",
        "url": "/2017/05/11/fixing-ubuntu-desktops-wifi-disconnect-issue-on-wake/"
      }
      ,
    
      "2017-05-05-download-and-store-a-websites-public-cert-into-a-java-keystore": {
        "title": "Download and store a website&amp;#8217;s public cert into a Java keystore",
        "author": "Manthan Dave",
        "category": "",
        "content": "The other day I had to download a public cert from a web service’s host and store it in my java keystore so that it can be trusted. Here’s what I did:openssl s_client -showcerts -connect www.manthanhd.com:443 &lt; /dev/null 2&gt;/dev/null| sed -n -e '/BEGIN\\ CERTIFICATE/,/END\\ CERTIFICATE/ p' &gt; /tmp/www-manthanhd-com.certsudo keytool -import -file /tmp/www-manthanhd-com.cert -alias wwwmanthanhdcom -keystore /opt/java/jre/lib/security/cacerts -storepass changeitThe first line downloads the public cert from www.manthanhd.com and stores it in /tmp/www-manthanhd-com.cert.Next, we’re using keytool to import that certificate into the Java cacert keystore. I am only using sudo here because Java is installed as root. If in your case its not, you can just use the keytool command without the sudo prefix.Also, on my test box, the java keystore has the default java keystore password which is changeit. Make sure this matches whatever your keystore password is.Last but not least, the alias that the cert is imported against is important because this is what you will have to use to later find it. In this case I’m just using the hostname without any punctuations. This way, I can easily find any cert I want for any host if I need it.Thanks to Jamie Tanna (jvt.me) and Jack Gough (testingsyndicate.com) for their help on this.",
        "url": "/2017/05/05/download-and-store-a-websites-public-cert-into-a-java-keystore/"
      }
      ,
    
      "2017-04-28-minimal-express-server-setup-for-api-development": {
        "title": "Minimal Express server setup for API development",
        "author": "Manthan Dave",
        "category": "",
        "content": "Initialise npm with defaults.npm initCreate your main index.js entrypoint.touch index.jsInstall express, body-parser, morgan and winston packages.npm install --save express body-parser morgan winstonMake your index.js look like this.const express = require(\"express\");const app = express();global.port = process.env.PORT || 3000;global.logger = require(\"winston\");logger.level = process.env.LOG_LEVEL || \"debug\";const bodyParser = require(\"body-parser\");app.use(bodyParser.json());const morgan = require(\"morgan\");app.use(morgan('combined'));app.get(\"/\", function(req, res) {    return res.send({message: \"hello\"});});app.listen(port, function(err) {    if(err) {        logger.error(`Failed to listen on port ${port}.`);        return process.exit(1);    }    logger.info(`Listening on port ${port}.`);});This is probably one of the most light weight node.js configuration that I have ever written for building simple REST web services.In my opinion, this is a good starting point as it makes minimal assumptions about what you might need, letting you add whatever you need minimally on top.",
        "url": "/2017/04/28/minimal-express-server-setup-for-api-development/"
      }
      ,
    
      "2017-04-24-appending-to-crontab-using-a-single-shell-command": {
        "title": "Appending to crontab using a single shell command",
        "author": "Manthan Dave",
        "category": "",
        "content": "Usually to edit crontab for a user, you login as that user and then run:crontab -eThis usually opens up a text editor which then lets you edit the crontab. Once you are done, you save and quit, and this magically updates your crontab.Today I was writing a script that needed to update crontab without any user interaction. After doing some digging, I found this neat way of updating my crontab;(crontab -l ; echo \"* 1 * * 1 /usr/bin/letsencrypt renew &amp;&amp; service nginx restart\")| crontab -The above example is straight out of my shell script which renews my letsencrypt certificate and then restarts the nginx server.",
        "url": "/2017/04/24/appending-to-crontab-using-a-single-shell-command/"
      }
      ,
    
      "2017-04-20-setting-up-an-oauth2-provider": {
        "title": "Setting up an OAuth2 provider",
        "author": "Manthan Dave",
        "category": "",
        "content": "In this post, we’re going to talk about installing and setting up your very own OAuth2 provider. If you have used Facebook or Twitter logins, you’d know that they have their own OAuth2 providers. In reality, those are more than just OAuth2 providers as they also have OpenID Connect on them, however, that will be a post for another day.Why would I want an OAuth2 provider?Well, there are many reasons why you’d want an OAuth2 provider. \tBecause its cool. \tBecause its hip. \tBecause, why not?On a more serious note, if you have a bunch of applications running in your house, you can use your own OAuth2 provider to provide identity and custom authorisations to every app in a way that if one of those apps gets compromised, it won’t take your whole house down. This lets you operate all of your apps in a standard way.Also, who in your family doesn’t want “Sign in via &lt;insert_family_name&gt;” button? :PFor this post, we’re going to use Forgerock’s OpenAM version 13.Install prerequisitesOpenAM runs on tomcat so you need to install that first. We’re going to use OpenAM 13 for this demo along with Tomcat 8 running on Java 8. So the running order is: \tDownload Java 8 JDK \tDownload Tomcat 8 \tDownload OpenAM 13 WAR (web archive)Initial SetupBefore you start tomcat, copy the OpenAM 13 WAR into the tomcat webapps directory. This directory is typically under $TOMCAT_HOME/webapps where $TOMCAT_HOME is the root folder of where tomcat is installed. If you are unsure, $TOMCAT_HOME folder will also contain other directories like bin, conf, lib etc. When you copy the war over, make sure you rename it something memorable (and without any spaces or special characters) like openam.war. Whatever you rename this war to will become the context root of your web server. So if you rename the war file to openam.war, the openam server will become accessible at http://localhost:8080/openam.Start tomcat by running the startup.sh shell script located in $TOMCAT_HOME/bin directory. To ensure that the startup was successful, open catalina.out log file located in $TOMCAT_HOME/logs directory. The war file is relatively large and could take some time to deploy. Make sure the log ends with something like this:INFO [main] org.apache.catalina.startup.Catalina.start Server startup in 80585 msand has a line like:org.apache.catalina.startup.HostConfig.deployWAR Deployment of web application archive /var/tomcat/webapps/openam.war has finished in 80,158 mssomewhere above the startup log message.Manually configure OAuth2 providerSetup OAuth2 ProviderThis is the quick and dirty way of configuring OAuth2 provider if you are using a server that is going to be long lived, as in, not a docker image. Once you do this, make sure you back up the disk to ensure that the settings are preserved.In your web browser, navigate to the following URL:http://localhost:8080/openamGo through the default configuration. Setup the admin account password. Make sure you remember this as there is no way to recover it afterwards if you lose it. I usually have the admin account credentials as follows:Username: amadminPassword: password123// Password simple for readability purposesOnce you have completed the initial setup, it will take you to the login screen. Login with your admin credentials and you will be taken to the OpenAM console. Here, click on the “Top Level Realm /” to configure the realm.At this point, you should be able to see this:[caption id=”attachment_602” align=”aligncenter” width=”700”] OpenAM Realm Overview[/caption]In the common tasks pane, click “Configure OAuth Provider” button. From there, click “Configure OAuth 2.0” button. You should see something like this:[caption id=”attachment_603” align=”aligncenter” width=”700”] Configure OAuth Provider[/caption]On this screen, you can configure your OAuth2 provider however you want. For simplicity reasons, lets just keep everything as default and click “Create” button. For me, this action takes less than a second. Click “OK” on the pop up modal.Now you should be back to the OpenAM console realm overview page.Setup OAuth ClientsIf you have used any public OAuth2 providers like Facebook and Twitter, you should be familiar with OAuth clients. A client is an application that wants to access user data from the OAuth provider. Each client must first authenticate itself using a set of credentials and then request authorisation using scopes to access a set of resources.The first part, authentication, is achieved by issuing a set of credentials which are, usually, a clientId and clientSecret. We need to create a client and then issue a clientId and clientSecret for it to use.From the OpenAM dashboard, click on “Agents” on the left. In the resulting page, click on “OAuth 2.0/OpenID Connect Client” tab on the second tab row from the top.[caption id=”attachment_604” align=”aligncenter” width=”700”] Add OAuth2 Client[/caption]Click “New” under the Agent section. Here, Name refers to the ClientId and Password refers to the ClientSecret. Once you’ve created a strong secret, click “Create”. This will take you back to the “OAuth 2.0/OpenID Connect Client” agents screen. Click into the agent that you just created.Now this is the screen where you can fully configure almost every aspect about your client. You must explicitly whitelist the redirectionUri for your client as well as the scopes that are allowed for the client to access.Ideally you should repeat this for every client application you have such that each application has a unique set of clientId and clientSecret.Add usersCurrently, you won’t have many users in your OAuth2 provider. Out of the box OpenAM will come with an embedded OpenDJ instance. At its core, OpenDJ is an LDAP data store with a REST interface around it.The easiest way to add users is to click on “Subjects” item in the left pane on the OpenAM console realm overview page.Under “Users” section, click on “New”. This should open up a “New User” screen where you can add some details about your user. Also, in case you didn’t notice, your OpenAM instance comes built in with a “demo” user. The “demo” user has its default password set to “changeit”.Now that you have added some users and have a client, you should be able to write an OAuth2 client app that allows you to “Sign In” via your OAuth2 provider instance!",
        "url": "/2017/04/20/setting-up-an-oauth2-provider/"
      }
      ,
    
      "2017-04-15-fixing-docker-service-startup-after-using-an-alternative-graph-driver": {
        "title": "Fixing docker service startup after using an alternative graph driver",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I was playing around with devicemapper docker storage driver the other day. It was quite nice but when I stopped the modified docker daemon and tried to start the docker service in “normal” way, I received the following error:root@carbon:~# service docker startJob for docker.service failed because the control process exited with error code. See \"systemctl status docker.service\" and \"journalctl -xe\" for details.Clearly something had gone wrong. Upon running:systemctl status docker.serviceI found the following line most helpful in fixing the error:Jan 24 22:33:17 carbon dockerd[3753]: Error starting daemon: error initializing graphdriver: /var/lib/docker contains several valid graphdrivers: devicemapper, aufs; Please cleanup or explicitly choose storage driver (-sSo I ran the following command in order to remove my devicemapper driver:rm -rf /var/lib/docker/devicemapperBoom it worked!Alternatively, I could’ve backed it up to tmp instead by running:mv /var/lib/docker/devicemapper /tmp/But I didn’t need any of my images anyway so I removed it.Hope this helps.",
        "url": "/2017/04/15/fixing-docker-service-startup-after-using-an-alternative-graph-driver/"
      }
      ,
    
      "2017-04-13-which-process-is-using-this-port": {
        "title": "Which process is using this port?",
        "author": "Manthan Dave",
        "category": "",
        "content": "Sometimes, I get errors like “address is already in use” but struggle to find out what’s using that port. This happens especially with loads of stuff running in background like docker, vagrant vms, local server instances, ssh tunnels etc.Here’s an easy way to figure out whats running on a certain port:netstat -vanp tcp | grep 8080The second last column refers to the process using that port.For example, running that on my mac:➜ ~ netstat -vanp tcp | grep 8080tcp4 0 0 *.8080 *.* LISTEN 65536 65536 5500 0Indicates that process ID 5500 is using port 8080. Doing a process check on that tells me:➜ ~ ps -ef | grep 5500394661014 5500 5443 0 7:15am ?? 0:42.61 /Applications/VirtualBox.app/Contents/MacOS/VBoxHeadless --comment kitchen-awesomevm --startvm b459e3ea-566b-4184-a109-cf97338958aa --vrde configHope this helps!",
        "url": "/2017/04/13/which-process-is-using-this-port/"
      }
      ,
    
      "2017-04-11-enable-tomcat-debugging": {
        "title": "Enable tomcat debugging",
        "author": "Manthan Dave",
        "category": "",
        "content": "Shutdown tomcat and make sure that it has properly shut down by monitoring the tomcat process.Once it has completely shutdown, export the following environment variables:export JPDA_ADDRESS=8000export JPDA_TRANSPORT=dt_socketHere, we’re setting the jpda address to port 8000. Note this down as you will need this port number in order to connect via your IDE or whatever debugging tool you’re using.Options above will run tomcat as usual so if you want to debug something that happens early on in the lifecycle, you either need to be really quick about attaching your debugger or add the following JPDA option:export JPDA_SUSPEND=yThis will suspend the tomcat startup until you attach your debugger.Next, start tomcat jpda using catalina script:./catalina.sh jpda startAt this stage, you can monitor the log in catalina.out file to make sure everything goes smoothly.Now setup your IDE to connect to tomcat at port 8000. If you are using vagrant as your VM, make sure you are port forwarding 8000 in your network settings. If not, you can always tunnel in:vagrant ssh -- -L 8000:localhost:8000Or if its a remote server, you can just use the normal ssh command:ssh -L 8000:localhost:8000 user@server.comHope this helps!Credits \tConor Restall: For suggesting JPDA_SUSPEND option.",
        "url": "/2017/04/11/enable-tomcat-debugging/"
      }
      ,
    
      "2017-03-30-forgerock-unsummit-bristol-2017-notes": {
        "title": "Forgerock UnSummit Bristol 2017 Notes",
        "author": "Manthan Dave",
        "category": "",
        "content": "10:00 Kickoff[caption id=”attachment_580” align=”aligncenter” width=”700”] Finalised agenda of the day @ Forgerock UnSummit Bristol 2017[/caption]Forgerock on DockerStart off with base docker image containing OpenAM. This docker image has a folder for config which can be volume mounted to allow loading custom config. During runtime, it detects the config and runs it against amster (ssoadm replacement).Autonomous instances will allow clustering of OpenDJs so that the OpenAM servers don’t have to know about individual OpenDJs.ssoadm will for each command, create JVM, load context, run the command, destroy context and then finally destroy JVM. This slows down the tool by a significant amount. The new tool amster supports sessions where once fired, it creates a session and then any command can be run in that session more effectively than ssoadm.All future versions of OpenAM will be able to import the configuration from their previous versions. So, OpenAM 15 will know out of the box how to import config from OpenAM 14.New OpenDJ proxy allows orchestration of OpenDJ instances in a shard fashion. Helps with elastic scaling.Docker expects logs to be in stdout. OpenAM has a new audit framework (common framework across all products) which can be used to send out logs to different output endpoints like stdout, splunk etc.Cloud ReadinessUse ELB / HA Proxy in front of OpenDJ instances within a region. This helps openam instances avoid knowing individual opendj servers that they have access to. Potentially use one ELB pe region, then use Route 53 to use geo-location to resolve to relavent ELB. Use replication manager to enable replication amongst those OpenDJ instances. General consensus on avoiding ELB as it has unpredictable behaviour at TCP level at times (struggles to handle big spikes of load since it needs to be prewarmed).Forgerock RoadmapOpenAM 14/14.5OpenAM 14.5 brings stateless architecture to authentication modules. This means that the authentication chain can be used across openam servers without having to have sticky sessions.Amster brings remote management to OpenAM. Unlike ssoadm where you had to be on the server in order to run it, amster can run remotely as it uses the openam’s restful APIs.Support for authentication trees (directed asyclic graphs) in order to enable consumers use complex flows.(14.0) Push authentication allows authenticating mid authentication chain from mobile directly from the user. This can be done using something like Touch ID etc.(14.5) Push authorisation allows authorising of specific transaction or activity mid session directly from the user. This can be done using something like Touch ID etc.OpenDJ 4/4.5Proxy services will allow horizontal scalability. This is done by replicating the sharding architecture like the NoSQL databases.      Loadbalancing and failover        High availability        Access control  Platform improvements will ease continuous delivery using tools like amster and being able to use kubernetes on top of docker containers. The release will come with official base docker containers which can be extended as needed.      DevOps        Docker containers  Database  Memory BackendSecurity  Database encryptionStateless application architectureTypically during authentication, when a token is issued, the token itself is a reference to its details that are stored in OpenDJ. This does not work at scale because of the eventual consistency with OpenDJ.With stateless sessions, these details are stored in the token itself and when the token is presented, it is unpacked and validated (decrypted and signature verified).However, this presents a new problem. What happens when the session is invalidated explicitly, outside of the session timeout? Well, this requires a black list. While the black list will only contain tokens that have been explicitly invalidated outside of their timeouts, once the timeout passes, they are removed. This keeps things tidy.OpenAM UI CustomizationSetup themes in ThemeConfiguration.js file within the XUI war.",
        "url": "/2017/03/30/forgerock-unsummit-bristol-2017-notes/"
      }
      ,
    
      "2017-01-23-generating-random-passwords-from-linux-command-line": {
        "title": "Generating random passwords from linux command line",
        "author": "Manthan Dave",
        "category": "",
        "content": "Managing production passwords isn’t a trivial task. I was trying to deploy a containerized app the other day that had a database deployed with it. During the deployment, I was trying to find an easy way to set a secure password. I didn’t want anyone to know the password because I wanted only the application to know it and no one else. Also, the container was setup in a way that the database cannot be accessed from the outside world.So instead of hard-coding the password, after doing some research, I used the following command:&lt; /dev/urandom tr -dc _A-Z-a-z-0-9+= | head -c${1:-32};echo;This might look like some gibberish but it uses linux’s /dev/urandom to generate some randomness and extracts human readable characters like alphabets, numbers and common symbols like underscores, hyphens, pluses and equals.Try it! it works!Now, using clever bash interpolation syntax you can embed this password throughout your script in a secure way.",
        "url": "/2017/01/23/generating-random-passwords-from-linux-command-line/"
      }
      ,
    
      "2017-01-19-setting-up-corporate-proxy-on-docker-for-mac": {
        "title": "Setting up corporate proxy on Docker for mac",
        "author": "Manthan Dave",
        "category": "",
        "content": "Working with Docker on corporate proxy is a painful experience. Mainly because there aren’t many guides available to do it. Finally after banging my head on the desk for a long time, my friend and colleague at https://nextmetaphor.io showed me how to do it.First of all, fire up your terminal and open up docker tty in screen.screen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/ttyIf you see a blank screen, press enter. You should see a prompt.Make sure you are in the docker VM by typing the hostname command. You should see the response as moby. If your response is other than that, try that screen command again./ # hostnamemobyNow we want to view docker’s routing table. This is because we’ll need to find out the IP address of the host machine that is running the proxy. This is specific to my setup where I have a charles proxy server running on my machine which proxies to the remote corporate proxy./ # netstat -rnKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         192.168.65.1    0.0.0.0         UG        0 0          0 eth0172.17.0.0      0.0.0.0         255.255.0.0     U         0 0          0 docker0192.168.65.0    0.0.0.0         255.255.255.240 U         0 0          0 eth0Get the gateway entry for 0.0.0.0. In my case this is 192.168.65.1. That’s the IP for the host machine running docker. For the proxy running on your local machine, just map it to the port. My charles server is running on port 8099 so my proxy will be:http://192.168.65.1:8099Close screen by pressing Ctrl + a \\ key. Once you’ve exited, open up docker for mac preferences.Go to the “advanced” tab and fill out your proxy settings.Hit apply and restart when done!Your Docker for Mac should now work harmoniously with your proxy!",
        "url": "/2017/01/19/setting-up-corporate-proxy-on-docker-for-mac/"
      }
      ,
    
      "2017-01-02-codeeval-fizz-buzz-java": {
        "title": "CodeEval: Fizz Buzz (Java)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Finally managed to get some spare time in order to do this. Helped be clear my head a bit. Here’s my quick 1 minute solution to CodeEval’s fizz buzz problem in Java:/* Sample code to read in test cases:*/import java.io.*;public class Main {    public static void main (String[] args) throws IOException {        File file = new File(args[0]);        BufferedReader buffer = new BufferedReader(new FileReader(file));        String line;        while ((line = buffer.readLine()) != null) {            line = line.trim();            final String[] inputStringArray = line.split(\" \");            final int divisor1 = Integer.parseInt(inputStringArray[0]);            final int divisor2 = Integer.parseInt(inputStringArray[1]);            final int countingLength = Integer.parseInt(inputStringArray[2]);                        for(int i = 1; i &lt;= countingLength; i++) {                final StringBuilder responseBuilder = new StringBuilder();                if(i % divisor1 == 0) responseBuilder.append(\"F\");                if(i % divisor2 == 0) responseBuilder.append(\"B\");                if(responseBuilder.length() == 0) responseBuilder.append(i);                System.out.print(responseBuilder.toString() + \" \");            }                        System.out.println();        }    }}Upon submitting, CodeEval marked my solution as unique (go figure!) which was a bit weird. However, I thought I could do better. So, here’s a second approach using lambdas:/* Sample code to read in test cases:*/import java.io.*;import java.util.Arrays;public class Main {    public static void main (String[] args) throws IOException {        final File file = new File(args[0]);        final BufferedReader buffer = new BufferedReader(new FileReader(file));        String line;        while ((line = buffer.readLine()) != null) {            line = line.trim();            final int[] inputs = Arrays.stream(line.split(\" \")).mapToInt(Integer::parseInt).toArray();            final int divisor1 = inputs[0];            final int divisor2 = inputs[1];            final int countingLength = inputs[2];                        for(int i = 1; i &lt;= countingLength; i++) {                final StringBuilder responseBuilder = new StringBuilder();                if(i % divisor1 == 0) responseBuilder.append(\"F\");                if(i % divisor2 == 0) responseBuilder.append(\"B\");                if(responseBuilder.length() == 0) responseBuilder.append(i);                                responseBuilder.append(\" \");                System.out.print(responseBuilder.toString());            }                        System.out.println();        }    }}This takes slightly longer (3-5 ms) and uses more memory.",
        "url": "/2017/01/02/codeeval-fizz-buzz-java/"
      }
      ,
    
      "2016-12-06-blue-green-deployments-on-aws": {
        "title": "Blue-green deployments on AWS",
        "author": "Manthan Dave",
        "category": "",
        "content": "Before we do this, make sure that you have a service that you want to deploy. To keep things simple, I followed the spring boot tutorial on making a restful web service. It was quick and the app worked like a charm. As usual, I went a bit extra and made my app return a stubbed list of users. You don't have to. Make sure you have a /healthcheck endpoint and another endpoint that you can test with. In my case, I have /users which returns a list of users.All righty then. Lets get a high level overview of what things are and how they are going to work. But before we do that, lets go through a quick real-ish life scenario.Say you have a service that you have deployed onto AWS. Now you have a newer version of that service that you'd like to test. Since you never know if something works without actually trying it out, normally, after exhaustive testing in staging and other environments, you'd deploy that service into production to all your users. But ah ha! That one guy in your team forgot that one test case which made it blow up which means every single user of yours is now seeing error pages everywhere. This is bad so you roll it back to the previous version. Doesn't sound too bad yet but by the time you do this, you'd have lost a couple of hours in time which would translate into actual money lost to the company which could eventually make a dent in your end of the year bonus.But what if there was a better way. What if you could've deployed that change to only one of your hundred servers. This would mean that if it does blow up, it would've only affected one percent of your users (or rather more accurately one percent of servers). This better way is the blue-green deployments.Blue-green deployments (a.k.a A/B deployments) are used quite often as part of software development lifecycle to test out a certain piece of functionality in a live production environment for a select group of users. Basically what I said but in a fancier language.In order to do this, we need to break down the AWS infrastructure our service depends on. In most cases, the following resources are used: \tElastic Load Balancing (ELB) \tAutoscaling Groups (ASG) \tLaunch ConfigurationNote that I have not explicitly called out Elastic Compute (EC2) instances in the above list of resources. This is mainly because we are implicitly using them, via the launch configuration but are not explicitly creating a resource of that type. Also, at this level, blue-green does not have anything to do with databases which is why we'll leave it out this time.Like most people, you would have a single cloud formation script with all the above resources in it. Whilst this is great, to do blue-green deployments, we'll need to break it down. You could go as fine as one cloud formation script per resource, however, I have them down into two scripts; first for ELB and second for ASG as well as launch configuration. I tried breaking them down into three scripts but something about that didn't quite feel right as I felt that ASG and launch configuration are quite tightly coupled as is.The way this is going to work is that we'll deploy an ELB using the ELB cloud formation scripts across two availability zones. In my case, this is eu-west-1a and eu-west-1b. This ELB will have a unique LoadBalancerName attribute. In our case, since we are only doing single region, this will be  something like UserServiceLB.Once that is deployed, we will then deploy the blue service stack using the second cloud formation script which will deploy our launch configuration as well as a auto scaling group. This auto scaling group will need to be attached to a load balancer. Yep, you guessed it. We'll specify the previously defined unique LoadBalancerName here. So, say for instance UserServiceV1 (blue), this will look like following:If, say now we want to deploy V2 (green) of the service, we'll have to deploy another stack with different launch configuration but still have the autoscaling group pointing at the same ELB. This might look like following:Now the ELB will send some of the traffic to the new green (color not indicative of health) instances. Because this is only some of the traffic, if something does go wrong, it is only impacted to a small subset of customers. Also, if it all goes horribly wrong (murphy's law not intended), we can simply delete the green stack and it will all happily keep ticking.If the new green stack is proving its worth, we can make changes to the autoscaling group to increase the number of instances while at the same time making changes to the blue stack to decrease the number of its instances. As you can see, all this can quite easily be automated.Now lets get on with the how. These are the scripts that I used.elb.json:{  \"Parameters\": {    \"LoadBalancerName\": {      \"Type\": \"String\",      \"Description\": \"Name of this loadbalancer. This is the name that you must specify when attaching an autoscaling group.\"    }  },  \"Resources\": {    \"UserServiceELB\": {      \"Type\" : \"AWS::ElasticLoadBalancing::LoadBalancer\",      \"Properties\": {        \"LoadBalancerName\": {\"Ref\": \"LoadBalancerName\"},        \"SecurityGroups\": [\"sg-xxxxxxx\"],        \"AvailabilityZones\": [\"eu-west-1a\", \"eu-west-1b\"],        \"Listeners\" : [ {          \"LoadBalancerPort\" : \"80\",          \"InstancePort\" : \"8080\",          \"Protocol\" : \"HTTP\"        } ],        \"HealthCheck\" : {          \"Target\" : \"HTTP:8080/healthcheck\",          \"HealthyThreshold\" : \"3\",          \"UnhealthyThreshold\" : \"5\",          \"Interval\" : \"30\",          \"Timeout\" : \"5\"        },        \"Tags\": [          {\"Key\": \"Name\", \"Value\": {\"Ref\": \"LoadBalancerName\"}}        ]      }    }  }}This is the one that actually deploys the load balancer. Make sure that you replace the security groups with one of yours or else the script won't work.So lets quickly examine what this script is doing. This one deploys a single resource of type LoadBalancer. This is a AWS Classic Load balancer. Why not V2? Well, this is what we need for now. I think we might need V2 when we do containerised deployments. I'll cover that some another time.Listeners section indicates that the load balancer is listening on port 80 while forwarding requests to port 8080 on instances. If your sample app is listening on a different port, other than 8080, you will have to change the value for InstancePort attribute.The HealthCheck section lets the loadbalancer know what to health check on. In this case, it is calling /healthcheck endpoint on my app. This will let it know whether or not the instance is listening correctly. The URL doesn't need to be /healthcheck, it can be anything as long as it returns 200.If you upload this cloud formation to the AWS web console, you will be prompted to enter a value for the LoadBalancerName parameter (as defined in the Parameters section). Whatever value you enter, make a note of it.Next, we'll deploy the app itself.app.json:{  \"Parameters\": {    \"ASGMinSize\": {      \"Type\": \"Number\",      \"Default\": \"1\",      \"Description\": \"Minimum number of instances required for this app's autoscaling group\"    },    \"ASGMaxSize\": {      \"Type\": \"Number\",      \"Default\": \"1\",      \"Description\": \"Maximum number of instances required for this app's autoscaling group\"    },    \"LoadBalancerName\": {      \"Type\": \"String\",      \"Description\": \"Name of the loadbalancer to attach this app's autoscaling group to\"    }  },  \"Resources\": {    \"UserServiceScalingGroup\": {      \"Type\" : \"AWS::AutoScaling::AutoScalingGroup\",      \"Properties\" : {        \"AvailabilityZones\": [\"eu-west-1a\", \"eu-west-1b\"],        \"LaunchConfigurationName\" : { \"Ref\" : \"UserServiceLaunchConfig\" },        \"MinSize\" : {\"Ref\": \"ASGMinSize\"},        \"MaxSize\" : {\"Ref\": \"ASGMaxSize\"},        \"LoadBalancerNames\" : [ {\"Ref\": \"LoadBalancerName\"} ]      }    },    \"UserServiceLaunchConfig\": {      \"Type\" : \"AWS::AutoScaling::LaunchConfiguration\",      \"Properties\": {        \"KeyName\" : \"aws-keypair\",        \"ImageId\": \"ami-9398d3e0\",        \"SecurityGroups\": [\"sg-xxxxxxx\"],        \"InstanceType\": \"t2.nano\",        \"IamInstanceProfile\": \"arn:aws:iam::999987541517:instance-profile/UserServiceRoles-UserServiceInstanceProfile-4D8TE56SDVZM\",        \"UserData\"       : { \"Fn::Base64\" : { \"Fn::Join\" : [\"\", [          \"#!/bin/bash -xe\\n\",          \"yum update -y\\n\",          \"yum remove -y java-1.7.0-openjdk\\n\",          \"yum install -y java-1.8.0-openjdk-headless.x86_64\\n\",          \"aws s3 cp s3://manthanhd-buildartifacts/services/userservice/1.0.0/user-service-1.0.0.jar .\\n\",          \"/bin/env java -jar user-service-1.0.0.jar\\n\"        ]]}}      }    }  }}This will deploy the launch configuration as well as the autoscaling group. Again, make sure you replace the SecurityGroups attribute with value of your own or else the script won't work. Also, note that the UserData is configured to deploy a java application because, as I mentioned in the beginning, I am deploying a Spring Boot service. If you are using something else other than a java based service, you might have to change it to suit your needs.Let's quickly go through that cloud formation script. As you can see, we are deploying one autoscaling group and one launch configuration. The auto scaling group defines the min size, max size, name of the load balancer as well as the name of the launch configuration. The first three parameters are referenced from the parameters section whereas the launch configuration is a live reference from the launch configuration resource below. Also, note that we are deploying the autoscaling group into two availability zones within eu-west region, same as the load balancer.The launch configuration is rather simple. It basically instucts AWS as to how to launch the instance. The imageId attribute is the default AWS AMI. I've made the KeyName generic as in aws-keypair. It doesn't need to have a value, however, it is useful when you need to debug something.The launch configuration also attaches instance profile to the launched EC2 instance. In my case, this is a restricted instance profile that only allows the EC2 instance to get and list objects in a specific folder within my S3 bucket.If you refer back to the architecture diagram, you'll notice that we are deploying the launch configuration and autoscaling group separately to the load balancer. During the deployment of the autoscaling group, we are specifying the load balancer to attach the autoscaling group to. This is where you need to use the LoadBalancerName attribute that you previously noted down.Once you have roles and security groups in place with updated scripts, go ahead and deploy the elb.json as well as app.json. For app.json, make sure you suffix the stack name with V1 so that we know that this is the first deployment.This will create three instances under the elb. Now deploy the app.json again but this time with only one instance. Lets pretend that this is a new version. Make sure you suffix the stack name with V2. Once deployed, you will see that your new instance has been attached to the same ELB. Really cool right? Now, you could just update that existing V1 stack. Go back to app.json and change the instance count to 2 and update the V1 stack. Now you should see AWS shrink that autoscaling group to 2 from 3 whilst V2 is still 1. You get the idea.All this, can of course be quite easily automated using the AWS's CLI and APIs.Hope this has helped drive your blue green deployments on AWS. I have been piecing together notes for a while and finally managed to drive a PoC out. You can find the code here:https://github.com/manthanhd/aws-blue-green-java",
        "url": "/2016/12/06/blue-green-deployments-on-aws/"
      }
      ,
    
      "2016-12-05-gradle-multi-module-projects": {
        "title": "Gradle multi module projects",
        "author": "Manthan Dave",
        "category": "",
        "content": "Recently I’ve had to deal with a lot of gradle projects, specifically the multi-module ones. It’s dead easy to setup, however, every time I do it, I have to refer to one of my old projects to see the layout. So, hopefully, while this document will certainly help future me, I hope it is of help to you too.First things first, create a project directory. We will refer this directory as ‘project root directory’ in future. For all the awesome things that I do, we’ll call this one, drum roll please … awesomeproject.mkdir awesomeprojectcd awesomeprojectOnce in that directory, lets create the build.gradle file at the project root with the following content:// build.gradleapply plugin: 'distribution'Alongside this, create a settings.gradle file but leave it blank for now.touch settings.gradleNow create your sub-module project directories. In this example, we’ll create two directories, awesomeproject-scripts which will hold our deployment scripts and awesomeproject-service which will contain our actual java code.mkdir awesomeproject-scripts awesomeproject-serviceTime to create gradle files for your sub-modules. We won’t go into the detail of what the gradle files will actually contain, however, for semi-completeness, we’ll just apply some plugins as a marker to indicate the type of sub-module projects.// awesomeproject-scripts/build.gradleapply plugin: 'distribution'// awesomeproject-service/build.gradleapply plugin: 'java'As you can see from above, our awesomeproject-scripts/build.gradle file applies the distribution plugin to indicate that it will need to archive its contents while the awesomeproject-service/build.gradle applies the java plugin to indicate that it will be compiling its java source.Amazing! Now we just need to tell our root project to include these two sub-modules. This is simple. You know we left that settings.gradle file in the project root blank? Well, lets change that.// settings.gradleinclude 'awesomeproject-scripts', 'awesomeproject-service'Here, we are telling the root gradle project to include the sub-modules. Gradle will then expect build.gradle files within these directories and will execute jobs. It’s pretty clever so if you’ve got a module that needs to execute one of the other sub-modules first, it will run it in the correct required order.More detailed documentation on this is available on gradle’s website, however, this should get you up and running quickly.",
        "url": "/2016/12/05/gradle-multi-module-projects/"
      }
      ,
    
      "2016-11-11-getting-started-with-chat-bots-using-talkify": {
        "title": "Getting started with chat bots using Talkify",
        "author": "Manthan Dave",
        "category": "",
        "content": "Chat bots have always seemed so complex. They process natural language from text so it must be hard right? After all, how can you make sense of loose words into computer instructions and then back? It must be hard.Well, it is hard. Kinda. A lot has already been solved around natural language processing so the amount you have to do to get started has reduced by significant amount. Tools are already there, you just have to use them.I used those tools and still found it to be difficult. I wanted to make the process of building chat bots as easy as getting started with web development. So, I built Talkify.Takify is an Open Source framework for building chat bots. It is written in node.js and here’s how you can build your very own chat bot in a couple of minutes.We’ll be building a chat bot that I like to call “sidekick”. This is a simple bot that tells you knock knock and chuck norris jokes.The template for this already exists on my GitHub so lets just clone it for now. Assuming you have git command line installed, run:git clone https://github.com/manthanhd/talkify-example-sidekick.gitThis will create a folder called talkify-example-sidekick within your current working directory. Go into that folder using the cd command.You’ll now need to install module dependencies. This should take a couple of seconds depending on your internet connection. Run the following command to tell npm to do this for you.npm installNow if you list your current directory (using ls if using linux/mac or dir for windows), you’ll notice two JavaScript files. These are index.js and cli.js. The index.js file contains the main bot code while the cli.js  provides an interface to talk the bot via the command line interface. For this tutorial, lets run the cli.js file using node.node cli.jsThis should give you a prompt.C:\\Users\\manthanhd\\Documents\\projects\\sidekick&gt;node cli.js BOT&gt; Ready.YOU&gt;Give it a try. Ask the bot to tell you a knock knock joke. Or ask it to tell you a Chuck Norris joke. You should get responses like so:YOU&gt; tell me a knock knock joke BOT&gt;  Knock, knock. BOT&gt;  Who’s there? BOT&gt;  Rufus. BOT&gt;  Rufus who? BOT&gt;  Rufus the most important part of your house.YOU&gt; tell me a chuck norris joke BOT&gt;  There are only two things that can cut diamonds: other diamonds, and Chuck Norris.If you keep asking for knock knock jokes, it will eventually run out. Try it!Awesome work! You’ve setup the bot correctly!Now lets look under the hood inside the index.js to better understand whats going on. First couple of lines in this file are a bunch of requires.const knockKnockJokes = require('knock-knock-jokes');const cnApi = require('chuck-norris-api');const talkify = require('talkify');This is like an “import” statement where its initialising libraries like the knock-knock-jokes, chuck-norris-api and talkify.The next couple of lines are importing template functions from Talkify for us to use later on in the code.const Bot = talkify.Bot;const BotTypes = talkify.BotTypes;const SingleLineMessage = BotTypes.SingleLineMessage;const TrainingDocument = BotTypes.TrainingDocument;const Skill = BotTypes.Skill;If the previous require statements were loading libraries, this is equivalent to loading specific things that we will be needing from those libraries.In the next line we are initialising the bot itself using the previously loaded Bot function.const bot = new Bot();The constructor for Bot can accept an optional configuration object but to keep things simple, we’re just going to use default configuration.It might look like a lot is happening in the following lines but in essence, we’re simply training the bot to learn a couple of words and associate them with topics. To do that, we are using the trainAll function of the bot and are passing in an array of objects as well as a callback function. Each of these objects are of TrainingDocument type.bot.trainAll([    new TrainingDocument('knock_joke', 'knock'),    new TrainingDocument('knock_joke', 'knock knock'),    new TrainingDocument('chuck_norris_joke', 'chuck norris'),    new TrainingDocument('chuck_norris_joke', 'chuck'),    new TrainingDocument('chuck_norris_joke', 'norris'),    new TrainingDocument('chuck_norris_joke', 'chuck norris joke'),], function () {    console.log(' BOT&gt; Ready.');});As you can see, each TrainingDocument object accepts two parameters. The first parameter is the topic name and the second is text that the bot will learn to associate with the previously mentioned topic.The callback function that is passed to the trainAll function is executed when the training for the bot has finished. Here, we’re just printing out a message indicating that the bot is ready.Next, we’re defining two skill objects by utilising the Skill constructor.const kJokeSkill = new Skill('my_knock_knock_joke_skill', 'knock_joke', function (context, request, response) {    if (!context.kJokes) {        context.kJokes = [];    }    let newJoke = knockKnockJokes();    let counter = 0;    while(counter &lt; 11 &amp;&amp; context.kJokes.indexOf(newJoke) !== -1) {        newJoke = knockKnockJokes();        counter++;    }    if(counter === 11) {        return response.send(new SingleLineMessage('Sorry I am out of knock knock jokes. :('));    }    context.kJokes.push(newJoke);    return response.send(new SingleLineMessage(newJoke));});const cJokeSkill = new Skill('my_chuck_norris_joke_skill', 'chuck_norris_joke', function(context, request, response) {    return cnApi.getRandom().then(function(data) {        return response.send(new SingleLineMessage(data.value.joke));    });});As you can see, this accepts three parameters, unique name of the skill, topic to associate the skill with and a function to execute as part of that skill.This function is called an apply function and it must accept at least three parameters. These are context, request and response.The context parameter is the context that the bot loads for you automatically. This is request driven context and is associated with an end user.  Think of this as how a web session works.The request parameter contains metadata about the request like the actual raw sentence in the request, topic resolution confidence level etc.The response parameter contains a bunch of methods that you can use to set states and respond to the user. More information on this entire section is available in the Skills wiki page.Within the skill functions we are using the knock knock joke and the chuck norris library to get jokes that the bot can respond with.Next two lines is where we are making the bot aware of the skills by adding them to it.bot.addSkill(kJokeSkill);bot.addSkill(cJokeSkill);module.exports = bot;The last line exports the bot so that it can be used by other files within this project. We are doing this so that we can separate the bot logic from the logic of getting the raw request from the user and then responding with it. The latter logic in this case can be found from cli.js file.This means that you can quite easily replace the cli.js file with a web hook from Facebook messenger, Apple iMessage, Skype or even a web request from an iPhone app that works with Siri to enable voice.So that’s it. That was a quick whistle stop guide to writing your own first chat bot. Hope you enjoyed it.The project is available on my GitHub page: https://github.com/manthanhd/talkifyIf you want to request a feature or file a bug, please feel free to raise an issue on the GitHub repository link above. I am also open to contributors so feel free to fork the repository and contribute.Just for reference, here’s the NPM package link: https://www.npmjs.com/package/talkify",
        "url": "/2016/11/11/getting-started-with-chat-bots-using-talkify/"
      }
      ,
    
      "2016-10-19-why-are-my-expectjs-spies-blocking-calls": {
        "title": "Why are my ExpectJS spies blocking calls?",
        "author": "Manthan Dave",
        "category": "",
        "content": "While I love writing tests in JavaScript, it is sometimes incredibly painful to debug through the asynchronous test code. Today, a weirdness with ExpectJS happened. I had the following test code:var contextStore = {    put: function (id, context, callback) {        return callback(undefined, context);    },    get: function (id, callback) {        return callback(undefined, undefined);    }};var contextStore_putSpy = expect.spyOn(contextStore, 'put');var contextStore_getSpy = expect.spyOn(contextStore, 'get');var bot = new Bot({contextStore: contextStore});return bot.resolve(123, \"Hello. Hi\", function (err, messages) {    if (err) return done(err);    expect(contextStore_putSpy).toHaveBeenCalled();    expect(contextStore_getSpy).toHaveBeenCalled();    return done();});Simple right? Nope. Some how my callback was not being called which was preventing the test from finishing which in turn caused a test failure. I spent good amount of time debugging step by step and finally found the issue.My fake contextStore was not being called. Hmm. How can this happen? I’ve defined functions that do respond asynchronously and have set spies on them as well. If anything, my spies should tell me that the function has been called!Nothing could be further away from the truth! Well, you see, spies work differently in Java than in JavaScript. While using Mockito with Java, I’ve found that Spies never block the calls on the object that they are spying on unless explicitly told to. However, this behaviour different in JavaScript. ExpectJS blocks the calls to spies unless told otherwise!At the end, solution was simple. I changed lines 11 and 12 to:var contextStore_putSpy = expect.spyOn(contextStore, 'put').andCallThrough();var contextStore_getSpy = expect.spyOn(contextStore, 'get').andCallThrough();Boom! And it worked!",
        "url": "/2016/10/19/why-are-my-expectjs-spies-blocking-calls/"
      }
      ,
    
      "2016-10-16-aws-cloud-formation-describe-permission-error": {
        "title": "AWS cloud formation describe* permission error",
        "author": "Manthan Dave",
        "category": "",
        "content": "So you know sometimes, its difficult to work with AWS cloud formation scripts. On surface, errors are seemingly random and unrelated to the script. This is what happened today. I wrote an awesome parameterised cloud formation script. I was quite proud of it, mainly because most of my parameters were typed. This mean that the parameters like, ec2_security_groups had a type of List&lt;AWS::EC2::SecurityGroup::Id&gt;. This not only makes it easier to work with that cloud formation script from the AWS console, but also makes it very easy to work with from a CI/CD pipeline as if those parameters are invalid, the script will fail instantly, instead of waiting for resources to deploy.However, while doing this, I completely missed the fact that in order for cloud formation to validate your input parameters, it needs to look them up first. What IAM policy permission does a resource lookup need? describe!For once, my IAM role had tightly restricted permissions and because of this I had to go on to expand them slightly to allow for describe permissions.I kept getting this error about the role not having the describe permission and I kept wondering why it needed that. Well now you know too!",
        "url": "/2016/10/16/aws-cloud-formation-describe-permission-error/"
      }
      ,
    
      "2016-09-14-introduction-to-docker": {
        "title": "Introduction to Docker",
        "author": "Manthan Dave",
        "category": "",
        "content": "Deploying applications is a complex task. You have to create some VMs, be it on DigitalOcean or AWS, download and install necessary prerequesites and then deploy your application. It would be easy if it were to end there, however, it doesn't.Following this you have application maintenance which includes deploying updated versions of your application in addition to other things like bug fixes. And this is where the real complexity starts. Your updated version might need another dependent application to be installed which in turn might need a version of some random tool to be upgraded. You did what was necessary but then you find out that the deployment of the updated application failed because you forgot to do that one null check in some corner of your application. So frantically you download the previous version of your application and try to restore it only to find that it doesn't work anymore since you upgraded that random tool to support your newer application.While all this is happening either your entire application is unavailable because your application is single instance or if it is indeed multi-instance split by a loadbalancer, all the other instances are being stressed out because one of the node is down.And now you are thinking. Well, there has to be a better way.Well my friend, there is.*queue star wars intro soundtrack*Docker.Docker allows you to create and deploy application containers. What is an Application Container you ask? Well, you can think of an application container like a VM but unlike a fully fledged VM, its the bit that surrounds your application. So in essence, instead of it being a virtual machine, it is a virtual container for your application. This allows application containers to start up within seconds while a virtual machine could take minutes. Also, application containers take up much less RAM as they don't have to load entire operating system in memory but only the bits that surround your application.So how can these amazing and fast application containers help simplify your deployments? Here's how.Because creating and destroying application containers is a inexpensive process, both in terms of compute resources and time, it encourages a philosophy of disposable infrastructure. The idea is that instead of creating your infrastructure and taking care of it for its lifetime, you create it only when you need it and then destroy it when its not needed. Also, because all containers emerge from their respective Dockerfile(s), which is code, they can be versioned along side your actual application. This means that if you roll back to a previous version, you will deploy a docker container for that application using that version of the Dockerfile. Also, because the container contains your project and all of its dependencies, its self contained and can be stood up or torn down without any impact on any of the existing versions.We've been on about Dockerfile for a while now so before we go any further lets see what it actually looks like. Here's an example:FROM ubuntu:14.04RUN apt-get update -yRUN apt-get install -y openjdk-7-jre-headlessADD build/libs/*.jar /appEXPOSE 8080ENTRYPOINT java -jar /app/*.jarThat is a simple Dockerfile that deploys a Spring Boot application in a ubuntu based container. Briefly, the above Dockerfile defines a container image. The first line defines that our image is FROM (based on) ubuntu 14.04 base image. Upon creating the image, it RUNs apt-get update -y  and apt-get install -y openjdk-7-headless commands. In addition to that, it adds (read copies) *.jar file from the present working directory to /app folder to make it available in the container. Also, because we know that the application is going to run on port 8080, we're EXPOSE(ing) (defining) that port so that it can be accessed from outside the container. Finally, we're defining the ENTRYPOINT (command that docker will execute when the container is started) as a java -jar command to execute our Spring Boot application.I'll go into the details about all the sections that comprise a Dockerfile some other time but for now, the thing to take away is that a Dockerfile forms basis of your container and is used to define what your application container image looks like. The image can then be used to create and run the container.I am using a custom spring boot application, however, if you follow the REST tutorial on Spring Boot Tutorials website, you should end up with the same project as me. Within your project, make sure that the Dockerfile is located at the root of the project (in same directory as the src and build folders).So once you have a Dockerfile, you can run the following command to create your image. Make sure that your Dockerfile is called exactly as Dockerfile and you are executing the following command from the same folder as the Dockerfile.docker build -t bootdemo2 .The above command builds our image and tags it (defined by -t option) with bootdemo2 tag. When the version (tag) is unspecified, docker assumes latest. However, if you do want to specify one, you just need to replace bootdemo2 with bootdemo2:1.0 or rather, more generically, bootdemo2:&lt;version&gt;. So building the first version of the application would be like so:docker build -t bootdemo2:1.0 .And then subsequently, the next minor version would be like:docker build -t bootdemo2:1.2 .In this case, just leave the tag blank to allow it to default to latest.If everything went OK, you'll be able to see your image listed within the list of docker images on your machine. This can be viewed using the following command:docker imagesYou should see something like this in your output:REPOSITORY              TAG                 IMAGE ID            CREATED             SIZEbootdemo2               latest              4150e61520b2        27 hours ago        339.3 MBAmazing! You've created your first Docker image. Hurrah! Lets run it to start a container off of that image. Simply run:docker run -p 8080:8080 --name myspringbootapp --rm -it bootdemo2You should see output like following:  .   ____          _            __ _ _ /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\ \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )  '  |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot ::        (v1.4.0.RELEASE)2016-09-14 15:02:01.125  INFO 6 --- [           main] c.manthanhd.eventsmgmtapis.Application   : Starting Application on 4c6454648246 with PID 6 (/project/build/libs/events-0.1.0.jar started by root in /)2016-09-14 15:02:01.129  INFO 6 --- [           main] c.manthanhd.eventsmgmtapis.Application   : No active profile set, falling back to default profiles: default2016-09-14 15:02:01.265  INFO 6 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5b8ed9b6: startup date [Wed Sep 14 15:02:01 UTC 2016]; root of context hierarchy2016-09-14 15:02:03.278  INFO 6 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)2016-09-14 15:02:03.294  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat2016-09-14 15:02:03.296  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.42016-09-14 15:02:03.424  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext2016-09-14 15:02:03.425  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2163 ms2016-09-14 15:02:03.591  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: 'dispatcherServlet' to [/]2016-09-14 15:02:03.598  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]2016-09-14 15:02:03.598  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]2016-09-14 15:02:03.599  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]2016-09-14 15:02:03.599  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]2016-09-14 15:02:04.003  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5b8ed9b6: startup date [Wed Sep 14 15:02:01 UTC 2016]; root of context hierarchy2016-09-14 15:02:04.087  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/events],methods=[GET]}\" onto public org.springframework.http.ResponseEntity&lt;com.manthanhd.eventsmgmtapis.events.models.EventList&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.getEvents()2016-09-14 15:02:04.088  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/events/{id}],methods=[GET]}\" onto public org.springframework.http.ResponseEntity&lt;com.manthanhd.eventsmgmtapis.events.models.Event&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.getSingleEvent(java.lang.String)2016-09-14 15:02:04.088  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/events],methods=[POST]}\" onto public org.springframework.http.ResponseEntity&lt;com.manthanhd.eventsmgmtapis.events.models.Event&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.createNewEvent(com.manthanhd.eventsmgmtapis.events.models.Event)2016-09-14 15:02:04.088  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],methods=[GET]}\" onto public org.springframework.http.ResponseEntity&lt;java.lang.String&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.error()2016-09-14 15:02:04.091  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error],produces=[text/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)2016-09-14 15:02:04.091  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[/error]}\" onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)2016-09-14 15:02:04.126  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]2016-09-14 15:02:04.126  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]2016-09-14 15:02:04.171  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]2016-09-14 15:02:04.396  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup2016-09-14 15:02:04.465  INFO 6 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)2016-09-14 15:02:04.470  INFO 6 --- [           main] c.manthanhd.eventsmgmtapis.Application   : Started Application in 4.07 seconds (JVM running for 4.811)Hurrah! Your container is running. Before we open our champagne bottles, lets break down that command to better understand what we just did.The base command is docker run which tells it to run something. All the parameters that follow tells it what to run.First one is the -p parameter. Remember that EXPOSE 8080? Well that was exposed on the container. However, if you want that port to be available on your host (similar to port forwarding), you'd need this bit. In the 8080:8080, the first part (before the colon) is the port on the container and the second part (after the colon) is the port on your machine. This mapping is optional. If you don't provide this value, it won't bind the port to your local port so to access your container you'll have to specify your container IP address instead of just localhost.Next is the name of the running container. This is entirely optional. If you don't specify one, it will generate a random name.After that is the --rm parameter. Again, this is optional as it tells Docker to remove the container after its stopped. By default, Docker will keep a container around so that you can inspect its logs to determine why it was shut down. For our test we just need it to run our container and then remove it when we don't need it.Last set of parameters, -i and -t (combined into -it) tells Docker to run our container in interactive mode (-i) with a psuedo TTY (-t). We've chosen interactive mode to make it easier for us to see the output and terminate the container as we can just press Ctrl + C to stop it instead of running docker stop &lt;containerId&gt; command.Lastly, we specify the image we want to create a container from. In this case its bootdemo2. Since we haven't specified a tag, docker assumes its latest. If we do want to specify one, say, 1.0, it would be like bootdemo2:1.0.To check everything that Docker is currently running, you can run the following in a new tab.docker ps -aYou should get something like:CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMESbe82e8618345        bootdemo2           \"/bin/sh -c 'java -Dj\"   33 seconds ago      Up 31 seconds       0.0.0.0:8080-&gt;8080/tcp   myspringbootappAlso, just quickly, if you get the following error:Cannot connect to the Docker daemon. Is the docker daemon running on this host?Just update your host environment with the following command:eval $(docker-machine env default)and then re-run the above docker ps -a command.To check whether or not the application is working fine, you could just navigate to your greeting endpoint (in my case http://localhost:8080/events) in your browser. This should work if you've passed in the -p parameter. However, if you are using boot2docker or docker on a non-linux based operating system, you will have to make sure that your docker-machine has port forwarding setup for 8080 (or whatever port you are trying to setup).As you can see, we've got one docker container running. Because we are running it in interactive mode, there are two ways of stopping it. You can stop is by pressing Ctrl + C command in the window where its running interactively, but as you guessed it, this only works when its running in interactive mode. Normal way to stop your container is by using the docker stop command.docker stop &lt;container name / ID&gt;You can obtain the container name from whatever you named your container as, or if you didn't explicitly name it, you can obtain the ID from the docker ps command.Also, because we started it with --rm command, doing docker stop will remove the container as well. However, if we didn't have that flag in, you'd have to run the below command after the stop command:docker stop &lt;container name / ID&gt;Once you've stopped your container, you could try re-running it but this time without the --rm flag and see the difference when it comes to stopping your container.Relating back to our earlier analogy of problems with deploying different versions of a single application, using docker, since you would have a docker image for every version of your application, you'd just swap out an old container with a new one. Even better than that, you could try out the any version of your application locally and since its a container, it would run in exactly the same way in production. If things don't work out, just remove that version of the container and re-deploy a version that works.Simple! I hope this post has given you a good introduction to Docker and the problem it solves for you. I'd love to hear about your experiences with Docker. Drop a note down in the comments below or on my twitter account @davemanthan.",
        "url": "/2016/09/14/introduction-to-docker/"
      }
      ,
    
      "2016-09-08-deploying-wordpress-on-coreos": {
        "title": "Deploying single instance WordPress site on CoreOS",
        "author": "Manthan Dave",
        "category": "",
        "content": "Lets start off by deploying a single MySQL DB instance. For this, we're going to use the default mysql image from docker hub. In order to deploy anything to CoreOS, you need to first create a service file. Here's one for the MySQL that we're going to deploy.[Unit]Description=MySQL database for wordpress[Service]ExecStartPre=-/usr/bin/docker stop wpdbExecStartPre=-/usr/bin/docker rm wpdbExecStartPre=-/usr/bin/docker pull mysql:latestExecStart=/usr/bin/docker run -e MYSQL_ROOT_PASSWORD=glory86 --name wpdb -t mysql:latest[X-Fleet]Conflicts=wpdb.serviceSave that file as wpdb.service. Lets examine that file. As you can see, the file is split into three distinct sections. Unit, Service and X-Fleet. The Unit section tells CoreOS, or more specifically fleet, what this service is about and what it is for. Since this one is quite simple, we only have a Description here.The second section is Service. This basically defines the mysql service. We've got four lines. The first three cover ExecStartPre section where we define commands that it needs to run before starting our service. In this case, we are stopping and removing any existing containers named wpdb (as that's what we're calling our database instance). We're also pulling in the latest mysql image from dockerhub so that we can be prepared when the service is started.Next we're defining what happens when the service is started by defining the ExecStart property. Here, we're running a simple docker run command. Since the mysql image needs a password to be defined in the environment, we're passing one here. We're also naming our database to be wpdb. Note that we are not binding any ports as we don't want access to the mysql locally on any machine. Later on, we'll let our wordpress docker container talk directly to the mysql container by using docker link.Now load up the wpdb.service using fleetctl command:fleetctl load wpdb.serviceOnce loaded, you can start it by running the start command:fleetctl start wpdb.serviceNow that our database container is deployed, lets deploy our wordpress container. To do that, we need another service file.[Unit]Description=Wordpress blog server for www.manthanhd.comAfter=wpdb.serviceRequires=wpdb.service[Service]ExecStartPre=-/usr/bin/docker stop www-manthanhd-comExecStartPre=-/usr/bin/docker rm www-manthanhd-comExecStartPre=-/usr/bin/docker pull wordpress:latestExecStart=/usr/bin/docker run --name www-manthanhd-com --link wpdb:mysql -p 80:80 -e WORDPRESS_DB_USER=root -e WORDPRESS_DB_PASSWORD=glory86 -e WORDPRESS_DB_HOST=mysql -t wordpress:latest[X-Fleet]Conflicts=www-manthanhd.serviceMachineOf=wpdb.serviceSave that file as www-manthanhd.service. As you can see, this file is very similar to the wpdb.service file that we created earlier. However, there are a few changes.Since we want to make sure that our database container is running before we run our wordpress container, we have to make sure that we have After and Requires properties defined accordingly. In this case, we're instructing fleet to run our wordpress container that we require the wpdb.service and to run this container after the wpdb.service.Another notable change is in the X-Fleet section. Here we're defining an extra property called MachineOf where we're telling fleet to deploy this wordpress container on the same machine as the wpdb.service container. This is to ensure minimum latency between wordpress and the database.We do have a slight change in the ExecStart where we're now linking the wpdb container to the wordpress container by using docker link as well as binding port 80.Once done, load up the wordpress container service using fleetctl:fleetctl load www-manthanhd.serviceAnd then start it:fleetctl start www-manthanhd.serviceLets check if the database and the wordpress containers are actually running. To do that, run:fleetctl list-unitsYou should see output similar to this:UNIT MACHINE ACTIVE SUBwpdb.service 7528fd75.../172.17.8.101 active runningwww-manthanhd.service 7528fd75.../172.17.8.101 active runningThat's telling us that wpdb.service and www-manthanhd.service are both running on the same machine - 172.17.8.101 and are actually running!Cool! Lets try to see if we can access wordpress. You can do this by navigating to port 80 of 172.17.8.101 or whatever ip address is shown above for www-manthanhd.service above for you.",
        "url": "/2016/09/08/deploying-wordpress-on-coreos/"
      }
      ,
    
      "2016-08-27-some-quick-tomcat-shell-functions": {
        "title": "Some quick tomcat shell functions",
        "author": "Manthan Dave",
        "category": "",
        "content": "Here are some bash profile functions to help you use tomcat as a pro. The following needs to be done in either your .profile file or .bash_profile, depending on what shell you have and whichever method you prefer. First off, make sure you have exported CATALINA_HOME variable pointing to the base of your tomcat installation.export CATALINA_HOME=/opt/tomcatOnce you’ve got that done, you can define the following shell functions.function tomst() {  ${CATALINA_HOME}/bin/startup.sh}function tomsh() {    ${CATALINA_HOME}/bin/shutdown.sh}function tomlog() {    less ${CATALINA_HOME}/logs/catalina.out}function tomls() {    ls -l ${CATALINA_HOME}/webapps | grep -v .war$}function tomlogf() {    tail -f ${CATALINA_HOME}/logs/catalina.out}function tomps() {    ps -ef | grep tomcat | grep -v grep | cut -d' ' -f2}function tomkill() {    kill `tomps`}Lets break these down. The first command, tomst , will start up your tomcat using the startup.sh script. Depending on your setup, you might prefer to use the startup.sh script over the service command. This is a neat way to do it from any directory.Next is the tomsh command which will help you shut down your tomcat container. Again, like the previous command, this is using the shutdown.sh script instead of the service command. You could replace both of these with the service commands if you prefer.The tomlog and tomlogf commands both display the catalina.log log file, however the tomlogf command will follow (tail) the log.The tomls command will show whats in your webapps directory. Now, this command will only show you your war files so these might not necessarily be deployed, but its a good way to list your deployments.The tomps command will show you the process ID of the running tomcat container. As of this moment, this only looks for the keyword tomcat in your processes. If you have multiple tomcat containers running, you will have to make the grep more specific.Lastly, the tomkill command effectively runs the tomps command but with a prefix of a kill command, effectively executing a kill on your tomcat process.",
        "url": "/2016/08/27/some-quick-tomcat-shell-functions/"
      }
      ,
    
      "2016-08-27-some-docker-helper-commands": {
        "title": "Some docker helper commands",
        "author": "Manthan Dave",
        "category": "",
        "content": "So a lot of times when you’re working with some cool docker containers, you might need to remove all those dead containers that are just lying around. This could happen if you’re running docker containers in background or forgot to use –rm flag.Not to worry, just run this command to remove ALL running docker containers in one go: docker ps -a | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}That will list all docker containers, cut the first column, remove the one with CONTAINER in it and pipe all the remaining ones one by one using xargs into docker rm command.Awesome right? Well, what if you only wanted to remove only the ones that have been STOPPED? Here you go:docker ps -a | grep Exited | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}As you can see, this command only removes the containers that have ‘Exited’.To stop all running containers:docker ps -a | grep Up | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker stop {}Some helper functions:function dockstopall() {    docker ps -a | grep Up | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker stop {}}function dockrmall() {    docker ps -a | grep Exited | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}}Add these to your .profile or .bash_profile or whatever shell profile you use. Once added, to apply in existing shell, run:source ~/.profile ",
        "url": "/2016/08/27/some-docker-helper-commands/"
      }
      ,
    
      "2016-08-22-streaming-folders-from-a-server-using-tar-and-gzip": {
        "title": "Streaming folders from a server over SSH using tar and gzip",
        "author": "Manthan Dave",
        "category": "",
        "content": "This is how you can stream a folder or multiple folders from a server over SSH protocol.To stream folders in uncompressed, raw format, run:ssh ec2-user@xx.xxx.xx.xxx \"sudo tar -cO /folder1 /opt/folder2\" | tar -xf -Since the tar file is being streamed raw, this could potentially take longer as more data is being passed between server and client. In my test for 700 megabytes of data, this command took 1:56 seconds.There’s another method. This streams the tar file, pipes it into gzip on the remote server which comes out as compressed gzipped stream down to the client. The client then passes this compressed stream into gunzip which decompresses the stream and finally then pipes it into tar to extract it into its original folders.ssh -i ~/.ssh/lao345-sandbox.pem ec2-user@10.122.64.165 \"sudo tar -cO /prod/msp /opt/beasys | gzip -c -\" | gunzip -c - | tar -xf -With the same data as above, this method took smaller amount of time, 1:13 seconds to be exact.",
        "url": "/2016/08/22/streaming-folders-from-a-server-using-tar-and-gzip/"
      }
      ,
    
      "2016-08-12-docker-proxy-settings": {
        "title": "Docker proxy settings",
        "author": "Manthan Dave",
        "category": "",
        "content": "Here’s a quick way to configure docker to work with proxy. Keep in mind that this only works if you are using boot2docker or a docker-machine. This generally applies to Windows or Mac operating systems.Create a new docker-machine with proxy:docker-machine create -d virtualbox \\    --engine-env HTTP_PROXY=http://user:password@example.com:port \\    --engine-env HTTPS_PROXY=https://user:password@example.com:port \\    --engine-env NO_PROXY=example2.com \\    proxyboxEdit user, password, example.com and port variables with appropriate values. Also, make sure you have correct domains excluded within NO_PROXY variable.To change proxy credentials later on, edit /var/lib/boot2docker/profile and set HTTP_PROXY and HTTPS_PROXY environment variables.",
        "url": "/2016/08/12/docker-proxy-settings/"
      }
      ,
    
      "2016-06-26-taking-out-netflixs-gradle-lint-plugin-for-a-spin": {
        "title": "Taking out Netflix&amp;#8217;s gradle-lint-plugin for a spin",
        "author": "Manthan Dave",
        "category": "",
        "content": "Dependency management is a complex thing. A typical gradle project has several dependencies, each of which in turn has multiple other dependencies and so on. This continues and what forms as a result is called a dependency graph. While this is great, unused dependencies is a big problem. For instance, say your project depends on dependency A which then depends on B, C and D. Your project also depends directly with another dependency E which in turn depends on F and G. The dependency graph would look something like this:Your Project -&gt; A -&gt; (B, C, D)Your Project -&gt; E -&gt; (F, G)Now, your project probably only uses code in A which only then directly depends on some code in B. Also, maybe you also have some code that depends on E but that code in E doesn't depend directly on neither F, nor G. This means that C, D, F and G are your unused dependencies.If you knew this from the start, you could just exclude those libraries. However, tracking this manually is incredibly difficult as the more complex the libraries are, the more complex the web of inter-dependencies becomes. I've been looking for an automated solution to this for a while and recently, I found that Netflix has a plugin that could help. This is called the gradle-lint-plugin.Its part of Netflix's OSS and is available on github to download. Its artifacts are available in JCenter maven repository. To test it out, I quickly got a gradle project up and running by following the spring boot tutorial. To make things a slightly more interesting, I added the org.apache:httpcomponents:httpclient library as a dependency in my build.gradle file:compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'Also, for reference, here's my gradle version:C:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test&gt;gradle -v------------------------------------------------------------Gradle 2.12------------------------------------------------------------Build time:   2016-03-14 08:32:03 UTCBuild number: noneRevision:     b29fbb64ad6b068cb3f05f7e40dc670472129bc0Groovy:       2.4.4Ant:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013JVM:          1.7.0_10 (Oracle Corporation 23.6-b04)OS:           Windows 8 6.2 amd64I ran the gradle clean build command to assess the initial size of the build. As expected, the war file was massive at 14.3 megabytes. For a small project that has a simple healthcheck endpoint, this is huge. So, to streamline this, I followed the gradle-lint-plugin guide to use and apply the plugin. I added some rules of my own too. Here's what my build.gradle file looks like:buildscript {    repositories {        mavenCentral()        jcenter()    }    dependencies {        classpath(\"org.springframework.boot:spring-boot-gradle-plugin:1.3.5.RELEASE\")    }}plugins {    id 'nebula.lint' version '0.30.12'}allprojects {    apply plugin: 'nebula.lint'    gradleLint.rules = ['all-dependency', 'unused-dependency', 'unused-exclude-by-dep'] // add as many rules here as you'd like}apply plugin: 'java'apply plugin: 'eclipse'apply plugin: 'idea'apply plugin: 'spring-boot'jar {    baseName = 'gs-spring-boot'    version =  '0.1.0'}repositories {    mavenCentral()    jcenter()}sourceCompatibility = 1.7targetCompatibility = 1.7dependencies {    compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'    // tag::jetty[]    compile(\"org.springframework.boot:spring-boot-starter-web\") {        exclude module: \"spring-boot-starter-tomcat\"    }    compile(\"org.springframework.boot:spring-boot-starter-jetty\")    // end::jetty[]    // tag::actuator[]    compile(\"org.springframework.boot:spring-boot-starter-actuator\")    // end::actuator[]    testCompile(\"junit:junit\")}task wrapper(type: Wrapper) {    gradleVersion = '2.1'}Once that was out of the way, I simply ran gradle clean build command.C:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test&gt;gradle clean build:clean:compileJava:processResources UP-TO-DATE:classes:findMainClass:jar:bootRepackage:assemble:compileTestJava UP-TO-DATE:processTestResources UP-TO-DATE:testClasses UP-TO-DATE:test UP-TO-DATE:check UP-TO-DATE:build:lintGradleThis project contains lint violations.  A complete listing of the violations follows.Because none were serious, the build's overall status was unaffected.warning   unused-dependency                  one or more classes in org.springframework:spring-web:4.2.6.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot:1.3.5.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework:spring-context:4.2.6.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework:spring-web:4.2.6.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot:1.3.5.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE are required by your code directlywarning   unused-dependency                  one or more classes in org.springframework:spring-context:4.2.6.RELEASE are required by your code directlywarning   unused-dependency                  this dependency is unused and can be removedbuild.gradle:39compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'warning   unused-dependency                  this dependency is unused and can be removedbuild.gradle:39compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'? build.gradle: 10 problems (0 errors, 10 warnings)To apply fixes automatically, run fixGradleLint, review, and commit the changes.BUILD SUCCESSFULTotal time: 15.742 secsExcellent! It found the unused dependencies. I was quite pleased with this result so I ran gradle fixGradleLint command. According to the wiki, this command should fix the issues that it found by making changes to the build.gradle file. Unfortunately, this didn't work as it resulted in an error.C:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test&gt;gradle fixGradleLint:fixGradleLint FAILEDFAILURE: Build failed with an exception.* What went wrong:Execution failed for task ':fixGradleLint'.&gt; com.netflix.nebula.lint.jgit.api.errors.PatchApplyException: Cannot apply: HunkHeader[36,7-&gt;36,14]* Try:Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.BUILD FAILEDTotal time: 10.808 secsI've already raised a github issue to draw their attention to this problem. For information on progress, feel free to watch or add to the issue.So, even if the automated fix didn't work, I wasn't done yet. I looked around in the build directory looking for something that I can use to apply for the fix myself. After all, since gradle lintGradle and gradle fixGradleLint are two separate commands, independent of each other, it must store some form of state somewhere. Fortunately I found it. It stores its state in a file called lint.patch directly under the build directory at the root of your project.Hurrah! I quickly opened it. It was a normal git patch file. While looking at it, I was slightly confused with the additions that it was making. Here's what mine looked like:diff --git a/build.gradle b/build.gradle--- a/build.gradle+++ b/build.gradle@@ -36,7 +36,14 @@ targetCompatibility = 1.7  dependencies {+   compile 'org.springframework:spring-web:4.2.6.RELEASE'+   compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'+   compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'+   compile 'org.springframework:spring-context:4.2.6.RELEASE'+   compile 'org.springframework:spring-web:4.2.6.RELEASE'+   compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'+   compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'+   compile 'org.springframework:spring-context:4.2.6.RELEASE'-    compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'     // tag::jetty[]     compile(\"org.springframework.boot:spring-boot-starter-web\") {         exclude module: \"spring-boot-starter-tomcat\"Looking at that file, I could immediately tell that it was adding some compile time dependencies and was removing one httpcomponents dependency. While I agree with the removal of the httpcomponents dependency, I was slightly confused why it wasn't replacing the existing spring-boot dependencies with the ones it was adding. It should've replaced them because having both provides no benefit. What its trying to add is more specific than the dependencies that are already there! To prove myself right, I removed all the existing compile dependencies (expect testCompile ones) and added the ones that it was going to add from the patch file. Here's what the dependency section of my build.gradle looked like:dependencies {    compile 'org.springframework:spring-web:4.2.6.RELEASE'    compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'    compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'    compile 'org.springframework:spring-context:4.2.6.RELEASE'    compile 'org.springframework:spring-web:4.2.6.RELEASE'    compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'    compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'    compile 'org.springframework:spring-context:4.2.6.RELEASE'    /*compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'    // tag::jetty[]    compile(\"org.springframework.boot:spring-boot-starter-web\") {        exclude module: \"spring-boot-starter-tomcat\"    }    compile(\"org.springframework.boot:spring-boot-starter-jetty\")    // end::jetty[]    // tag::actuator[]    compile(\"org.springframework.boot:spring-boot-starter-actuator\")*/    // end::actuator[]    testCompile(\"junit:junit\")}I quickly ran gradle clean build and sure enough, this time the build size had shrunk to almost a third of its original size - 5.5 megabytes! Also, the lintGradle task at the end of the build didn't report any errors.I can certainly see the value of this plugin in a continuous integration system as unused libraries cause host of memory related issues like lack of permgen space in java web containers. Also, its better to let developers focus on core development and let the periphery issues such as these be handled by an automated plugin like this one.I hope the developers on the github project fix the issues related to the fixGradleLint task soon.Check out the source for this test on github.",
        "url": "/2016/06/26/taking-out-netflixs-gradle-lint-plugin-for-a-spin/"
      }
      ,
    
      "2016-06-26-handling-file-uploads-in-express": {
        "title": "Handling file uploads in Express using multer",
        "author": "Manthan Dave",
        "category": "",
        "content": "First of all, download and install multer.npm install --save multerInclude the dependency.var multer = require(\"multer\");Initialise it into a service. The command below will initialise it with default storage type (being DiskStorage) and upload destination being the uploads/ folder.var uploadService = multer({ dest: \"uploads/\" });If you don't want the file to be saved anywhere in the file system and want to use memory storage, initialise it as follows.var uploadService = multer({ storage: multer.memoryStorage() });You can add file size limits too. The command below sets the file upload limit to be 12 megabytes.var uploadService = multer({storage: multer.memoryStorage(), limits: {fileSize: 1000 * 1000 * 12}});You can then pass around the uploadService object to other modules that use it.function FileUploadRoute(express, uploadService) {To read uploads in a route, add it to the second parameter of the route definition. The following commands processes a single file uploaded under file attribute within post form-data.router.post(\"/\", uploadService.single('file'), function (req, res) {    var uploadedFile = req.file;});The uploadedFile object here contains data about the file that was uploaded. The buffer attribute of this object contains the actual file content. The object also has other attributes like filename, mimeType and encoding that can be used to process the file according its type.For more information, see multer on npm.",
        "url": "/2016/06/26/handling-file-uploads-in-express/"
      }
      ,
    
      "2016-05-13-clearing-passwords-on-linux": {
        "title": "Clearing passwords on Linux",
        "author": "Manthan Dave",
        "category": "",
        "content": "As root, quickest way is to use the passwd  command:passwd -d &lt;user&gt;If there are failed attempts, you’d need to reset those as well:pam_tally2 --user=&lt;user&gt; --resetTo verify:pam_tally2 --user=&lt;user&gt;",
        "url": "/2016/05/13/clearing-passwords-on-linux/"
      }
      ,
    
      "2016-04-08-simplifying-management-of-forks-with-npm-forkpool-module": {
        "title": "Simplifying management of forks with npm forkpool module",
        "author": "Manthan Dave",
        "category": "",
        "content": "BackstoryRecently, I've been working on a project that does a lot of heavy weight image processing. In a nutshell, a user uploads an image and then the website does a whole load of processing on that image and returns with the results. Depending on the size of the image it could take a while for it to load the results. The websites does other stuff as well so the scenario where a few users are concurrently uploading images, waiting for response while a whole load of other set of users are navigating to other parts of the website is very real. The performance was respectable, however, I wanted more.After a couple of hours of investigating around the issue, I finally settled on using forks for handling this type of load. A fork is a process that is spun off of a parent process. In this case, when a user uploads an image on the website, the node server creates a fork that then does the image processing. Meanwhile, the parent process hooks into the \"exit\" callback of the forked process so that it knows when the forked process has exited. When this happens, the parent process returns the results to the user.Now, every node process, no matter how asynchronous, is single threaded. This means that it utilises at most a single core of your CPU. So the fact that the new fork powered architecture creates a fork process to handle the heavy image processing means lower load on the single node process. It also means that the load is spread evenly across cpu cores.However, here lies the problem. Every time a new customer arrives on the website and uploads a new image, the node server spins up a new process. So, if many customers arrive and upload large images which could take 10-15 seconds to load, it will spin up a new process for each of those customers. Since each process maps to roughly one CPU core, having too many processes doing CPU intensive things will start to slow down everything thats running on that machine. This means that they will slow down each other as they race to get as much CPU time as they can.This problem also has a simple solution. Limit the number of forks so that they do not exceed a certain number. This number will vary upon usage and circumstances but after doing some benchmarking, in my case, this was 3. This meant that my application should, at any given time run at most 3 forks (processes).In technical speak, this is called pooling of resources. The idea behind this is that you create a pool of fixed size and then take things in and out of pool. Translating this into what I needed to do with my fork problem, I needed a pool of size 3 to manage my forks so that it will allow me to put at most 3 things in it. If I tried to put more things in it, it should queue those things, wait for current things that are running in it to finish and then automatically start the queued things.I googled for some time, trying to find a good way to pool my forks. I found a couple of solutions but none were as elegant as I'd like them to be. I did find some good ones but they either were deprecated or weren't being maintained anymore. So, I got my IDE ready and started programming what will be known as the forkpool module.ForkpoolThe module comprises of two basic things. Forkpool and Forklet . Forklet  is a definition of a fork instance. For example, if you wanted to fork a node module named worker.js , your forklet will comprise of three things. First being the location of the module, second being additional environment configuration needed to run the fork and third (optional) being the time within which the fork must complete its execution (if it doesn't, it gets killed). The third option of specifying timeout was the one I couldn't find available in any of the existing fork pools and hence decided to create it.So once you have a forklet , you can use the forkpool to create a fork. Upon creating the forkpool, you can define the size of the pool in its constructor. This value is the maximum number of forks you want running in parallel at any given time.Once you have a forkpool  and forklet  objects you can execute forks. If you execute more forks than the capacity, it will queue the rest of the forks up and execute them one by one as the running forks finish their execution.The module also comes with a event system comprised of a set of events. Every time something happens with the forks, a event is triggered. You can subscribe to events on a specific fork or to all forks across the entire pool. Working within the asynchronous architecture of node, this just made sense. There are various kinds of events that let you know what is happening to a fork. These are scheduled , started , timedout and exited . Event callbacks also come with additional contextual information that provide more information regarding that specific event.Checkout these links below for more information regarding the module:NPM, Github, Travis CI",
        "url": "/2016/04/08/simplifying-management-of-forks-with-npm-forkpool-module/"
      }
      ,
    
      "2016-03-23-appdynamics-power-user-training-notes-day-three": {
        "title": "AppDynamics Power User Training Notes &amp;#8211; Day Three",
        "author": "Manthan Dave",
        "category": "",
        "content": "Dashboards. Useful for displaying and sharing information across teams or departments. Allows integrating other tools via iFrame widget. Dashboards can also be embedded into other services or even web pages.A dashboard can also be converted into a report. One can also schedule periodic emailing of reports to a recipient list. This might be useful to create a “weekly/monthly digest” of data relating to an environment.Policy triggers define events that trigger actions. When the criteria in a policy is met, a corresponding trigger is activated. This can be used to control behaviour such as increaing thread pool size as load increases etc. The trigger must be associated with a health rule violation. For example, if a health rule specifies “calls higher than normal” then a policy can be created based on this heath rule. The trigger gets executed when violation of the specified health rule occurs. Triggers can span different stages of health rule violations, for example, start and/or end. More than one health rule violation event can be specified.When defining actions for policy triggers, one can configure the action to require human approval before executing a particular action. In this case, one must specify email address of the approver within the action.Runbook automation is a remediation action that is executed on a node on a policy trigger. For instance, if for some reason there’s a java out of memory error, the runbook automation can backup the logs and restart the JVM. Everything that the runbook automation does is logged with the action that was executed, time and the local script that the automation ran to carry out the specified action. There’s three types:Remediation action: Run arbitrary script on a nodeHTTP request: Make a HTTP request to some server notifying it of the issue.Cloud auto-scaling: Scale out or scale in on cloud services like AWS.",
        "url": "/2016/03/23/appdynamics-power-user-training-notes-day-three/"
      }
      ,
    
      "2016-03-23-appdynamics-power-user-training-notes-day-two": {
        "title": "AppDynamics Power User Training Notes &amp;#8211; Day Two",
        "author": "Manthan Dave",
        "category": "",
        "content": "Health rules are conditions that if fulfilled relate to good health for a particular node. The conditions can be based on load (calls/min), response times (x ms average) or error rate. AppDynamics provides 7 health rules out of the box. When one or many health rules are violated, an alert is issued. Depending on the integrations configured, this alert can be an email or SMS.Health rule can be based on transaction performance or node health. In case of transaction performance, it can target specific transactions or all transactions in a tier. However, in case of node heath, this can target specific node or nodes in a tier or nodes by type.The data that health rule uses can also be configured. This can be a time period that it should use. This can be in free form numeric minutes. One can also specify wait time after a violation, i.e. how long should it wait to reassess health rule after it has been violated. This should be long enough so that it has enough new data to positively reassess the condition.Diagnostic session is a time period in which AppDynamics collects extra snapshots for one or more business transactions. All transactions have full call graphs. Diagnostic sessions are normally used after a health rule violation.Diagnostic sessions can be policy based where it is automatically executed as soon as a particular health rule violation has happened. Depending on the situation, the policy can then have actions where it can send an email or a HTTP request with event details.Development mode is where it creates snapshots where it creates transactions for every business transaction. Highly advised that it is only used in testing environments as it has more overhead compared normal production mode. It has safeguards built in where it automatically turns itself off to production mode if 500 calls/min or 90% heap utilisation happens.All HTTP error codes from 400 to 505 are treated as errors. Also, all exceptions within JVM that are handled but logged as fatal using Log4J or java.util.logging are also treated as errors.Scenario: Customer calls regarding an error that they saw on the page.Solution: Start a diagnostic session or periodic collection or brief development mode to get snapshots with full call graphs. Go to application dashboard and on the right pane, click errors. Or, go to snapshots and filter by errors. Once you locate the snapshot with errors that has full call graph (blue document icon next to it),  double click on it to view and go to error details. Make sure you turn off the development mode.A business transaction is detected at the point when it enters the application. If a transaction or node is not detected, it can manually be added. Configuration &gt; Instrumentation &gt; Transaction detection. In the custom rules section, a custom rule for detecting a transaction can be added.Exit point is a call that leaves node or environment either to a new monitored environment or a tier. Example database/mail server etc.Use Data Collectors to collect data that's passed into methods or payloads. Its available in Application &gt; Configuration &gt; Instrumentation &gt; Data Collectors tab. For methods, use getter chain and specify object getter to invoke which is likely to provide data that's needed. For instance, to collect movie title from an object that’s passed into search(movieObject) method, if the getter chain is getTitle() then the full invocation will be movieObject.getTitle().",
        "url": "/2016/03/23/appdynamics-power-user-training-notes-day-two/"
      }
      ,
    
      "2016-03-23-appdynamics-power-user-training-notes-day-one": {
        "title": "AppDynamics Power User Training Notes &amp;#8211; Day One",
        "author": "Manthan Dave",
        "category": "",
        "content": "Node is mapped to individual JVM or CLR application in environment. If an environment has multiple JVMs running then each maps to a node.Tier is a logical piece of application. For example a piece of functionality. Each web server has node. A single tier can span multiple web servers and each of those web servers can contain multiple nodes.Application traffic is organised into business transactions. Each transaction is a distinct user activity like Login or register etc. When a request comes in for the first time, it is tagged with a GUID and that GUID is tracked across the environment. Requests with similar patterns are grouped together. This group is then given a default name. This is default name is based on how the application is designed.Since each business transaction works across nodes, each has its own flow map.A business transaction is a specific user activity. It represents a flow of data across the application. A tier, on the other hand represents a static, logical set of services.Transaction snapshot is a static state of a transaction at any given time. It collects detailed information regarding one business transaction at any given time.Periodic snapshot is a snapshot that’s taken once every 10 minutes. This is taken regardless of any errors occurring within the application.Slow and error transaction snapshot is a snapshot that is taken when calls happening below a certain threshold or when errors are happening above a certain error rate.Snapshots can be archived. Once a snapshot is archived, it is permanently visible in the UI and won’t get deleted with periodic cleanup. A transaction that has been archived cannot be unarchived and must be deleted if archive is no longer needed.AppDynamics architectureApp server agent. This could be .NET, Java, PHP etc. This is mandatory.Machine agent is optional. Collects raw metrics regarding the machine that it is on.AppDynamics supports role based authentication. Also supports other integrations like LDAP/SAML etc.AppDynamics controller never actively initiates connection to an agent. The agent connects to the AppDynamics controller which then responds with any additional requests.Baseline is a set of values calculated from metrics within a time range. We could capture “ideal” scenario by recording a baseline when application was running smoothly. Then at a later date, when application isn’t performing optimally, one can compare the current performance with the historic “ideal” baseline to identify problems.Baseline can also be a dynamic, rolling baseline where, for example, every monday 2pm, baseline is taken. Now next monday at 2pm metrics are compared to the baseline that was taken on the previous monday. This is a dynamic baseline so if the current performance peaks, the baseline will adjust itself to compensate for the peak.When a baseline is defined, if the deviation of the current data is certain level above the baseline, AppDynamics will automatically raise an alarm indicating that.Rather than deleting a transaction, it is better to exclude it as restoring a deleted transaction is much harder (needs re-creating) than restoring a excluded one.",
        "url": "/2016/03/23/appdynamics-power-user-training-notes-day-one/"
      }
      ,
    
      "2016-03-18-service-script-for-tomcat": {
        "title": "Service script for Tomcat",
        "author": "Manthan Dave",
        "category": "",
        "content": "Quick script to allow starting and stopping tomcat from the service command.Create a script called tomcat7  in /etc/init.d  like so:vim /etc/init.d/tomcat7Here’s what goes into the script:#!/bin/bash## chkconfig: - 85 15# description: Jakarta Tomcat Java Servlets and JSP server# processname: tomcat# pidfile: /var/www/tomcat/tomcat.pid# Source function library.. /etc/init.d/functionsTOMCAT_USER=tomcatCATALINA_HOME=/var/tomcatRETVAL=$?case \"$1\" instart)        echo \"Starting Tomcat\"        exec su -l $TOMCAT_USER -c \"$CATALINA_HOME/bin/catalina.sh start\";;stop)        echo \"Stopping Tomcat\"        exec su -l $TOMCAT_USER -c \"$CATALINA_HOME/bin/catalina.sh stop -force\"        numproc=`ps -ef | grep \"$CATALINA_HOME/bin/bootstrap.jar\" | grep -v grep |awk -F' ' '{ print $2 }'`;          if [ $numproc ]; then          echo \"###kill pid: $numproc\"            kill -9 $numproc          fi;;debug)        echo \"Starting Tomcat in debug\"        exec su -l $TOMCAT_USER -c \"$CATALINA_HOME/bin/catalina.sh jpda start\";;restart)        $0 stop        sleep 15        $0 start;;*)        echo $\"Usage: $0 {start|stop|restart}\"        exit 1;;esacexit $RETVALThe commands start, stop, debug and restart can be run via service. Example:service tomcat7 startThen apply appropriate permissions:chmod 755 /etc/init.d/tomcat7chown root:root /etc/init.d/tomcat7 ",
        "url": "/2016/03/18/service-script-for-tomcat/"
      }
      ,
    
      "2016-03-16-useful-docker-commands": {
        "title": "Useful Docker commands",
        "author": "Manthan Dave",
        "category": "",
        "content": "Managing containersRemove all old containers:docker ps -a | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rmRemove all stopped containers:docker ps -a | grep 'Exited' | awk '{print $1}' | xargs --no-run-if-empty docker rmManaging imagesRemove all untagged images:docker rmi $(docker images -a | grep \"^&lt;none&gt;\" | awk '{print $3}')Remove all untagged images (force):docker rmi -f $(docker images -a | grep \"^&lt;none&gt;\" | awk '{print $3}') ",
        "url": "/2016/03/16/useful-docker-commands/"
      }
      ,
    
      "2016-03-02-notes-from-aws-developer-training-day-three": {
        "title": "Notes from AWS Developer Training (Day Three)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Creating Serverless ProjectsIn a server less environment, Amazon Lambda can be used in conjunction with Amazon API Gateway for HTTP interfacing, Amazon S3 for storage, Amazon ElastiCache for caching and DynamoDB/RDS for database storage. Checkout the Servless Application Framework at serverless.com for more info.Securing data in AWSInfrastructure should be treated as code, I.e. Version control systems. Automate security and increase testing frequency via CI/CD. Fail early and fast. Test at production scale. No need to keep the test servers alive. Spin up the entire production environment in test, deploy the code, run the tests and then tear down the environment.DevOps provides efficiency that speeds up the development lifecycle (build &gt; test &gt; release &gt; monitor &gt; plan). Validate security in each step to avoid slowing down the whole lifecycle.Amazon’s shared responsibility model.How do we get certificates for the internal private DNS for an AWS EC2 instance?All requests to AWS must be signed using access key ID and secret access key. If using the SDK, this is done automatically. However, if this is done manually, use signature version 4 to sign requests.Support v2 and v4, most services support v4. V4 is more secure as it derives a key from secret access key and then uses that derived key to sign requests. V2 on the other hand does this very plainly as it uses the secret key directly to sign requests.One way to provide permissions is to add users to a group that has policies attached to it. All users in that group have permissions assigned to them as long as they are in that group. Drawback is that this is a static set of permissions. As in if two developers are out on holiday for  weeks and only one is available, all three developers still have all the access. Also if one of the developers need access to EMR, and if policy is changed to include that permission, all the developers, whether or not they need it, inherit that permission.Better way to do this is to use IAM roles. Create a role called EMR with the policy. The user that needs permission assumes that role. This provides that user with a new set of access key and secret key that has access to EMR.When authenticating with SSO or AD, the authenticating application checks if the user is in the right group to access the target application. If the user isn't in the right group then it must check if there are roles that can be assumed to access the target application. If the roles exist, the authenticating application then checks if the user is able or allowed to assume any of those roles. If the user is allowed, the application assumes the role for him, granting access. One of the ways this can be done is by using SAML assertion with IAM.Getting started with IAMOnce you have the root account, create a general IAM account for yourself. Assign permissions that you need to yourself. If a admin is needed, create a group called Admins and then assign basic admin permissions to that role. When a new admin joins, he/she can be added to Admins group. Give permissions to admins group as needed.For a developer who joins the team, again create a Developers group and assign permissions to that group as needed. When a special permission is needed to access a specific thing, create a role for that and allow people to assume roles for a limited time or as long as they need it for. For temporary contractors and consultants, assign one permission called Assume Role. This allows them to temporarily assume roles when needed.The assume role permission can be restricted by IP, as in you’re only allowed to assume a role when you're on site.For authentication, AWS management console needs username and password. Other tools like AWS CLI, SDKs and API queries need access key and secret keys.NEVER USE ROOT ACCOUNT CREDENTIALS ANYWHERE.Two types of IAM permission types. User centric where each user is given access to specific resources. Second type of permission type is resource centric where specific access to a resource is opened up to a set of users.Use AWS provided library of pre-defined Policy Templates or AWS policy generator. Preferably, create policies in object oriented way using AWS Cloud Formation or AWS SDKs.IAM policy rule procedure is to deny access by default until access is explicitly allowed in all policies. If something is explicitly denied, it can never be allowed. Explicit deny cannot be overwritten.Might be possible to restrict users from uploading content above a certain size.For external users, use OpenID Connect for authentication with external services like Google/Facebook/Twitter login along with IAM for authorisation.CachingCloudFront caches content based on edge location. If content is available, it will serve from its edge location, if not, it will serve from S3.Each caching behaviour can be configured. Params like Path Pattern to be cached, request origin to forward to, if forward URLs should have query strings in them etc.Path patterns max is 255 chars. Case sensitive. Eg. /*.jpg means cache all JPG files.With regards to query strings, cache behaviour will differ based on value of query strings in the request. Example: /hello?query1=a and /hello?query2=b will result in different cache copies served. Cache can also serve different URLs based on different request header values or cookies.The time to live (TTL) value controls how long the value should stay in the cache. Longer TTLs for static content, short for dynamic.A custom error page page can be configured to be serve if the cache fails to find the origin content (if origin changed but cache didn't for some reason and its expecting the file to be present on origin).Authentication works via public/private key pair. CloudFront has the public key while the application has the private key. The URLs must be signed before sending to cloud front. The application signs using its private key, sends URL to cloud front. If the URL is valid, cloud front serves the content.User sessions can be stored in DynamoDB or ElastiCache, depending on need.ElastiCache runs, underneath the covers, me cache or redis cluster. Provides managed service that include patching and backup.Memcache is known for high performance and scale. Allows running of large nodes with multiple cores or threads. Ability to scale in or out. Data can be partitioned across multiple shards. One downside is that the data model is very simple.Redis is quite similar to Memcache where it's open source and in memory. Redis can only scale vertically (check!!). Has simple master/slave model, not shards (check!!). Comes with complex data types. Has pub/sub capability - the client being informed of events on the server. Not very good for multi-threaded performance (check!!).Cache is loaded by lazy loading as in data is only loaded when required.CloudWatchOne can use CloudWatch agent to aggregate logs from EC2 instances to the central CloudWatch server. To do this, install the cloudwatch agent on the EC2 instance.From any log streams, custom cloudwatch metric can be created. Once the metric is created, an alarm can be configured.Cloud watch alarms can call HTTP endpoints, like calling web hooks to trigger different kind of behaviours based on the event.Cloud watch metrics can be aggregated on the server side by providing params like start and end times, period and metric names.CloudWatch dashboards can be used to provide a bird's eye view of all metrics pertaining to a specific application. It also allows adding of text widgets which one can use to provide some free text or images.JSON path expression can be used to search logs within CloudWatch. This style of expression can also be used to extract values from logs. Once values are extracted, they can be used to create a metric. For instance, use Regex to extract response times from tomcat logs, create a CloudWatch metric out of it called Latency and then set a CloudWatch alarm that fires off an email when latency exceeds 2 seconds.CloudWatch can be integrated into other services like Elasticsearch and Kibana. It can also work with AppDynamics or New Relic.EC2 and Auto scalingEC2 instances can be launched with a script that is executed upon launch. This script or piece of code is called user data.When architecting a VPC, make sure that the auto scaling group is spread across two availability zones and each logical component (database server or application server) is in its own private subnet.Auto scaling can scale applications based on cloud watch metrics or time. Time based works like… Scaling out on Monday morning and scaling in on Friday evenings.Auto scaling forms two parts: Launch configuration and Amazon Machine Image (AMI). Launch configuration tells it how to launch something and what to do when it's launched. AMI tells it what to launch. AMI is in three categories: Bare (nothing but base image), Silver (optimised based OS image with prerequisites (Java/Node) installed, Gold (Fully fledged application with application code and prerequisites.If one wants the deployment to fail if something in user data fails, then one needs to initiate the cfn-signal with -e option specifying the error code. One can also use cfn-in it to initialise the script. This attribute in cloud formation will do roll back on its own without the sub-script having to call cfn-signal.",
        "url": "/2016/03/02/notes-from-aws-developer-training-day-three/"
      }
      ,
    
      "2016-03-02-notes-from-aws-developer-training-day-two": {
        "title": "Notes from AWS Developer Training (Day Two)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Achieving loose coupling with EventsAmazon SQS, SNS, DynamoDB Streams, Kinesis Streams, Lambda.With the event driven architecture, two systems don't need to know about each other. Each of them can fire events while the other responds to that specific event.SNS has publish/subscribe model. When publisher pushes, all subscribers immediately get the message. This can be email, SMS, SQS, Lambda etc.SQS queuing for delivery method. Messages are persisted until they are polled. Extremely scalable. Can potentially contain millions of messages.SNS and SQS can be used together. SNS delivers to the SQS queues each for one EC2 instance who then carry out a specific task. On push, SQS provides no guarantee for ordering.DynamoDB streams allow firing of events in a stream. For example, on image upload, the image is sent to S3 and the metadata is stored in DynamoDB. When the metadata is stored, DynamoDB fires off event in a stream which is read by Lambda. This can then do some computation around the data coming from DynamoDB.Amazon Kinesis Streams is a fully managed streaming data service. Helps aggregate data. Data stored in a Kinesis stream is ordered. Client can read a small segment from the stream. Allows adding of checkpoints so that if the client that is reading from a stream fails, it can continue from the last checkpoint.Lambda is a event handler. Not a replacement but a variation of event driven server less code.KinesisOffers services to load and process large amounts of data. Allows building of streaming data applications.First thing that is to be created in Kinesis is a stream. Data records are put into the stream which are then read out from the other side. Stream has shards. Each shard has 1 MBPS write and 100 MBPS. Reads 5 transactions per second and writes are 1000 records per second. Shards can be merged afterwards if they are not in use. This action is reversible.Each data record has three parts. Sequence number (automatically created), partition key and then the data to be streamed. Data can be JSON/XML/Binary etc. The partition key determines which shard gets the data.Operations on stream: Create/List/Delete. Operations on shard: Retrieve/Reshard (increase or merge shard count)/Change data retention period. Every data record has its own expiration time.When data is inserted, the sequence number is automatically assigned and based on the partition key hash it is put in a shard that it selects.KPL (Kinesis Producer Library) acts as an intermediary between producer and a stream, Kinda transactional in the sense that when asked to insert multiple records, if one of them fails insert due to an issue (eg throughput) it denies request. However, the application can still choose to insert a value regardless of other values failing insert.KCL (Kinesis Consumer Library) is used by consumers to read/consume values from the shards. Consumers can read from the beginning of the stream/latest/specific part of a stream based on the sequence number. The GetRecordsAPI requires an iterator.If the data is being read from the tip of the stream, start the consumers first and then the producers otherwise the consumers won't read beginning of the data.When storing data randomise partition keys to make sure of even distribution of data between shards. Provision few extra shards to handle unexpected demands. More shards are needed to support several consumers simultaneously reading from a stream.Batch data before writes for efficiency. On throughput exceeded error, retry/log and monitor/reshard shards. Increase DynamoDB table throughput in extreme cases. Best to have DynamoDB handle duplicates as two records with same keys will be an overwrite.Check permissions to make sure that the code has access to Amazon Kinesis streams, DynamoDB and CloudWatch services.Handle exceptions in processRecords method of IRecordProcessor object.Amazon Simple Work Flow (SWF)Automate flow of events. Could also have long running human tasks. For instance, have a workflow to create and email an invoice and then wait for someone to click approve link within the email to then resume the workflow to further dispatch the product.Two types of work flow tasks: worker and decider. Decider can have code running behind it. Whatever thing the decider touches, it must have permission to access it. For instance, a decider to read from DynamoDB must have access to read data from DynamoDB. As the name suggests, the decider decides outcome. In the invoicing system, if the user clicks reject button in the invoice email, the decider can then cancel the order but continue with dispatch if the user clicks approve button.When the workflow starts the first time, SWF calls the decider which decides which step to go next. The control is then handed back to SWF from the decider and then based on the outcome of decider, SWF then goes to that step. The tasks that are executed can be executed asynchronously.No visual way to create the work flows out of the box. Requires programmatic creation via SDK.Amazon Simple Queue Service (SQS)At least once delivery. Does not have all features of a fully fledged queuing solution. Highly scalable.Provides dead letter queues for messages that haven’t been delivered.Distributed nature means that some messages may not be available across all SQS servers.Message can appear in the queue after some time specified in delay seconds. Queue can restrict size of messages being put by specifying MaximumMessageSize. MessageRetentionPeriod specifies how long should a message be kept in the queue. This can be up to 14 days.While receiving messages, one can use short polling or long polling. Short polling means that SQS returns messages in short bursts (couple at a time), however, with Long polling, it blocks you until it finds significant number of messages. The block time is specified by the client, could be 5-10 seconds. Use this if processing large number of messages in batch is easier than in small batches.Visibility timeout is the time in which the message is invisible for a certain time and then becomes visible. This is for systems that need more time before they can pick up a message. The message handle can be used to extend this timeout if more time is needed.In Java, client side buffering is allowed where the messages can be pre fetched into a local buffer. Automatically batches messages for send or delete operations.Queues can be shared to another AWS accounts with or without credentials (anonymously). Admin can also create policies that grant access to a specific queue. This can then be assigned to a role.Requests incoming to a queue cannot be directly throttled. So if a queue is shared with someone else, they could potentially DDoS the queue and have you charged for each request which could be disastrous. However, this can be implicitly controlled by giving someone access to an Amazon API Gateway endpoint which then throttles all requests from there on to the SQS.Simple Notification Service (SNS)Pub-sub model. Messages are sent to the SNS which then propagates them down to different services or applications. SNS messages are not persisted while SQS messages are. Also, interaction with SNS is passive (push) while the one with SQS is active (poll). Also, usually in SNS, one publisher talks to multiple consumers while SQS is usually meant for one publisher talking to one subscriber.Massively useful in decoupling applications. Supports event driven architecture where based on the message, the receivers could each do different things. For instance when an invoice is placed in SNS, one system that is subscribed could process the order associated with the invoice while second system could send email confirming the receipt of the order.SNS provides good fan out for massively parallel tasks. Example, lambda can listen for SNS messages, do the processing and then push to S3.Does not batch messages. Every notification is a message. Does not guarantee ordering of messages. SNS Delivery policy, that applies to HTTP subscribers can be used to control retries in case of message delivery failure. Messages can contain up to 256KB of data.Message sizes or format can be controlled using the message structure attribute of the message. For instance, in case of SMS message, the characters can be restricted to 160 characters. Refer to the documentation on the AWS website.Use cloud watch to keep track of stats about SNS.LambdaCompute service similar to EC2. Automatically manages compute resources. Requires zero administration. Supports Python, Java (v8) and Node.js. Lets people focus on code, not administration. Removes need to have a lot of servers for a simple task. Hooks very well into Amazon AWS services. Known as connective tissue for AWS services. Eg. Fetch a file when it's uploaded to S3, do something with it and then send a SNS push notification with the resulting output of data.Once created, lambda function can be invoked by:\tPush model by publishing an event.\tPull model by watching a stream or source (Kinesis stream or SQS queue polls)\tSynchronous invocation by calling a function or API directly from Lambda.Push event modelEvent source must itself invoke a lambda function directly. From Amazon Echo, S3, SNS or Cognito. Example: User pushes an item to S3. S3 then pushes an event to Lambda which then assumes an execution role to process the file and upload the processed file back to lambda.Pull modelLambda polls the event service and then on event does things. For example polling a DynamoDB or Kinesis stream. The polling does not affect the cost associated with making requests. Upon event detection, it assumes Execution role and processes the event.Synchronous modelInvoke function in lambda using RequestResposne invocation type. Function executes and then returns immediately.When granting permissions, the source of the event must have permission to execute lambda functions. Whatever lambda then does must be able to execute operations that it is trying to do. For example, s3 upload event must have access to execute lambda. Then if the lambda function is uploading records to DynamoDB, it should have access to insert a DynamoDB record.If the role isn't specified, it assumes basic lambda role. The basic role has the least privilege of being able to write a log file.When writing a function, choose the amount of memory that you want to allocate. It then allocates CPU power. Additional memory can be configured in 64 MB increments upto 1536 MB. All calls are limited to five minutes of execution time. The default timeout is 3 seconds but can be set to any value between 1 and 300ms. Duration is calculated to nearest 100 milliseconds.Lambda functions be scheduled for execution. For example, get MI data every hour. Schedule can be set for fixed rate at every hour or 15 minutes. A crown expression can be provided.Packages for lambda functions can go up to 50 MB in size.Format for specifying handler for a Java application is &lt;package name&gt;.&lt;class name&gt;::&lt;function name&gt;Functions take longer to execute the first time because it creates a new container and allocates resources to it. However its faster from there on.Very easy integration with AWS API gateway where based on a request, a lambda function can be executed. The incoming request parameters can be mapped to lambda function parameters using a mapping template. For incoming JSON data, the mapping template allows mapping using json path.Sounds like TDD is the best practice for writing lambda functions. All the functions should be strictly tested before publishing them.When subsequent updates are pushed to a lambda function, it automatically versions them. It allows reverting back to an old version if the latest is broken. One can also specify a specific version to lambda to execute. One can also define aliases to lambda versions. Example, dev can point to latest version 5, system test points to 4 and production points to 3. Aliases are just pointers so if one wants to put the latest version in production then one only needs to change the production alias to latest version, eg. 5.Depending on what a lambda function is doing, it could be a cheaper alternative to an EC2 server.Lambda functions must be written in a stateless way with minimal overhead as they will be executed in a stateless manner. Default soft limit for executing lambda functions in parallel is 100. Depending on use, this could be extended by a simple request. Its advised to use versioning and aliases to deploy lambda functions.Dry run is a feature with Amazon Lambda that allows one to check permissions without actually executing the lambda function.",
        "url": "/2016/03/02/notes-from-aws-developer-training-day-two/"
      }
      ,
    
      "2016-03-02-notes-from-aws-developer-training-day-one": {
        "title": "Notes from AWS Developer Training (Day One)",
        "author": "Manthan Dave",
        "category": "",
        "content": "BasicsService client API has objects for request and response data. Contrasting old way of retrieving things using the AWS SDK with the new way, it looks like they have switched from SOAP API to a RESTful API. The old way requires you to create Request and Response objects every time you want to do anything with the AWS API. Looking into the implementation, this looks a lot like SOAP.The new way is a lot more neater. A request can be built using builders that work on a conceptual model of the request. Response is also a lot more conceptual and easy to read.Signing requests with your credentialsSignature prevents the request from being tampered as the signature will become invalid as soon as someone changes the request in transit. It is also time based so that the replay attacks can be prevented.SDK signs requests automatically so normally no manual intervention is required.SDKsRegarding regions, when instantiating the service client, a region must be set. In all languages, except Java, if a region isn't specified, it will go and find the region specified in .aws/credentials file under [default].Exceptions are of two types in Java and .Net: AmazonServiceException is exception coming from the service itself, I.e. Remote. AmazonClientException is an exception happening on the client side due to bad request. This is slightly different in JavaScript due to callbacks. Here, callbacks are always in function(error, data) format.SDK will retry most requests that fail except ones that have service errors like access denied etc.A set of client credentials won't be locked out unless all the requests that are coming in look like an attack.AWS Data StorageS3, Glacier, DynamoDB, ElastiCache, RDS, RedShift.Every byte you change will need a reupload of the whole file. Not good for data that is changing constantly. Use S3 if you want your data to be read a lot more than changed. Highly scalable and durable. Allows hosting of static websites. Two plans: S3 Standard and S3 Standard - Infrequent Access.Use Glacier for files that you want to keep for long time. Archival solution, cheap long term storage. Retrieval is slow, about 3-5 hours to process a file retrieval. Old backups, auditing data, log files for keep sake should go in Glacier. Create a job in S3 that moves a file that hasn't been read in one or two months to Glacier and then another policy that deletes old data after 3-5 years from Glacier.DynamoDB NoSQL Key Value pair solution. For data that's not too complex or relational or transactional. Scales horizontally as per need.RDS is traditional relational database. Use for transactional or relational data. Scales horizontally with automatic failover to secondary instances. Allows variety of engines like Amazon Aurora, Oracle, MS SQL Server, Postgres, MySQL etc.ElastiCache caching solution. Store lightweight short lived data. Blazing fast retrieval. Allows creation and scale cache cluster. Based on memcache and redis.RedShift allows complex queries on massive data sets (petabyte scale). Data warehousing solution. Massively parallel model. Takes your query, compiles it and runs it in parallel across a large number of instances. Customers could take their load balancer logs, put them into RedShift and then compile list of customers and their frequency including their country of origin. RedShift provides JDBC and ODBC drivers to enable you to use familiar SQL client tools.Developing storage solutions with Amazon S3Data is stored in S3 buckets as objects. The bucket names must be globally unique. Bucket names: 3-63 chars, lowercase letters, numbers, hyphens and no periods.When storing files/objects in a bucket, prefer to use the path structure for file names so that rules can be created easily. Paths actually has no meaning to S3 but it's useful for us as it enables rule creation.For retrieval, use path style https://&lt;region specific endpoint&gt;/&lt;bucket name&gt;/&lt;object name&gt; or virtual hosted style http://&lt;bucket name&gt;.s3.amazonaws.com/&lt;object name&gt;.While uploading files, if the file size is &gt; 100MB, use multi-part upload. Usually this rule is for files that are &gt; 5GB.While reading, you can read the entire file or only read a specific range of bytes.While deleting, if versioning is enabled, on delete it will revert to the previous version if it exists. However, if versioning is not enabled then the file is permanently deleted. For objects or buckets backed up to glacier, they can be restored back at any time.Pre-signed URLs allows one to provide access to someone to upload or download a file to bucket to/from S3. Pre-signed URLs expire after a specific time period. This period by default is ….SSL-encrypted end points allow security of data in transit.S3 allows CORS to be specified. Add domains to it so that browser requests are allowed. Ex. Allow www.capitalone.com to access fonts bucket.For high performance, randomise part at the beginning of the key. This is because of the way S3 hashes URLs. If the beginning part is the same, all files end up on the same partition which will create bottleneck.Avoid unnecessary requests. S3 doesn't return back 304 for subsequent requests, it will always serve the file. Use a CDN instead.Verbose logging is allowed for S3. Use this only in development, and not on production system. Every HTTP request generates a request ID pair in logs. Reference this request ID when contacting AWS support.Storing data using DynamoDBCan store JSON documents but these must be looked up by the key only.Common use cases: Gaming, AdTech, Mobile apps, IoT. Send small data with very high frequency eg. Analytics. Record size must be less than 400KB. Each table has a key attribute which is the partition key. The partition key is hashed and then the data is put in that partition. Hence similar data must have same/similar partition key.Sort key is optional. For example, date of the transaction can be the sort key. This is the key that can be used for secondary lookup. For instance, if partition key is A and sort key is B, when doing lookup by A, it will return all data matching partition key A. However, we can narrow the search by looking up by partition key A and sort key B.DynamoDB is eventually consistent, usually within a second of the operation. Request can specify strong consistency but that means data might take longer to write and will cost more. Default is eventual consistency.Read capacity is reads per second of items up to 4 KB in size. Write capacity unit is in number of 1 KB writes per second.Important to choose the right partition key as it directly affects the throughput. The total throughput is divided across partitions so if a set of partition keys are being looked up more often than others Then it means that the partition key must be fixed or changed.The secondary index can be changed from the default.Streams allow firing of events based on an event. For instance, if user Jane changes her exercise from Walking to Spinning, then an update event is fired to all other systems notifying them of the change. Works much like Kinesis streams.Values can be retrieved matching a specific primary key condition. THis can be refined by providing a filter expression. If the data is looked up by a field that is not index, a scan operation will happen which is much slower than index lookup as it has to scan through the entire database looking for the record.Pagination: returns up to 1 MB of data or limit by number of items to be returned, like 2 records.Query can return many items, getItem returns a specific item by key. PutItem adds a new item. UpdateItem updates a specific item. RemoveItem means removing a specific attribute from an item while DeleteItem deletes a whole item. AddItem adds an attribute to an item.Writes provide free reads as the written value is returned when it's written. Write operations can be made conditional. For instance perform write operation only when account lock timeout has passed.In Java and .NET there's a persistence model available. This allows the client to do client side operations.DynamoDB local is a client side application that mimics DynamoDB service. Can be used for testing applications before production and avoid additional cost.Also comes with access management to control access to resources and actions.Best practiceMake sure that there are no hot (overused) partitions. Caching can help relieve this but ideally design partition keys so that this doesn't happen in first place.Separate data that is not used very frequently on a different partitions and then adjust throughout. For instance, latest data can go on a high throughput partitions and old data is moved to one of the older partitions with lower throughput.Use one to many tables instead of one table with large number of attributes. This is more scalable. Store frequently accessed small attributes in a separate table. Example separate company stock table from company details as the stock table will have information updated and read more frequently than details.Max size for value is 400KB, can store binary.",
        "url": "/2016/03/02/notes-from-aws-developer-training-day-one/"
      }
      ,
    
      "2016-02-11-decision-making-framework-for-community-of-practices-draft-2": {
        "title": "Decision making framework for Community of Practices (Draft 2)",
        "author": "Manthan Dave",
        "category": "",
        "content": "“Empowering, open and well-managed”EmpoweringEveryone in the CoP and outside CoP must be able to demonstrate idea, opinions and thoughts. The CoP must respectfully accommodate individual thoughts and factor in thoughts of every participating individuals when making decision regarding a new idea or thought process.If sufficient number of members are not present in the CoP to participate in the decision then the decision must be withheld until such people in required capacity are present. However, if required, due to a valid business reason, this can be overridden and an executive decision can be made with the number of members available at the time.In such a case, the community (when members in sufficient capacity are present) has the right to question, criticise and challenge the decision that has been made. If the executive decision maker is unable to provide reasonable reason as to why the decision was made (why it was made in absence of sufficient community members as well), the community has the right to undo / repair the decision, even to full extent where possible.Depending on the size of the community, it may not be possible to accommodate everyone in every decision all the time as not all members might be able to make it to the community all the time. In such a case one of the two below techniques can be used to distribute power.Two TwinsEach community member elects his/her counterpart. This is the person that they trust with their decision. Ideally, the counterpart must be unique i.e. if someone is already someone else’s counterpart then they can not be elected a counterpart again. If this is happening frequently then you might want to see the “Executive Lead” approach. The counterpart must be someone they trust with a decision and ideally from a different team.The idea of choosing the counterpart is that if someone isn’t available at a time when a decision is made then the counterpart can speak on their behalf. This is why this person has to be someone they trust will make a sound judgement. This helps reduce number of people who need to be present when making a decision and also helps avoid people making executive decisions when sufficient number of people aren’t available (as theoretically one person is representing two). This works best when members in the community are equal to or below 20.Executive LeadDivide the total members of community by 7, round it down and the result is the number of executive leads the community needs. An executive lead is someone that everyone in the community is comfortable with and is trusting to make a sound decision. In community’s absence a vote is still made within the executive leads but this helps reduce the number of people that need to be present.Being an executive lead also comes with responsibility. He/she is responsible for communicating a decision clearly to the community, along with the reasons if the decision has already been made. When participating in a decision, the executive lead must also think about the wider impact and not just impact in his/her immediate vicinity (per se). The community has the right to demote an executive lead if sufficient number (see impact sizes) think that they’ve had a negative or not much impact in the role.The impact size agreement levels also apply to the executive leads. Unless the required number agree, the decision must not be made otherwise. Executive leads nullify the need of having to make an executive decision as they are the community representatives.Depending on size of the community and type of the organisation executive leads can have more responsibilities that help them justify the need for them to stay being an executive lead. This could range from empowering individuals beyond their potential to influencing the range of impact a particular community has.Ideally the community should cycle their executive leads once every couple of months, depending on cadence.OpenEvery decision that goes through the community must be communicated very clearly and in “as is” format. Any community member is encouraged to challenge the decision no matter how senior a person who has proposed/made the decision is. For this, open forums, wiki pages and online chat (eg. slack) can be used.When an idea is presented, it must be sized according to its impact. This could work similarly to story points (range 1, 3, 5, 8, 13, 20). Breakdowns according to impact are as follows:Impact Size: 1, 3Low impact. Eg. Someone proposing a minor non breaking change in a library that impacts a small project and future projects.Needs agreement from 60% of the community.Impact Size: 8, 13Medium impact. Eg. Someone proposing a major non breaking change/feature in a library that may impact multiple existing projects and all future projects.Needs agreement from 70% of the community.Impact Size: 20, 40High Impact. Eg. Someone proposing a change in framework that is being used. This change impacts current projects and all future projects.Needs agreement from at least 80% of the community.Well-managedCommunity refines its own processes to make sure that they stay current with changing times. This can be achieved with a retrospective every once or twice a month (depending on cadence). Defect within a process should be raised like a normal issue or idea and discussed as such. The community as a whole has the power to improve, challenge, criticise and disagree with its own processes. These processes shouldn’t be treated as a barrier but more like a malleable guideline that is in place to grow the community. If it is not being used in line with the ideology then the community is empowered to replace its processes with ones that it sees fit.The goal of the community is to create a utopia where people can openly voice their ideas, share opinions and collectively improve the organisation as a whole.",
        "url": "/2016/02/11/decision-making-framework-for-community-of-practices-draft-2/"
      }
      ,
    
      "2016-02-10-building-a-machine-learning-recommendation-system-in-under-a-minute": {
        "title": "Building a machine learning recommendation system in under a minute",
        "author": "Manthan Dave",
        "category": "",
        "content": "Theoratically there are only two steps to this (well, three if you count the cd).Clone GitHub repository available here.git clone https://github.com/manthanhd/apache-mahout-recommendation-starter.gitChange to that directory using cd. Assuming you have gradle installed, run:gradle clean runYou should see output like:11:47:58: Executing external tasks 'clean run'...:clean:compileJava:processResources:classeslog4j:WARN No appenders could be found for logger (org.apache.mahout.cf.taste.impl.model.file.FileDataModel).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.:runRecommendedItem[item:12, value:4.8328104]RecommendedItem[item:13, value:4.6656213]RecommendedItem[item:14, value:4.331242]BUILD SUCCESSFULTotal time: 0.52 secs11:47:58: External tasks execution finished 'clean run'.The highlighted lines are the recommendations that have been generated. The training data set for this can be found in src/main/resources/dataset.csv file. You can also find it here.From those highlighted lines, the item:12, item:13 and item:14 refer to the ItemId (second column in data set) and value:4.83, value:4.66 and value:4.33 refer to the recommendation strength. This range depends on the implementation.public static void main(String[] args) throws TasteException {    Starter starter = new Starter();    starter.recommendFor(2, 3);}Looking at the code in Starter.java file, the recommendation is made for UserId (first column in data set) 2. By providing 3 as the second argument we’re asking for 3 item recommendations for this user.Well, so now that you have the basic framework in place, feel free to play with the UserId values for recommendation, recommendation sizes or even with the data set itself. The more training data you add to the data set, the more accurate its recommendations will be.",
        "url": "/2016/02/10/building-a-machine-learning-recommendation-system-in-under-a-minute/"
      }
      ,
    
      "2016-02-06-what-ive-learnt-about-writing-expressjs-applications": {
        "title": "What I&amp;#8217;ve learnt about writing Expressjs applications",
        "author": "Manthan Dave",
        "category": "",
        "content": "There’s a lot of ways you could write an express application. If you searched for Express app generator, you’ll probably find at least five in the first page of search results.In my opinion, most of these are designed for hackathons - to get you up and running as fast as possible but also as crudely as possible. While they are good when you want to try something out quickly, most of that code will turn into a big pile of mess as your application grows.I’ve been in this situation before. I get my application up and running as fast as possible and in order to get my first MVP done but then quickly realise that the auto generated code that was meant to accelerate my development is now slowing me down. To remedy this, this is now how I develop my node.js applications.Just a quick note. I still use the ‘express-generator ’ module for all my express applications.Every Route  file should have its own function. Consider this:// file: routes/IndexRoute.js function IndexRoute(express) {     var router = express.Router();     router.get('/', function(req, res) {         return res.render('index');     });     return router;}module.exports = IndexRoute;Now, implement this in app.js  like so:// file: app.js var express = require('express'); ... var indexRoute = require('./routes/IndexRoute')(express); ... app.use('/', indexRoute);I quite like this approach because it keeps code clean and avoids having to use require everywhere. Also, by just looking at the module you can figure out what it depends on. For instance, here in IndexRoute.js  file (above) you can tell that it depends on express. For more complicated modules that require additional dependencies you can keep adding them in the module function declaration. For example:function UserRoute(express, userModel, moment) { ... }module.exports = UserRoute;Here the UserRoute clearly depends on express, userModel (mongoose model) and moment (npm moment.js).Sometimes when developing cool and awesome stuff, it is easy to get carried away. For instance, in the case above, the UserRoute  is doing too much. It’s doing something with time using moment and also manipulating user in the database using userModel . While it’s borderline fine (with massive cringe), you might want to think about what the UserRoute  should actually be doing.In most cases, the way I group responsibilities is following. Everything suffixed with Route  manages Expressjs route mappings. One level down is Controller  which actually manages what’s being done to that endpoint that has been mapped. Another level down is Service . This layer is more intelligent than controller and most of your logic should go in here. Lastly, you have Model . In case of mongoose this is just a model definition but there’s nothing to stop you from doing more complex data interactions here.This kind of code separation really helps as it is highly cohesive which in turn makes it testable. Now you can test the service and inject all its dependencies directly using the function parameters. Also, when you change definition of a service, it will break all your tests and yes that is a good thing because those broken tests will tell you the size of your impact crater that the change has created. If you didn’t have those tests in place, you might have never known and maybe spent hours debugging trying to find out what went wrong. I know this because I’ve been there!One more advantage of this that I can see is that it will stop you from accidently redefining something in your code that you would’ve if you used require all the time. This is especially true with Mongoose model definitions. It doesn’t like it when you define a model twice and that is really easy to do when you’re using require. Using this method, you require your model once in app.js and then pass it down to every service that requires it. In some cases it could also make your code more efficient as you aren’t redefining objects evey time something like a service call is made.I’m still working on finding the best way to write applications. Every time I write a new application, I learn about a new problem which I then set out to solve. I hope you do too!",
        "url": "/2016/02/06/what-ive-learnt-about-writing-expressjs-applications/"
      }
      ,
    
      "2016-01-22-gulpfile-js-for-vanilla-angularjs-projects": {
        "title": "Gulpfile.js for vanilla AngularJS projects",
        "author": "Manthan Dave",
        "category": "",
        "content": "Since I’ve learnt Angular, most of my front-end web applications are written in it. Its brilliant. Everything works fine, up to a point where your application has grown 10 times the size it was since you started it. At some point you’d want something to accelerate your delivery. That’s when gulp comes in.Gulp is kinda like a build tool but for javascript based applications. It runs on Node.js and is available on npm. All you need to do is:npm install -g gulpor if you are using a unix based application:sudo npm install -g gulpThat gets you access to the gulp command line. Once you have gulp installed, you need to something that tells it what to do with your project. Unlike many other build tools, gulp favours code over configuration. This means that rather than writing XML or YAML files that define the tasks, you write code. This is what makes gulp unique.So, here’s what I have. Below is my gulpfile.js that I use in most of my AngularJS projects:/*** Created by manthanhd on 04/01/2016.*/var gulp = require('gulp'),gutil = require('gulp-util'),sourcemaps = require('gulp-sourcemaps'),concat = require('gulp-concat'),ngAnnotate = require('gulp-ng-annotate'),watch = require('gulp-watch'),uglify = require('gulp-uglify');gulp.task('default', ['build-js']);gulp.task('build-js', function() {return gulp.src(['public/app/**/*.js']).pipe(sourcemaps.init()).pipe(concat('bundle.min.js')).pipe(ngAnnotate())//only uglify if gulp is ran with '--type production'.pipe(gutil.env.type === 'production' ? uglify() : gutil.noop()).pipe(gutil.env.type === 'production' ? gutil.noop() : sourcemaps.write()).pipe(gulp.dest('public/dist'));});gulp.task('watch', function() {gulp.watch('public/app/**/*.js', ['build-js']);});Most of my AngularJS code resides in public/app directory and hence you’ll see that in multiple places in that file.A quick glance should tell you that this minifies all of my AngularJS code and combines it all in one big file called bundle.min.js which is then placed in public/dist folder. My HTML page is configured to only reference bundle.min.js.When executed normally, it doesn’t minify the code because it assumes that the javascript code is being used for development. However, when I run gulp --type production it minifies the code and doesn’t write sourcemaps.You can install all of this by running:npm install --save-dev gulp gulp-concat gulp-ng-annotate gulp-sourcemaps gulp-uglify gulp-util gulp-watchOh and here’s my package.json: {\"name\": \"my-sample-angular-project\",\"version\": \"0.0.0\",\"private\": true,\"scripts\": {\"start\": \"node ./bin/www\"},\"devDependencies\": {\"gulp\": \"^3.9.0\",\"gulp-concat\": \"^2.6.0\",\"gulp-ng-annotate\": \"^1.1.0\",\"gulp-sourcemaps\": \"^1.6.0\",\"gulp-uglify\": \"^1.5.1\",\"gulp-util\": \"^3.0.7\",\"gulp-watch\": \"^4.3.5\"}} ",
        "url": "/2016/01/22/gulpfile-js-for-vanilla-angularjs-projects/"
      }
      ,
    
      "2016-01-15-waiting-for-tomcat-to-start-up-in-a-script": {
        "title": "Waiting for tomcat to start up in a script",
        "author": "Manthan Dave",
        "category": "",
        "content": "So here’s something that I have found that waits for tomcat to come up but rather than polling or a static time based wait, it uses FIFO pipeline to wait.function isTomcatUp {           # Use FIFO pipeline to check catalina.out for server startup notification rather than    # ping with an HTTP request. This was recommended by ForgeRock (Zoltan).       FIFO=/tmp/notifytomcatfifo    mkfifo \"${FIFO}\" || exit 1    {        # run tail in the background so that the shell can        # kill tail when notified that grep has exited        tail -f $CATALINA_HOME/logs/catalina.out &amp;        # remember tail's PID        TAILPID=$!        # wait for notification that grep has exited        read foo &lt;${FIFO}        # grep has exited, time to go        kill \"${TAILPID}\"    } | {        grep -m 1 \"INFO: Server startup\"        # notify the first pipeline stage that grep is done        echo &gt;${FIFO}    }    # clean up    rm \"${FIFO}\"}Drop this into a tomcat-util.sh file, make it executable and then source the file:chmod u+x tomcat-util.shsource tomcat-util.shYou’ll now have isTomcatUp available as a bash command. ",
        "url": "/2016/01/15/waiting-for-tomcat-to-start-up-in-a-script/"
      }
      ,
    
      "2016-01-11-the-restful-way-of-the-apis": {
        "title": "Some thoughts on the RESTful way of the APIs",
        "author": "Manthan Dave",
        "category": "",
        "content": "So, it's been a while now since I have written a post reflecting my thoughts on a subject. It has been primarily like this since I've been busy doing what I always do - trying out new cool things and ideas.In almost every app, every SaaS platform or every web based backend system I write, the first thing I start with is an API. This API first attitude has helped me in many ways. First and foremost, it helps me think how the architecture must work to suffice the needs of the client. It also leads me to think about containers and how best to distribute load etc. Now, this might seem like an overkill when just trying out a new idea but it at least gives me a rough estimate of the scope of the project that I'm looking at. Besides, now a days, working with thick client frameworks like Angular JS almost demands that you have a strong backend with a well written collection of APIs.Writing restful APIs time and time again have helped me improve my understanding regarding the idea of \"restful APIs\" with every iteration and hence, I've come up with few thoughts regarding ways in which a RESTful API should be written. So, here we go.A URL path should contain singular nouns and nothing else. This comes straight out of the REST specification. However, there have been cases where I have been forced to use verbs but that has only been the case when I haven't been very clear about what that endpoint is doing. Before defining your endpoints, be very clear on what you are trying to achieve. For instance, once I was writing an endpoint for an application to process an incoming web hook. Hence, I named the endpoint \"POST: /process\". While I actually did implement this, it kept bugging me throughout the implementation of the application. Finally, when that piece of functionality was coming to an end did I get crystal clear understanding regarding what that endpoint was doing. When a web hook gets triggered, things happen within the application. THAT thing is an Event! Without doubt, I quickly renamed the endpoint to \"POST: /event\" and my mind was at peace. In retrospect, I think I should've created a separate endpoint for each application consuming that endpoint.While developing one of the APIs, I had a devious idea. Normally when defining an endpoint that allows one to retrieve a resource, say for instance a post object, one's target endpoint might look like \"GET: /post/1\" where \"post\" is the resource and \"1\" is the ID of the resource. But what if I wanted to select posts by their category ID? Would my target endpoint look like \"GET: /post/1\"? If yes then whats the difference between the first and second call? How about \"GET: /post/byCategory/1\"? Or even \"GET: /postsByCategory/1\"?For some time I had a crazy obsession with such endpoints to an extent that quite a few endpoints during that time were defined like this. However, after a while these endpoints just started to look weird. Also, when I started developing the frontend AngularJS application, making queries started to feel more difficult than it should've been. For instance, I had a dropdown that allowed a user to select the search type (category|tags|name) and then whatever the user typed in the search box got searched. Initially I had a horrible if|elseif|else blocks which chose which endpoint to call based on selected search type. However, this quickly became really boring, tedious and frankly, dirty. So, I revisited my endpoints again. If I, as someone who had made these endpoints struggled to develop an application using them, then what should I expect from the developers who will build their applications on my platform?Finally, I ended up having a couple of thoughts regarding how this should be done. \"GET: /post?categoryId=5\" was my final answer. This not only solved the problem of querying by multiple search parameters but also made it easier for the consuming application to call the endpoint. Now, this new endpoint meant that I only had to assign field names to each dropdown and the query would build itself!Query strings represent things that you want to do to your resource. This is normally fairly obvious and I will cover more of this in depth in later paragraphs but for starters, query strings can be nouns, adjectives or even verbs (rare) with values or switches on them. For instance, \"GET: /person/1?gender=male\" clearly means that give me a person with ID 1 whose gender is male.You can also do other neat things with query strings like provide metadata regarding your call. For instance, \"GET: /person/1?_select=firstname,lastname\" means give me person whose ID is 1 and only select firstname and lastname fields of each object. This kind of functionality is especially useful as it helps clients save their precious processing power, memory and network bandwidth. Think of small micro controllers displaying temperature information or mobile apps running on 2G or even Edge network. Any query parameter indicating metadata should begin with an underscore so that they can easily be differentiated in a pool of query parameters.Never ever build a query within a query. I saw some examples on the web where someone had built an endpoint like \"GET: /post/?query=startsWith:Senate,hasMinWords:100\". I mean, why? How does one even go thinking like this? This is a horrible idea because you're forcing your users to learn your query language even when the principles you're building this on (REST) provides a framework like query string for you to use. The above query within query can be broken down as \"GET: /post?startsWith=Senate&amp;hasMinWords=100\". This is perfectly acceptable.You could ask, what if I wanted to search posts by their post header, body and tag components? Well, you could just do \"GET: /post?headerStartsWith=Senate&amp;bodyHasMinWords=100\" or even \"GET: /post?header.startsWith=Senate&amp;body.hasMinWords=100\" although the first one reads better but might be slightly tricky to implement.Respond properly, don't be lazy. When it comes to responses, it's easy to forget how important it is to respond properly. In most of the APIs that I write, for every POST request, I like to respond with HTTP 200 returning the object instance that was created as a result of that request. The same goes for PUT, but not for DELETE. The name of that HTTP method, DELETE clearly means that the resource was deleted, i.e. it doesn't exist. It makes me sad when I see implementations online where people return the resource that was deleted. If it was deleted what can the client possibly use it for? Hence, for DELETE, the response should be HTTP 204 no content.Now, depending on how secure you want to make your API, the behaviour of your endpoints on error will differ. It is incredibly useful to respond with a stack trace in a development environment but probably not when it comes to production. It pays in the long run to respond in a meaningful way when something goes wrong. At the very least, you must respond with HTTP 400 when it is the client's fault, for example validation of the input data gone wrong. However, there are tons of other error codes you could use for other situations.Sometimes you might have a protected resource, something that requires a login. In situations such as these, there are two ways you can respond, 401 (Unauthorized) and 403 (Forbidden). Typically, I like to respond with 403 when user is not logged in, i.e. not authenticated and 401 when the user is authenticated but doesn't have enough privileges i.e. is not authorized to do that operation. Never ever redirect to login page or API endpoint when the client is trying to do something that requires login.There is no shame in responding with HTTP 500. Yes you read that right. HTTP 500 does not mean that your app has crashed. It only means that something has gone wrong at the server side and the server doesn't want to tell you about it. Think about it. If the app had crashed, you wouldn't receive any response. It'd just die. When writing any API during my first iteration, I just respond with 500s in all places where I think things might go wrong. I normally then go back and improve things one by one, handling errors carefully and responding in a helpful way.Now, a word on caching. As far as I know, most containers and frameworks handle caching internally. However, sometimes you might need to configure stuff at endpoint level. For instance, if your endpoint is outputting information that gets updated as part of a batch process that occurs infrequently, it will be incredibly useful to use the expires headers. Also, when responding to a request, if the if-modified-since header is indicating that the data that you have on the server hasn't changed since last time, respond with 304. This is incredibly useful especially when your client is a webapp running in a web browser as it will help improve client experience dramatically.Login requests must always be a POST request. I get this a lot. If you ask this to me a couple of years ago, I'd probably say that the objective of Login is to \"get\" a session and hence it should be a GET request. I don't even know how I got that logic but one of my colleagues opened my eyes when they mentioned that the objective of Login is not to GET a login session but to CREATE one and hence it is a POST request. Same goes for registration. Your response at this stage depends. You could either respond with the user object which contains information about the logged in user or you can just return with 204. However, bear in mind that you'll have to strip out sensitive information like passwords and security question answers when responding with the user object to avoid security nightmare.However, for consumer facing applications, game has slightly changed with the introduction of social logins. In this case, you'll be making everyone's life easier if the initial request to login via social network was a GET request. As a general rule of thumb, I normally go with \"GET: /auth/&lt;provider name&gt;\" type of format when introducing social logins where in case of Facebook, this becomes \"GET: /auth/facebook\".So I hope this has given you something to think about. Let me know if you have any questions, comments or feedback. I'd love to hear your thoughts on this!",
        "url": "/2016/01/11/the-restful-way-of-the-apis/"
      }
      ,
    
      "2016-01-10-notes-from-techcrunch-disrupt-london-2015": {
        "title": "Notes from TechCrunch Disrupt London 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "Day OneSevenhugs remoteEU startup. Easy control of Phillips hue lights. Maps the person with the remote into a 3D space. Could potentially be used for indoor navigation?Point the device at any smart bulb or device and instantly control it.Quite expensive. €200 to produce the remote.MainframeContext sensitive instant messaging with backwards compatibility to email. AI that goes through your email/messages looking for actions. Based on content and context, it converts them to tasks/calendar events/reminders. Pretty cool. Also does same thing for incoming email messages, which means that incoming email acts as an instant message. When sending instant message, if recipient is an email, it creates action links which allows the recipient to integrate with the system.MaxSA startup. Delivery provider in Lagos. Allows delivery in 3 hours. Drivers drive motorbikes to escape traffic. Customer website allows booking of deliveries with ease. Have launched APIs and are helping retailers integrate their platform with them. The delivery guys are also being trained to be sales agents. They pitch their product, increase awareness.YayPayAccounts receivable office for every small business. Setup invoice on YayPay. Client gets the invoice and can pay using various pay,net methods. Also allows clients to pay a deposit. Allows sending up a collection process, like sending an email two weeks before due date, SMS reminders two days before, call after a week after due date and then emails every two weeks. Uses machine learning to determine best payment method.FairFleetMunich, Germany. Inspect your construction site using drones. Crowd sourcing model. Choose the construction site you want to get inspected. Mark area of inspection. Set accuracy. Get accurate photos daily. Go back in timeline to inspect progress. Uses MapBox API for geo mapping.JukeDeck (winner of the startup battlefield)Artificially intelligent music player. Have build software that composes original music note by note. Google and the royal family have used their music. Choose the length, mood, style and it creates music from scratch. Most awesome demo with live rap.Move bubbleConsumer property app. Property feed. Learns with use. Allows setting up appointments with property owners.YoobicApps for helping sales employees work better with sales management team. App allows publishing of goals and missions that can be delivered wirelessly to various stores. Sales employees of their stores will get notified of available mission. App can also be used for inventory management (sort of). Analytics for viewing various metrics like sales percentages for various products etc. Claims can increase sales by 25%. Collaboration platform for brands and retailers.CarumaA device with two cameras, front and back, 4G, and wifi hotspot. Provides security to prevent theft. Uses motion and face detection to check if something is wrong. Sounds alarm even if the driver is dozing off. Live video recording and upload capabilities. Comes with the app that notifies when something's wrong with the car.Day twoFireside chat with CEO of TwilioJeff Lawson. Used to work at Amazon web services. Observed that communications is key when building a customer centric business. In those times, comms costed a lot and required a lot of hardware infrastructure. Set out to create Twilio to make software driven communications that can be setup in matter of minutes. Strongly believes that the ability to experiment stuff very quickly is key to innovation and hence has built a business that allows developers to do exactly that.Chat with Hassle.comAlex Depledge. Website to get house cleaners on demand. It is important to be naive when moving into a new industry. She was able to make such an impact in cleaning industry because she was naive. Lost a lot of money, down to £100 but kept pushing. She and her team coded all Christmas and launched the product in January and made more money in that month than all in last six months combined. Strongly believes in free market which allows movement of labour. Talks about gender equality. Started “Girls can code” movement and found that there are women in tech but they are just aren't recognised. Also, at the younger age, there is a serious lack of awareness about technology. For instance, girls didn't know what a UX developer was or what a full stack developer did in their day job.Thinks that government legislation needs to be refactored. The laws that govern today's business have been created in old times, situation that is probably not relevant anymore. Worse thing than refactoring code is writing more and more code on top of old code.",
        "url": "/2016/01/10/notes-from-techcrunch-disrupt-london-2015/"
      }
      ,
    
      "2015-10-22-notes-from-nganimate-2-0-at-angular-connect-2015": {
        "title": "Notes from ngAnimate 2.0 at Angular Connect 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "Day two14:30ngAnimate 2.0 by Robert MasserieNo longer css. Css doesn’t allow complex logic. Using domain specific language (DSL).Json file dictates chaining and choreography. Json is limited as it doesn’t alow dynamic logic. Represents static data.Now using full DSL with full programmatic API.ngAnimate 2.0 is a proof of concept as of now.Use of animation factory to create animations. Still allows one to define static animations but one can also use the programmatic API to include complex logic in animation flows. Seamlessly switch between parallel and sequential animations.Stagger animations by specifying key frames at various points in progress. For instance, apply colour change at 25%, and scaling at, 75%.It comes with better event integration. For instance trigger a modal animation from the point of click on the button. This is possible using the programmatic API.Now talking about Material Design.Pretty difficult to do using angular 1 as some of the animations originated from the point of click.Now, listen on click events and then run the ngAnimate functions to handle the animations programmatically.Tab animations are tricky because it animates the tabs themselves, the ink bar that transitions left or right and the actual content below. This is implemented in angular 2 by setting the tab index and then programmatically loading the rest or the animations I. E. Left or right based on what the tab index is.Now talking about what ngAnimate will be able to.Rich unit testing, performance tuning, migration and adaptive styling look like interesting goals.Animation asserts help test if an animation sequence has happened or not in unit test.Addressing performance by no longer doing reflows. Also pre calculating delays to provide more efficient animations.Considering an upgrade path from ngAnimate 1 to 2.Ability to control animations at any stage. Pause, fast forward or reverse. Showed a demo with a slider controlling full animation sequence of a mix of asynchronous and synchronous animation flows.",
        "url": "/2015/10/22/notes-from-nganimate-2-0-at-angular-connect-2015/"
      }
      ,
    
      "2015-10-22-notes-from-modern-authentication-solutions-using-oauth-2-0-openid-connect-and-angularjs-at-angular-connect-2015": {
        "title": "Notes from Modern authentication solutions using OAuth 2.0, OpenID Connect and AngularJS at Angular Connect 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "Day two0945Modern authentication solutions with oauth 2.0, openid connect and angularjs by Manfred SteyerOne client has too many accounts. One application is bound to many other applications. Lack of trust between applications.Oauth was developed at twitter and ma.goliaStandard for delegation of restricted rights.Client wants to access a resource server on the behalf of the user who owns the resource. The authorization server has access to the resource.Client sends the scope to the authorization server. The auth server then asks the user if the client should be granted permissions. The user then agrees if he wants to and then the auth server then sends the access token. Which the client then uses to get access to the restricted resources.Advantage is the client doesn’t have the password. The authorization server helps segregate duties and concerns between parties.FlowsAuthorisation code grant is designed for server side applications.Implicit grant is designed for single page applications.Now talking about Implicit grant.Openid connect defines how to use Oauth for authentication. Client gets id token which is a jwt token with information about the user. Can be signed by the issuer.Id token is for the client while the access token is for the resource server.Now showing the demo.It’s a single page application that provides some vouchers. To buy the voucher the user has to be logged in.Showing the oauth network flow using fiddler.",
        "url": "/2015/10/22/notes-from-modern-authentication-solutions-using-oauth-2-0-openid-connect-and-angularjs-at-angular-connect-2015/"
      }
      ,
    
      "2015-10-20-notes-from-extreme-programming-in-a-nutshell-at-angular-connect-2015": {
        "title": "Notes from Extreme Programming in a nutshell at Angular Connect 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "13:30Day oneExtreme programming in a nutshell.Extreme programming has practices that keep you safe. A bunch of practices that act as a safe net against failure.Of all the practices, going to talk about pairing, test first and continuous delivery.Starting with pair programming. Similar to Pacific Rim.All production code is developed by more than one person. When pairing, design decisions are discussed between pairs.Two roles, navigator and worker. Switch pairs everyday to make sure that everybody is exposed to the code. Everyone is responsible for how the code works.Make best use of the team expertise. Use everybody’s skills to the fullest and enable cross functional skills share. Allows sanity checking of ideas, avoiding rabbit holes.Taking pair programming to more extreme level. Mob programming. Taking the driver and navigator idea to a team. Whole team sits around a screen with one person with the keyboard.Driver types while team navigates. Run a timer to switch roles in and out. Kinda like coding dojo but do it in an open workspace. 3 or 4 people works the best. Fits nicely into size of ideal agile teams.This means that if one of the person has to go to the meeting the work continues.Use spikes. Spikes are done solo. Mob might split up and then do spike of their own. Each spike is an investigation. Prove out an architecture or a design idea and then bring it in and explain it to the team. Code produced is thrown away. This is done to maintain code quality and not use something that was developed in a rush.Don’t use code reviews. The process causes a longer delay between creation and release. Also causes bottleneck. Pair / mob programming removes the need of this.Now talking about Test First.Gives confidence to deploy and refactor. Avoid manual mistakes. Keeps focus on what we are building now instead of other distractions on what it might be in future.Create acceptance test first. Helps focus implementation. Make sure we only build what we need. Helps us with continuous delivery.Now talking about continuous delivery.Deploy several times a day. From implementation to live in a couple of hours. Keep development pipeline short.Use extensive monitoring to help track impact of releases. Measure various metrics, from hard metrics like CPU monitoring to actual speed of responses.Always committing to master. Avoid problems by decouple releases from development. Code is always released but use feature toggles to turn new features on or off. Settings based on environments.Toggles don’t stay for forever. Once it’s production ready and proven, it gets taken off and it’s permanently on.Use mutex for deployments instead of ci pipeline. Use deployment tokens like a teddy bear to allow teams to deploy to production.Now top 3 tips even if you’re not in extreme programming      Add monitoring. Use it even if you’re not using continuous delivery. Even if code is not changed, the environment might change. Start with generic health check and then proceed to what you need.        Test first bug fix. Diagnose the issue and the  reproduce the steps. Once done, write a test that reproduces the issue.        Pair code reviewsReview code while pairing. Eliminate bottlenecks.  Add practices from XP incrementally and see what works for you.Tech.unruly.co",
        "url": "/2015/10/20/notes-from-extreme-programming-in-a-nutshell-at-angular-connect-2015/"
      }
      ,
    
      "2015-10-20-notes-from-routing-in-eleven-dimensions-with-component-router-at-angular-connect-2015": {
        "title": "Notes from Routing in eleven dimensions with Component Router at Angular Connect 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "12:00Day One.Routing in eleven dimensions with component router by Brian FordRoute configuration. Mapping url to component.Route configuration meta data decorator helps to do this in angular 2. Use path parameters style (same as express) to define variables in routes when mapping.Inject route paramo service then in the target component to give access to those route params.Use square bracket router-link (property binding) directive to provide links between routers.From the RouteConfig, one can also link the routes by their id field (as attribute). If the route url has parameters then a map with parameters must be passed as second attribute to the property binding.This new Link DSL is robust and survives refractors better than URLs. Also provides a safety net in case one of the parameters aren’t met.Child routes allows embedding routes within routes. Basically nested routes.To make this work, add route config with router outlet directive in the template. Denote if that route has child route by suffixing url with /…URLs to child routes are relative to the url of the parent route. They have their own parameters so don’t need to worry about collisions. Could use id within parent route it the prefix of ./ notation.Now talking about auxiliary routes. Each component can have zero or more auxiliary routes.URLs are/parent(parent-aux)/child(child-aux)A modal is just an auxiliary routes. It can be thought of like tree of trees.",
        "url": "/2015/10/20/notes-from-routing-in-eleven-dimensions-with-component-router-at-angular-connect-2015/"
      }
      ,
    
      "2015-10-20-notes-from-full-stack-angular-2-at-angular-connect-2015": {
        "title": "Notes from Full stack Angular 2 at Angular Connect 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "11:30Day oneFull stack angular 2Why full stack?Why angular?Best tool for the job doesn’t always win. Sometimes it’s political driven.Communication problems. Context switching adds unnecessary overhead.Same things needs to be implemented many times over and over again.Committees, contention, context switching, code duplication. Problems in today’s world. Time wasted is significant.Challenges to use same technology across whole stack. Javascript gets bad rep sometimes as it used to be used only in the web browser.ES6 and ES7 plus angular 2 will make it more and more prominent for use in full stack.Lots of features makes it powerful. Dependency injection is one of the examples.Complex flows like communicating from browser to server to database still results in code duplication, even with javascript.Use dependency injection, one way to eliminate dom references, making it faster.Still DI is not ideal as DI adds complexity overhead.Demo is showing DI in action, switching implementation quickly by only modifying one line of code.@injectable annotation makes it easier to use DI.Now talking about Angular 2 Universal.Write client side app and just install the server side plugin to allow easy access via model. Basically converting a client side application to do server rendering.Example showing conversion of client side application into server side.Importing client side main angular app file on server and then setting it as model object to pass on to render method for rendering index view.Using ng2engine module from angular2-universal-preview module. Setting instance of that as app engine for all .ng2.html files and then binding ng2.engine as view engine in express app.Now reloading the server. Now the view is rendered on the server side!Working towards supporting more server based environments.Full stack angular 2 transforms full stack Web development.bit.ly/ng2stackFullstackangular2.com@jeffwhelpley @gdi2290",
        "url": "/2015/10/20/notes-from-full-stack-angular-2-at-angular-connect-2015/"
      }
      ,
    
      "2015-10-20-notes-from-whats-new-in-typescript-at-angular-connect-2015": {
        "title": "Notes from What&amp;#8217;s new in Typescript? At Angular Connect 2015",
        "author": "Manthan Dave",
        "category": "",
        "content": "1100Day OneWhat’s new in Typescript? By Bill TicehurstGoals is to make javascript development more productive and enjoyable.Hard to work in code bases with lack of types. Hard to debug.Superset of javascript that compiles into plain javascript. Simple mapping. Makes it easy to debug, back and forth.ES 6 sounds futuristic. Once you get used to it, it’s hard to go back.Static types allow better documentation which in turn gives better tooling. Allows jumping around points in code easier.Types are optional. Don’t need to have types everywhere. Kinda like code coverage. Do what you need.Now showing demos. There’s a plugin available for sublime which helps writing Typescript easier. Now urging audience to find bugs in presented demo code. Interfaces help debug code as it sets expectations for assignable types.Live stack trace. Bugs and trace disappears as you fix them. How cool is that?Debugging in this thing is amazing. Errors are helpful and intellisense is better as the code knows things!A variable can have more than one type. Like string or a string array. In this case, the Typescript expects common methods in both types to be valid. If specific methods needs to be used then need to cast or parse it.Generics!“this demo has blown my mind.”Now showing a demo with angular.Imports works dynamically with node modules. It just works!Urging developers to start distributing Typescript files along with their modules.Decorators are able to interfere with the declaration of an object. Logger decorator injects function before execution of the target function.Reflection API. Decorators don’t modify the object but adds metadata using the reflection API.Reflect.getmetadata(“annotations”, varname)Embed html code directly in javascript in jsx files. Typescript allows autocompletion in jsx files for both, html and Typescript / javascript. Coolest thing I’ve ever seen.Powerful project wide find references tooling + variable name refactoring. “is this available in all Typescript plugins?”ES 6 async and await looks incredibly powerful way to control async functions execution flow.await Promise.all(arrayOfPromises)Waits for all promises to be fulfilled before executing any of the remaining code.",
        "url": "/2015/10/20/notes-from-whats-new-in-typescript-at-angular-connect-2015/"
      }
      ,
    
      "2015-10-20-notes-from-angular-connect-2015-keynote": {
        "title": "Notes from Angular Connect 2015 Keynote",
        "author": "Manthan Dave",
        "category": "",
        "content": "0940Day One.Keynote.Makes it simpler to render stuff by pre rendering everything by adding a build step.Angular 2 alpha build blows every other mvvm framework out of the water.Angular 2 drops a lot of current directives by introducing property binding which allows binding models directly to properties like value. Drops about 43 directives.Template directive plus controller in angular 1 is now a component in angular 2. Use @Component from typescript.Es5/6, typescript and Dart are supported for using Angular 2.ES6, typescript and Dart, all need compilation. Typescript and Dart introduces types.Dart does not have a Javascript syntax.Angular 2 is written in typescript.Incremental releases. About 57 in last two weeks.Igor minar now presents tooling in AngularJSConfiguration is a major barrier to tool adoption.Github.com/angular/angular-cliStart a project byng new greetings-acCreates initial project scaffolding. Creates a build pipeline based on Typescript.Start a project and see live reload. Helpful error messages with stack trace right in the browser.Use ng generate component componentnameTo generate new component files including css, template and unit test spec files.Deploy to github byng github-pages:deployTo deploy directly to github pages.Now talking about angular batarang. Allows inspecting bugs in chrome.New called Batarangle. rangle.io/batarangleNow talking about Ionic Framework, Max Lynch and Adam Bradley.About 1.2 million Ionic and angular apps since 2/2014. Used by startups to the fortune 50.Working with angular 2 to optimise Ionic architecture to be ready for adoption.Reduce barrier to entry by allowing mostly ES2015/TypescriptOverhaul in the navigation system allowing apps to push boundaries.Material design support. New animation and scrolling. Powerful theming.Ionic 2 is ready to use today. ionic.io/2Ready enough to use but be warned of bugs.Giving out Angular themed Google Cardboard.Now talking about NativeScript, Dimo Iliev.Different way of thinking about web. Expectations of native applications few different from applications that run in the browser. How do you go native without actually going native.Use what we know, I. E. Express, npm, javascript, Angular, Typescript to write native applications that run inside a Javascript virtual machine. Note, not in a browser but in a Javascript virtual machine. Use angular 2 components.Demo looks awesome where a native list view is instantiated via javascript/css. Extensively uses Typescript and angular 2 components.“I wonder how this fares compared to Ionic.“Combining xml template with data to create native view. Same technology used in angular universe to render dom more efficiently.Now talking about Path to Angular 2.Angular Upgrade project in works, hoping to make it simpler to convert the existing application piece by piece whilst it’s live.Now talking about successful angular ecosystem, Jules Kremer.Angular being open source has helped its adoption. Collaboration with Typescript and visual studio code team to provide better tooling.yo office Command to build scaffolding for apps providing office integration.Oh my god. Angular 2 will support IE 9!Contributing to Angular is important as it helps shape the future.Now talking about Angular 2 release status.Greentea team building Angular 2. 70k devs, 300k LOC.Lots of Google projects porting to Angular 2. The dev team is using the feedback from people porting to Angular 2 to speedup development. Now talking about deleting cancer.Launched a general public appeal asking for everyone globally to register as a stem cell donor. Quick and easy one time action. Could save someone’s life if a match is found.Margot foundation is entirely volunteer based foundation.",
        "url": "/2015/10/20/notes-from-angular-connect-2015-keynote/"
      }
      ,
    
      "2015-09-03-creating-a-restful-api-express-series-3": {
        "title": "Creating a RESTful API (Express series 3)",
        "author": "Manthan Dave",
        "category": "",
        "content": "So in the last post we went over the project structure. This time, we’ll actually modify some of the code to create our own RESTful web service.If you don’t know what a RESTful web service is, you should check out the wikipedia entry for it.As discussed in the previous post, all of our routes are defined in files located in the routes folder which are then referenced and collated in the app.js file. So, to create our own RESTful API, we need to do something in this area.Lets checkout the index.js file and see whats in it.var express = require('express');var router = express.Router();/* GET home page. */router.get('/', function(req, res, next) {res.render('index', { title: 'Express' });});module.exports = router;This is the typical structure of any nodejs module. Top section is to declare dependencies and instantiate objects. Middle part is to actually do things. And the final bottom part is to clean up and export the module. This file says that when the root (/) is accessed by a HTTP GET method (indicated by router.get), call a function. This function is defined as function(req, res, next) {...}. The three variables, req, res and next are function parameters. I won’t go into the details regarding basic JavaScript in any of these tutorials. If you have any questions, check out some of many JavaScript resources available on the web.These parameters req, res and next are the request object, the response object and the function call to the next module in the middle ware stack respectively. The line res.render('index', { title: 'Express' }); is using the render function of the response object, asking express to render the template 'index' with parameters specified by the JSON object { title: 'Express' }. Since Express knows that we are using Jade templating engine (defined in app.js), it will go to the views folder and find the file named index.jade and will render it.Right, so now that’s out of the way, lets go and create our own service. For the purpose of this tutorial, we’ll create a health check web service. When it gets called, it will return OK status indicating that the server health is ok. Since this will require a GET request to be invoked, we’ll be using router.get and hence the first line of the code will be very similar to the already existing one.router.get('/', function(req, res, next) {// My health check API});Next, we need to change two things. One is the URL reference. Currently, its set to '/'. Because this has already been defined, lets modify it to our own path. This can be anything you want but for the sake of simplicity, lets make it /healthcheck. Your code should now look like this:router.get('/healthcheck', function(req, res, next) {// My health check API});The code doesn’t do anything yet. Lets change that. Remove the comment // My health check API and put the following logic in it.var statusString = 'OK';return res.send(statusString);After your change, the file index.js should look like this:var express = require('express');var router = express.Router();/* GET home page. */router.get('/', function(req, res, next) {res.render('index', { title: 'Express' });});router.get('/healthcheck', function(req, res, next) {var statusString = 'OK';return res.send(statusString);});module.exports = router;Awesome. Now go back to your command line and run npm start command to start your server. Once started, go to http://localhost:3000/healthcheck on your web browser. You should see the following output:OKNow this isn’t very clean as its returning plain text instead of proper JSON response. Shut down the server and go back to your IDE. Modify the code in the /healthcheck router in index.js to be the following:var statusObject = {status: 'OK'};return res.send(statusObject);Start the server and navigate back to http://localhost:3000/healthcheck in your browser. You should see a JSON response this time:{\"status\": \"OK\"}Great work! You’ve just written your first RESTful web service in Node.js using Express! In the next post we’ll take this to a next level and migrate what we’ve written to a separate file to reduce clutter and make the code a bit cleaner. Enjoy!",
        "url": "/2015/09/03/creating-a-restful-api-express-series-3/"
      }
      ,
    
      "2015-09-02-understanding-the-default-project-structure-express-series-2": {
        "title": "Understanding the default project structure (Express series 2)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Now that we have a basic express web app created, lets go through the project structure.package.jsonIn the last post we briefly went through the purpose of package.json. Now its time to go through it in bit more detail. As the extension .json indicates, data in this file is in json format. Yours should look something like this:{\"name\": \"myawesomeproject\",\"version\": \"0.0.0\",\"private\": true,\"scripts\": {\"start\": \"node ./bin/www\"},\"dependencies\": {\"body-parser\": \"~1.12.0\",\"cookie-parser\": \"~1.3.4\",\"debug\": \"~2.1.1\",\"express\": \"~4.12.2\",\"jade\": \"~1.9.2\",\"morgan\": \"~1.5.1\",\"serve-favicon\": \"~2.2.0\"}The name and version fields indicate the project name and version respectively. The field private indicates that this is a private npm project. So, in case you were developing a npm module, this field will determine whether or not to allow you to publicly publish your project. The scripts field contains commands and the respective files that should be executed when the command is called. The two most popular ones here are start and test. As you can see, the express tool automatically generates the start script command and maps it to ./bin/www.The last thing here is the dependencies field. This contains all the dependencies of your project. The tilda sign (~) prefixed in the version means that if a newer bug fix version is found then it’ll update it to that automatically. For instance, in our case, the current version of express module is 4.12.2. In case a newer version 4.12.3 is released, the tilda sign means that npm will fetch that version. However, if express 4.13 is released, npm won’t update to that. On the contrary, if the hat sign (^) was used, npm will update to 4.13 but will hold off for version 5.0.app.jsThis file is the central point for configuring the express web server. It defines several things like the view engine, logging levels etc and loads all middleware components (more on that in later posts). You’ll be visiting this file more often than you think once you start developing express web applications.If you scroll down, you’ll see that this file also loads routes. This is where the server routes are defined. In this case, the route file index.js is loaded in and all its routes are mapped from the root route (indicated by /). In the next line it also loads another route file called users.js, routes of which are mapped from /users.routesThis directory contains files that define different routes of the web application. You can define routes in multiple files here and then reference them in the app.js in parent folder. This helps to keep things separate from one another to avoid clutter. Although JavaScript has no concept of a “class”, you can define various objects here and then re-use them across your various routes. We’ll go over that in later posts.viewsThis directory contains all your templates. Since the default templating engine is Jade, you’ll find files with .jade extension here. You can use one of the other options that express tool provides like ejs or handlebars or you can choose a completely different templating engine as well. Whatever you choose will have to be configured in the app.js file. The templates that have been defined here can be changed on the fly, meaning that changing the template while the application is running will have immediate effect. In contrast to this, if you change one of the javascript files, you’ll have to restart the whole application. Check out Jade templating engine here http://jade-lang.com/publicAs the name suggests, this directory contains all the public assets. Whatever you place in here can be accessed without authentication as these are the assets that pages normally require to render. Any files here can be referenced from root level. For instance, a stylesheet named style.css located in stylesheets folder in public directory can be accessed by /stylesheets/style.css from the web server. You can create as many sub directories as you want without making any configuration changes in app.js.Excellent! Now you know what different parts of an express web app does. In the next post we’ll create our first RESTful web service using Express. Excited?",
        "url": "/2015/09/02/understanding-the-default-project-structure-express-series-2/"
      }
      ,
    
      "2015-09-01-thinking-iteratively": {
        "title": "Thinking iteratively (Scrum series 2)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Most of us think linearly, that is step by step, from start to finish. Think about making something physical, like a wooden table. Your mind quickly starts creating steps leading to that goal. Something like this feels familiar?\tBuy lots of raw wood.\tBuy sandpaper.\tBuy varnish.\tMake four legs.\tMake table top.\tApply sandpaper.\tJoin it all up.\tApply varnish.Sorry, I’m no expert in the art of table making but most of you would have a similar list of actions. This is linear, meaning that the table cannot be used at all unless all the steps are complete. If you don’t have a dining table at all in your house, then you’ll probably have to sit somewhere else to eat for a couple of days (depending on how good you are at making a wooden table).Now think about writing software, say a website to help out your friend sell hats for cats online. Automatically you’ll start thinking:\tDesign frontend.\tDevelop backend.\tImplement frontend.\tIntegrate payment systems.\tIntegrate email notification service.\tTest.\tDeploy.\tHandover.There’s probably a lot of intermediary steps but this is the gist of it all. Again, if you look through carefully, your friend does not get her website unless all of the steps are complete.Iterative development only means that instead of doing everything in linear sequential steps, you are doing it in cycles. Delivery is key here meaning that you should aim to deliver at least once by the end of each cycle but mature teams are able to deliver the product more than twice each cycle. Thinking in line with the Scrum methodology, each cycle (or sprint) can be one week or up to one month. In our case, if we choose that one cycle is one week, here’s what it might look like:\tCycle 1:\tDesign basic frontend.\tImplement mock backend.\tImplement basic frontend.\tIntegrate mock payment system.\tIntegrate mock email notification service.\tDeploy and deliver.\tCycle 2:\tImprove frontend design.\tImplement basic backend functionality still mocking rest.\tImplement improved frontend design.\tIntegrate mock payment system.\tIntegrate mock email notification service.\tDeploy and deliver.\tCycle 3:\tImplement more backend functionality.\tRevisit frontend designs.\tImplement frontend design changes.\tIntegrate basic payment system.\tIntegrate mock email notification service.\tDeploy and deliver.\tCycle 4:\tImplement more backend functionality.\tRevisit frontend designs.\tImplement frontend design changes.\tIntegrate basic email notification service.\tDeploy and deliver.\tCycle 5:\tImplement more backend functionality.\tIntegrate advanced payment system.\tDeploy and deliver.\tCycle 6:\tImplement more backend functionality.\tRevisit frontend designs.\tImplement frontend design changes.\tIntegrate advanced email notification service.\tDeploy and deliver.Here, you are deploying and delivering the product to your friend every week so that she can inspect the whole flow. This also allows you to receive feedback regarding the entire product every week which is not possible in the linear way of doing things (as your product is not ready for your friend to look at until the end). Also you don’t have to come up with all the cycles at once. You basically have to have the current cycle plus at least next two cycles within your sights.Mocking things is very important in this flow as it helps your friend to “feel” how the website will look at the end. The mocks allow her to go through the checkout flow as if it was actually there. It might seem like a waste of time as you are developing something that you know will get replaced at the end anyway, it still has tremendous value as there is nothing worse than the client changing her mind and wanting to go in a completely different direction towards the end of the project. The mocks allow the client to focus as they get to see the product as it is being developed while also giving you continuous feedback.While this was a simple example, it gets quite a bit complicated when new projects are undertaken in large scale organisations as there can be lack of clarity before a project is started. I’ll do another post on the Scrum Framework for this sometime in near future but this post provides a base that can help you understand Scrum as it is also about iterative software development.Again, if you observe the technological advancements that humans has achieved so far, you’ll notice the iterative approach at large scale. We didn’t aspire to build shiny cars from day one. We first started with the wheel, advanced to wheel barrow, horse cart, trains and then finally a built car.Hope you enjoyed this article. I’ll appreciate it very much if you could leave your thoughts in the comments below.",
        "url": "/2015/09/01/thinking-iteratively/"
      }
      ,
    
      "2015-08-31-up-and-running-with-express": {
        "title": "Up and running with Express (Express series 1)",
        "author": "Manthan Dave",
        "category": "",
        "content": "For those who don’t know, Express is a web server framework that runs on Node.js. Basically, if you want to write a web application in node.js, express is one of your options. I prefer it because of its simplicity.The quickest way to write an express application is to use the express npm module. If you haven’t got it, install it by executing:npm install -g expressOnce installed, go to your projects folder, where you normally keep your projects and then execute:express myawesomeprojectYou should see some output and hopefully no errors. This command creates a basic project structure under myawesomeproject directory with some files for you to quickly get started. Some notable files are ./bin/www, ./app.js and ./routes/index.js.Now, cd into the project folder. You’ll see several files and folders in that directory. Don’t worry about that now. Let’s get the application server up and running. However, before we can do that, we’ll have to install the project dependencies. See the file called package.json? It defines the project and all its dependencies. NPM, being a dependency management system will go through the list of dependencies defined here and will then fetch them for you. If those dependencies have any tertiary dependencies then it’ll fetch them as well. To install the dependencies, execute:npm installIf you have a working internet connection, then this should go smoothly. Now let’s run the application. To do this, execute the following:npm startOpen your favourite web browser and navigate to the following url:http://localhost:3000/You should see a page with Welcome to myawesomeproject as its heading.Awesome work! You’ve just created your first express web application.In the next tutorial we’ll go through the project structure to see how it all fits together.",
        "url": "/2015/08/31/up-and-running-with-express/"
      }
      ,
    
      "2015-08-28-managing-your-executable-java-project-with-gradle": {
        "title": "Managing your executable Java project with Gradle",
        "author": "Manthan Dave",
        "category": "",
        "content": "I’ve been using Gradle for a while now and have quite used to it. Most of my Java projects are web applications so all Gradle has to do is to build them. Once built, I can just plop the war file onto tomcat and it just works. However, when I’m trying something out, I just need a simple .java file which I manually compile with javac and then run it with java command. This works most of the time, except when I wanted to try out something complex, something that uses loads of external libaries. Something like this normally needs a dependency management system like Gradle. Using my normal Gradle configuration, I got it to build the project really easily, however running it was quite difficult.The main difficulty came from classpath problems. When you are running a web application via tomcat, it usually takes care of all that stuff. However, when you’re doing it manually, YOU have to take care of it. Again, because this required manual effort, I started finding ways to achieve that via Gradle. After a couple of hours, I finally got it working with the following build.gradle file:apply plugin: 'java'apply plugin: 'idea'apply plugin: 'application'mainClassName = \"com.mypackage.awesomeapp.Main\"dependencies {compile 'org.apache.httpcomponents:httpclient:4.5'}run {systemProperties System.getProperties()println(systemProperties)}The two key things here are the apply plugin: 'application' and mainClassName = \"com.mypackage.awesomeapp.Main\" lines. As the statement suggests, the first one applies the application plugin, declaring your project as a standalone application. The second one tells Gradle what class is your main class. Gradle then sets this in the manifest file of the resulting jar file so that when executed, it knows what class to invoke first. Once you have the setup ready, execute:gradle runFeel free to give me a shout if you find better solution than this and I’ll make sure the article gets updated with proper credit. Enjoy!",
        "url": "/2015/08/28/managing-your-executable-java-project-with-gradle/"
      }
      ,
    
      "2015-08-27-streaming-an-archive-from-a-server-over-ssh-and-extracting-it-on-the-fly": {
        "title": "Streaming an archive from a server over SSH and extracting it on the fly",
        "author": "Manthan Dave",
        "category": "",
        "content": "I’d found this command a while ago now but had completely forgotten about it until I stumbled upon it yesterday. So, if you had one or more tar files on a server and wanted to get them all down and then extract them somewhere to do your work, the normal way to achieve that would be:scp awesomeuser@awesomeserver:/tmp/mytarfile.tar /storage/The problem with this is that its a two step action. 1) You download the tar file(s). 2) You extract the downloaded tar file(s). While this is “functional”, here’s an awesome way to do it:curl -u awesomeuser: --key /home/awesomeuser/.ssh/id_rsa --pubkey /home/awesomeuser/.ssh/id_rsa.pub scp://awesomeserver/tmp/mytarfile.tar | tar xf -Lets break that down. We’re using the curl command to download the file using awesomeuser as the username (indicated by -u awesomeuser). For authentication, we are using our private and public keys as indicated by --key /home/awesomeuser/.ssh/id_rsa and --pubkey /home/awesomeuser/.ssh/id_rsa.pub flags. We’re then asking the curl command to use the scp protocol for accessing our server (as indicated by scp://awesomeserver) and are then giving it the location of the tar file that we want to download (as indicated by /tmp/mytarfile.tar). Because curl command will output whatever it gets to standard out stream (STDOUT), we’re leveraging this capability by redirecting STDOUT to a pipe which is then fed into the tar command (as indicated by | tar xf -).In case you have a tar.gz file or a tgz file, use:curl -u awesomeuser: --key /home/awesomeuser/.ssh/id_rsa --pubkey /home/awesomeuser/.ssh/id_rsa.pub scp://awesomeserver/tmp/mytarfile.tar.gz | gunzip -c - | tar xf -Same principle except before the final pipe of tar xf -, we’re squeezing gunzip -c in between to g-unzip the file first before untarring it.",
        "url": "/2015/08/27/streaming-an-archive-from-a-server-over-ssh-and-extracting-it-on-the-fly/"
      }
      ,
    
      "2015-08-26-finding-text-in-text-files-within-multiple-tar-archives-without-extracting-any-file-onto-the-file-system": {
        "title": "Finding text in text files within multiple tar archives without extracting any file onto the file system",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I was working the other day and then suddenly I had a need to find a piece of text within loads of text files contained across multitude of tar files. In a normal case, you’d extract them all somewhere in temp directory and then use grep to find the text you want. However, this doesn’t work very well when the tar files you have are in thousands. Also, there is something uncool about doing stuff manually when you’re in a world where there’s always a better way.After an hour of experimentation, I found the following magic command:find /home/manthan/loads_of_tarfiles -name \"*.tar\" -type f -exec sh -c \"tar -tf {} | grep awesome | xargs -I found -n 1 tar -Oxf {} found | grep im\" \\; -printThe above command finds all files (indicated by -type f) in /home/manthan/loads_of_tarfiles folder with extension .tar (indicatetd by -name \"*.tar\"). For each file it finds, it runs a shell command (indicated by -exec sh -c) which lists all files contained within that tar file (indicated by tar -tf {}, where {} is each instance in which the find command found a tar file). For each file contained within the resident tar file, it then finds a text file whose name contains the word awesome (grep awesome). This can be anything as long as you know what kind of file you are looking for.For each text file (whose name contains the word awesome) that is found within the tar file, it then extracts the contents of that file to STDOUT (indicated by tar -Oxf {} found where {} is the tar file that is found in the first part of the command and found is the text file that is found in the latter part of the command by the grep awesome command). Now that the contents of the file are available in STDOUT, a simple search finds the text we need in that file (using grep im which searches for text im within the extracted file). If it is found, the matching contents are printed and the name of the tar file it matched in is also printed (indicated by -print).Phew! That was lengthy. As usual, if you have any questions or find a better way, let me know in the comments below and I’ll make sure to update the post with credits. Enjoy!",
        "url": "/2015/08/26/finding-text-in-text-files-within-multiple-tar-archives-without-extracting-any-file-onto-the-file-system/"
      }
      ,
    
      "2015-08-26-working-in-it-industry-this-made-me-smile": {
        "title": "Working in IT industry, this made me smile",
        "author": "Manthan Dave",
        "category": "",
        "content": "",
        "url": "/2015/08/26/working-in-it-industry-this-made-me-smile/"
      }
      ,
    
      "2015-08-25-finding-a-file-contained-within-a-bunch-of-tar-files-without-opening-any": {
        "title": "Finding a file contained within a bunch of tar files without opening any",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I was debugging some code the other day and I quickly found myself in a situation where I had to look for a class file that was located in one of the many millions of jar files that are there in my maven cache. My immediate reaction was\"Ugh, this is going to be loads of effort and is going to take ages!\".Luckily, quick one minute google found the following command:find /home/manthan/.m2 -name '*.jar' -type f|while read f; do p=\"$(tar tf $f|egrep awesome)\"; [ -n \"$p\" ] &amp;&amp; echo -e \"$f\\n$p\" ; p=\"\"; doneWhere awesome.class is the file I’m looking for in all files (indicated with -type f option) with extension jar found in /home/manthan/.m2 directory.Source: Superuser - Search through tar files for a pattern and print the full path of what’s found",
        "url": "/2015/08/25/finding-a-file-contained-within-a-bunch-of-tar-files-without-opening-any/"
      }
      ,
    
      "2015-08-24-developer-code-of-conduct-for-scrum-refinement-ceremonies": {
        "title": "Developer code of conduct for Scrum Refinement ceremonies (Scrum series 1)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Refinement sessions. Some people like them and few enjoy them. Over the years, I’ve worked across multiple companies who embrace Agile, particularly Scrum. I’ve started my career with Agile and so far, I’ve worked in organisations who are either mature Agile organisations or those who are transforming themselves into being Agile so I like to think that I have a pretty good grip on what an organisation’s Agile ideals should be. However, ideal things don’t happen in practice - and that’s a good thing because I don’t think that all organisations can wear one single Agile hat.Different places have different ideas about what a refinement session should be about. Some think that its an opportunity to connect with the product owner at technical level while others think that it is an opportunity to itemize stories by capturing more detail in them. Then there are also some outliers who think that refinement should be about the dev team and how they feel about the stories that are in the backlog.I think that all of the above views are correct - up to an extent.Refinement sessions provide an opportunity to connect with the product owner. While, as an agile team, you should be regularly talking to your product owner, constantly getting feedback on your work anyway, refinement sessions allow the whole team to come together to review upcoming work in the backlog and at the same time, negotiate the required amount of work with the product owner. However, this shouldn’t become a tug of war but rather an open dialogue where ideas are allowed to flow freely. This is an opportunity for the development team to connect with the product owner at ‘why’ and ‘what’ level so that they can find and apply meaning to their work.Refinement sessions are about itemizing stories by packing as much detail in them as possible. This is quite popular. I’ve seen people getting into low level details during refinement, talking about ‘how’ instead of just ‘what’ and ‘why’. If you have highly technical people in your team, who know loads of stuff and are packed with experience, or just people who are highly motivated and just can’t wait to start working, then either you’re already having this mindset or will be facing it soon. Its not a sin to go into low level detail but it is when people go into it for too long. Remember, “I want a API orchestration layer” or “I want my posts to be fetched from a JSON REST compatible API” said no customer ever. Refinement sessions are about capturing enough detail from the user’s point of view for the development team to get started.Refinement sessions are about dev team and how they feel about the current state of the backlog. Very few people see it from this point of view as most forget that it is a two way conversation between product owner and the development team. Product owners don’t own the team. They own the backlog and the work that is in it. Development team has every right to negotiate items that are on there if they feel that it is the right thing to do for the team. Another development team might not feel the same way but that’s not the point as they are not the ones working on it. In some cases, the backlog is not ready to be refined as items that are on it severely lack purpose, let alone the details. In this case, the product owner should go back to the drawing board and get the information that is required to give a story its meaning. The story can be skipped or the session can be ended if no story is fit enough for refinement.In addition to complying with all the “rules”, I think that the teams should also meta-refine refinement sessions. If they are becoming a chore find out why. Liven it up a bit. Change something, gamify, let someone else drive the sessions or try out new things in general. You’ll eventually find a sweet spot.",
        "url": "/2015/08/24/developer-code-of-conduct-for-scrum-refinement-ceremonies/"
      }
      ,
    
      "2015-08-24-finding-text-within-files-on-linux": {
        "title": "Finding text within files on Linux",
        "author": "Manthan Dave",
        "category": "",
        "content": "So at some point in your professional life, you have probably faced the following problem:I have a bunch of files and I want to quickly find a file that contains a particular text. I don't want to go through each file individually because that's lame. Also, I don't want to use GUI because I aspire to become pro at using Linux commands.Yeah, yeah we’ve all been there. As a beginner, I struggled a lot with this and even if I found a solution, I won’t be able to recall it the next time when I’d so desperately need it. Well, here it is!If you are looking in all files whose name ends with .txt:find /home/manthan/folder_o_files -name *.txt -type f -exec grep awesome {} \\; -print;In the above command, we’re looking through all the files (denoted by -type f) in /home/manthan/folder_o_files folder. The -exec flag allows user to specify what command to execute for line of output from the find command. In this case, we’re using our favourite command grep to find for text awesome. The opening and closing curly brackets (denoted by {}) serve as placeholder for each line of file outputted by the find command. We escape the semi-colon to end the grep command, ask find command to show its findings by supplying it with -print flag and then terminate our command by ending it with semi-colon.If you are unsure of the case of the text (i.e. upper-case or lower-case), stick -i flag right after grep.If for some reason you are looking through a bunch of tar files and are looking for a file that one of the tar files might contain, check out my other post on How to find a file contained with a bunch of tar files without opening (soon).Enjoy!",
        "url": "/2015/08/24/finding-text-within-files-on-linux/"
      }
      ,
    
      "2015-08-24-configuring-java-httpclient-to-use-proxy": {
        "title": "Configuring Java HttpClient to use proxy",
        "author": "Manthan Dave",
        "category": "",
        "content": "So, for various security reasons, at work, I have to go through proxy in order to access anything. I was doing some prototyping with sparkpost and none of my code worked as by default the code wasn’t going through the business proxy.Naturally, I thought that I’d set HTTP_PROXY and HTTPS_PROXY environment variables and then run my java code. I did and as it is with all things in Software Engineering industry, it didn’t work.After several hours of googling (or what felt like several hours) I finally found what was wrong with it. In Java, due to security reasons, all proxy variables are ignored unless they have been explicitly set in code. Here’s how you set them:String proxyHost = System.getenv(\"HTTP_PROXY_HOST\");String proxyPort = System.getenv(\"HTTP_PROXY_PORT\");String proxyUser = System.getenv(\"HTTP_PROXY_USER\");String proxyPassword = System.getenv(\"HTTP_PROXY_PWRD\");CredentialsProvider credsProvider = new BasicCredentialsProvider();credsProvider.setCredentials(new AuthScope(proxyHost, Integer.parseInt(proxyPort)),        new UsernamePasswordCredentials(proxyUser, proxyPassword));HttpHost proxyHostObject = new HttpHost(proxyHost, Integer.parseInt(proxyPort));HttpClient client = HttpClientBuilder.create().setProxy(proxyHostObject)    .setProxyAuthenticationStrategy(new ProxyAuthenticationStrategy())    .setDefaultCredentialsProvider(credsProvider)    .build();HttpGet getRequest = new HttpGet(URL);getRequest.addHeader(\"Authorization\", API_KEY);getRequest.addHeader(\"Content-Type\", \"application/json\");try {    System.out.println(\"Executing request...\");    HttpResponse response = client.execute(getRequest);    System.out.println(\"Request successfully executed.\");    HttpEntity entity = response.getEntity();    String responseString = EntityUtils.toString(entity);    System.out.println(responseString);} catch (IOException e) {    e.printStackTrace();}The CredentialsProvider class allows one to save the proxy credentials while the HttpHost class allows storing the proxy host.I personally am not a fan of this arrangement as the proxyHost and proxyPort is being duplicated twice in both classes. If you find a better arrangement, feel free to drop me a line.Thanks!",
        "url": "/2015/08/24/configuring-java-httpclient-to-use-proxy/"
      }
      ,
    
      "2015-08-06-how-to-get-file-sizes-recursively-in-a-directory-and-list-them-in-ascending-order": {
        "title": "How to get file sizes recursively in a directory and list them in ascending order",
        "author": "Manthan Dave",
        "category": "",
        "content": "When cleaning up unused files from my linux computer, I find it useful to know which files use most space so that I can easily determine which ones to remove.The most basic command to find file size of a given file is:du -sm myfile.txtIf you are in a folder with multiple files and want to get file sizes of all the files, the command is:du -sm *The above command does not get file sizes recursively. However, if you are happy with it and want to sort the file sizes in ascending order:du -sm * | sort -n -k 1Now, this is little useful as it does not do recursive file size listing. To do that:find . -type f -exec du -sm {} \\; | sort -n -k 1If you are looking for a particular type of file (eg. jar files):find . -name \"*.jar\" -type f -exec du -sm {} \\; | sort -n -k 1Enjoy!",
        "url": "/2015/08/06/how-to-get-file-sizes-recursively-in-a-directory-and-list-them-in-ascending-order/"
      }
      ,
    
      "2015-07-20-unixlinux-command-to-check-if-apache-tomcat-is-running": {
        "title": "Unix/Linux command to check if Apache Tomcat is running",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I’ve come across this problem quite a few times. Normal way to do this is:ps -ef | grep tomcatThis works most of the times. If tomcat is running, it gives between 1 and 2 lines back but if not, it gives anywhere between 0 and 1 lines back. A much cleaner use of the above command would be with wc -l:ps -ef | grep tomcat | wc -lHowever, this doesn’t solve the actual problem as along with the tomcat process, it also gives you the process of command \"grep tomcat\".Here’s the command to solve this problem. You can use either of the two below commands:ps -ef | grep tomcat | grep -v \"grep tomcat\" | wc -lps -ef | grep tomca[t] | wc -lThe first command explicly says that once you get a list of all processes containing the word tomcat, ignore lines containing words \"grep tomcat\". And then the usual, pipe it to word count and output the number of lines.The second one, however, tricks the grep into using a regular expression and ignoring itself. This is because the actual output containing \"grep tomca[t]\" will have the square brackets which obviously won’t match the actual regular expression.",
        "url": "/2015/07/20/unixlinux-command-to-check-if-apache-tomcat-is-running/"
      }
      ,
    
      "2014-09-12-eclipse-forgets-my-proxy-credentials": {
        "title": "Fixing proxy credentials dementia in Eclipse",
        "author": "Manthan Dave",
        "category": "",
        "content": "This usually happens when you are using a different eclipse installation to access your old eclipse workspace. Its something to do with the new installation not being able to access the secure storage created by your old installation. Here’s what I did to solve it:Close eclipse completely. Now fire up terminal or command prompt or whatever and navigate to your home directory. In my case it is '/Users/manthan/' directory.Now cd into .eclipse/org.eclipse.equinox.security directory.cd .eclipse/org.eclipse.equinox.securityIf you list files, you should see a file called secure_storage in it. Rename that to secure_storage.old or something like that. You can delete it if you want but I prefer to rename things, make sure it works and then delete the renamed file.mv secure_storage secure_storage.oldNow making sure that the secure_storage file has been renamed successfully, open eclipse and now try saving your credentials. It should ask for your key store master password and other stuff.That's it! You're done.",
        "url": "/2014/09/12/eclipse-forgets-my-proxy-credentials/"
      }
      ,
    
      "2014-09-06-moving-steam-games-to-another-drive": {
        "title": "Moving steam games to another drive",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I came across this recently and thought to share it with you guys.First of all, exit steam completely. If you’ve closed it, check if it’s in the system tray and if it is, exit from there as well. Now, fire up Windows Explorer and go to where steam is installed on your computer. In my case, it was in the default installation directory (C:\\Program Files (x86)\\Steam).Once in the folder, delete everything EXCEPT the SteamApps folder and Steam.exe. Now, go UP a directory level and move the whole Steam folder to wherever you want it to be. In my case, I moved it to E:Games folder.When the move is complete, double-click on the steam.exe. It’ll re-download steam client for you and will require you to login to your steam account again.So that’s it. You should have all your games available to you in your new drive location!Original source: https://support.steampowered.com/kb_article.php?ref=7418-YUBN-8129",
        "url": "/2014/09/06/moving-steam-games-to-another-drive/"
      }
      ,
    
      "2014-09-05-rejecting-a-pairing-request-from-a-bluetooth-device-permanently-on-your-mac": {
        "title": "Rejecting a pairing request from a bluetooth device permanently on your Mac",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I had this problem the other day where I kept on getting pairing requests from my friend’s bluetooth keyboard. This was rather annoying as the pairing dialog kept on popping up every few minutes. So I googled around a bit and found the following solution which worked for me.Turn your bluetooth off. This will disable your bluetooth mouse and keyboard and hence you will have to use the built-in ones.The problem is that at some point the keyboard that is nagging you to connect would have connected to your laptop. Your laptop remembers this and hence accepts incoming pairing request prompting you to verify it.Our aim is to make the machine forget that it was ever connected to this device. In order to do this, you will need to edit a file called com.apple.Bluetooth.plist. This file is located in /Library/Preferences and ~/Library/Preferences folder. This file is in binary so in order to be able to edit it, you will have to convert it to xml first. So, open up terminal and type:sudo plutil -convert xml1 /Library/Preferences/com.apple.Bluetooth.plistNow you can go on to finder or whatever you have and edit this file. Remove … followed by … tags which relate to the keyboard/device that you are trying to ban. Simple way is to find the name of the device in the file. This name will be in … tags. Once you find the name, remove the whole container … and preceding … tags. Once you do this, save the file and convert it back to binary using the following terminal command:sudo plutil -convert binary1 /Library/Preferences/com.apple.Bluetooth.plistDo the same for the com.apple.Bluetooth.plist file located in ~/Library/Preferences folder as well. If that folder does not have this file, copy it over from /Library/Preferences folder.Once you are done, turn the bluetooth on and now it shouldn’t prompt you for pairing requests.",
        "url": "/2014/09/05/rejecting-a-pairing-request-from-a-bluetooth-device-permanently-on-your-mac/"
      }
      ,
    
      "2013-12-07-an-investigation-regarding-the-effects-of-nosql-on-the-four-inherent-problems-of-software-engineering": {
        "title": "An investigation regarding the effects of NoSQL on the four inherent problems of Software Engineering",
        "author": "Manthan Dave",
        "category": "",
        "content": "So as you all know, I am currently in my final year of BEng (Hons) Software Engineering. In my final year, one of the courses that I am studying is Programming Frameworks. This course introduces to the academic side of Software Engineering and covers topics like quality plans, quality control, advanced methodologies, planning, frameworks etc.My coursework for this course was to prepare a quality plan for writing an academic paper and then write the academic paper. The main topic of the academic paper was to compare and contrast one of the three concepts with an academic paper \"No Silver Bullet - Essence and Accidents of Software Engineering\" written by Fred Brooks. I prepared the quality plan, wrote the academic paper and submitted the coursework last month. I received a really good mark for it - 88% and thus I thought that I should share the academic paper that I wrote in LaTeX. You can download the academic paper from here.\"An investigation regarding the effects of NoSQL on the four inherent problems of Software Engineering\"by Manthan Dave&nbsp;Let me know what you think about the paper in the comments below.",
        "url": "/2013/12/07/an-investigation-regarding-the-effects-of-nosql-on-the-four-inherent-problems-of-software-engineering/"
      }
      ,
    
      "2013-10-17-what-to-do-when-app-downloads-in-windows-88-1-store-get-stuck-at-pending-or-downloading-stage": {
        "title": "What to do when app downloads in Windows 8/8.1 store get stuck at &amp;#8220;Pending&amp;#8221; or &amp;#8220;Downloading&amp;#8221; stage?",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I had this problem today. Well to be honest, I had this problem before as well but I decided to fix it today. Earlier today, I upgraded to Windows 8.1 and I was hoping that the upgrade would resolve this problem. Well, it didn’t. After surfing through the web, I found the solution. So, here it goes:First of all, you need to be in safe mode. Open the charms bar, click on the power button, hold the shift key and while you’re holding it, click \"Restart\". This will restart Windows in troubleshooting/recovery mode.From the menu that has been presented to you, click \"Troubleshoot\". Now select \"Advanced options\" and from the subsequent screen click \"Windows Startup settings\". Finally, click \"Restart\" button.Now, if you have Windows 8 installed, you will have an old command prompt styled menu from which you can select \"Safe Mode with Networking\" by pressing navigational arrow keys. However, if you have Windows 8.1, then you will have to press number 5 key on your keyboard to go into \"Safe Mode with Networking\". Once you’re there, fire up command prompt. Type in:cd %systemroot%Then type:ren SoftwareDistribution SoftwareDistribution.oldThat’s it. Restart the computer by using the charms menu. This time, don’t hold down the shift key.When Windows normally boots up, open the store, wait a few seconds and try updating/downloading apps and it should work just fine.",
        "url": "/2013/10/17/what-to-do-when-app-downloads-in-windows-88-1-store-get-stuck-at-pending-or-downloading-stage/"
      }
      ,
    
      "2013-09-27-changing-keyboard-layout-in-ubuntu-server-linux-how-to": {
        "title": "Changing keyboard layout in Ubuntu Server (Linux, How to)",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I recently installed Ubuntu Server in a virtual machine and I had problems with my keyboard layout. I use UK layout while the default that comes with Ubuntu Server is US layout. As you can imagine, this caused problems and I had to go hunt for a solution. Finally, I found one and here it is:You need to reconfigure keyboard configuration. Type the following:sudo dpkg-reconfigure keyboard-configurationIf that doesn’t work, you might need to install console-data package. Install it by typing:sudo apt-get install console-dataand then try first step again.If everything was fine, you should see this:Using this you can select brand of your keyboard. I am running this virtual machine on my Acer laptop and as you can see it is listed there. You can use up/down/page up/page down keys to navigate through the list. When you have picked one, press enter to go to next screen.You should now see this:This is the language selection screen. Navigate through the list, select your language and as you did in previous one, press enter to move on to next screen.This is the screen where you select your keyboard type:Assuming you're using a qwerty keyboard for PC, you should just leave the first one selected and press enter. However, if you are running this on a mac or your keyboard is of different kind, you are free to choose whatever applies to you from the list and continue by pressing enter.The next screen allows you to map a key on your keyboard to alternate grammar key (Alt Gr).I have never used this key in my life for alternate grammar, however, hoping that I may use it at some point, I would map it to right-alt key (which actually is Alt Gr key). As usual, select one from the list and press enter.Next one is compose key. Same drill. Pick one and press enter:Compose key, if I remember correctly, allows you to type ASCII for certain characters when you press and hold it. This key on windows is left alt key however, here you can choose whatever you want for ubuntu server.When you press enter, the wizard will quit. So that was the keyboard configuration screen which allows you to configure your keyboard pretty neatly. Have fun!",
        "url": "/2013/09/27/changing-keyboard-layout-in-ubuntu-server-linux-how-to/"
      }
      ,
    
      "2013-09-14-assigning-static-ip-address-to-your-raspberry-pi-wifi": {
        "title": "Assigning static IP address to your Raspberry Pi (WiFi)",
        "author": "Manthan Dave",
        "category": "",
        "content": "When I first bought my Raspberry Pi, I had this problem. My router and TV are in different rooms and I don’t have a ethernet cable. This restricts networking ability of my Pi which is quite annoying. I searched for articles on assigning static IP addresses to Raspberry Pi but most of them were talking about assigning it for eth0. I, on the other hand wanted wlan0 static IP.Now, I could have just got a long ethernet cable and solve this whole problem but I was just annoyed and was constantly asking myself “Why isn’t this working over wifi”. So finally, after significant digging around, I found the solution. Here it goes:First of all, take a backup of /etc/network/interfaces file:pi@raspberrypi ~ $ sudo su -root@raspberrypi:~# cp /etc/network/interfaces .Now, in order to make this work, we need some information first. We need to find out the new static ip address that you want, gateway, mask, network and broadcast address. Assuming that you have a working wifi connection on your Raspberry Pi, type:root@raspberrypi:~# ifconfigFrom the output, the bit that we are interested in is:wlan0     Link encap:Ethernet  HWaddr 00:00:00:00:00:00inet addr:192.168.1.99  Bcast:192.168.1.255  Mask:255.255.255.0UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1RX packets:19153 errors:0 dropped:24512 overruns:0 frame:0TX packets:25553 errors:0 dropped:0 overruns:0 carrier:0collisions:0 txqueuelen:1000RX bytes:3724822 (3.5 MiB)  TX bytes:31437802 (29.9 MiB)The highlighted line will give you information about your current IP, broadcast and mask address. Take a note of the last two. So, in my case:address: (this is the IP address you wish to reserve as static eg. 192.168.1.69)mask: 255.255.255.0broadcast: 192.168.1.255To find out information about the last two bits, type:root@raspberrypi:~# netstat -nrKernel IP routing tableDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface0.0.0.0         192.168.1.1     0.0.0.0         UG        0 0          0 wlan0192.168.1.0     0.0.0.0         255.255.255.0   U         0 0          0 wlan0In my case:gateway: 192.168.1.1network (a.k.a destination): 192.168.1.0Now that we have all the information that we need, edit the /etc/network/interfaces file using nano. Putting all the bits and pieces together, it should look like:Press Ctrl + O to save.Now, since I was doing this whole process over SSH, I had to write a script to restart wifi because if I didn’t, as soon as I turn it off, it would also take away my SSH access rendering me powerless. You can do the following in either case. Create a new file called restartwifi.sh using nano in root home directory:root@raspberrypi:~# nano restartwifi.shNow, I am aware that this script can be improved but this is what I had before and it has worked without any problems. Press Ctrl + O to save the script. Then press Ctrl + X to exit.Make sure you edit permissions to make the script executable. Run:root@raspberrypi:~# chmod +x restartwifi.shIf you're not root, you'll have to prefix above command with \"sudo\".Now comes the moment of truth. Run the script to reset your wifi by typing following command:root@raspberrypi:~# nohup ./restartwifi.sh &amp;If you're in putty session, you'll immediately go offline. If you've done everything correctly, you should be able to ssh into your Pi, after couple of minutes, on your chosen IP address.",
        "url": "/2013/09/14/assigning-static-ip-address-to-your-raspberry-pi-wifi/"
      }
      ,
    
      "2013-09-04-how-to-fix-the-display-issue-where-edges-of-your-screen-are-cut-off-every-time-you-connect-your-pc-to-a-hdtv": {
        "title": "Fixing HDMI display underscan/overscan",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I have this problem every time I connect my laptop to my Samsung HDTV. I thought this was an issue with driver/video card but I wasn't convinced because I wanted a solution. Finally, today, I managed to find a solution for this. Here's how you can solve it:Go to display driver settings. In my case, since my laptop has two graphics cards - Nvidia 640m and Intel HD 4000 and since the primary display driver is Intel, I went to Intel HD Graphics Control Panel.Click on Display to view display settings.Ensure that you have connected your computer to TV using a HDMI cable. If you have, you should see something like \"Digital television SAMSUNG\" in the combo box under Select Display.Make sure that you have selected \"Display Settings\" tab and \"Basic\" twisty has been expanded. Under \"Scaling\", select \"Customize Aspect Ratio\". In the preview section, you will see two bars (horizontal and vertical) appear. You can adjust the aspect ratio using those. As you can see from the screenshot below, 63 vertical and 65 horizontal works perfect for me. Once you're done experimenting, hit apply.Additionally, you might want to save those settings under a different profile. You can click on save profile button to do that. Give it a meaningful name and you're done. That way, you can keep different profiles for different monitors/TVs.",
        "url": "/2013/09/04/how-to-fix-the-display-issue-where-edges-of-your-screen-are-cut-off-every-time-you-connect-your-pc-to-a-hdtv/"
      }
      ,
    
      "2013-09-03-google-now-shows-you-bus-times": {
        "title": "Google Now shows you bus times!",
        "author": "Manthan Dave",
        "category": "",
        "content": "So today I learnt that Google Now can show you bus times. I think this is cool as a student using public transport.Although, you have to be at or near the bus stop to view its times. It can pick that up from a calendar entry/email/search as well. It would've been  nice if I could search for a bus stop to view its times but as it turns out, you can't.So I guess this isn't convincing enough to make me uninstall my bus checker app but it's a start.  ",
        "url": "/2013/09/03/google-now-shows-you-bus-times/"
      }
      ,
    
      "2013-07-18-using-find-command-to-search-for-a-particular-directory-in-unix": {
        "title": "Using find command to search for a particular directory in unix",
        "author": "Manthan Dave",
        "category": "",
        "content": "I was looking for a particular directory today on my computer and I didn’t know how to do it. After spending some time searching for it, I found the following command:find [parent directory] -type d -name [directory name]So in my case:find /home/vader -type d -name wrapper_scriptsOptionally you can also use:find /home/vader -type d | grep wrapper",
        "url": "/2013/07/18/using-find-command-to-search-for-a-particular-directory-in-unix/"
      }
      ,
    
      "2013-06-26-controlling-led-brightness-using-potentiometer-and-arduino": {
        "title": "Controlling LED brightness using potentiometer and arduino",
        "author": "Manthan Dave",
        "category": "",
        "content": "I was doing experiments with my Arduino today and this was one of them. I have commented the code well so it should be easy to understand. Here you go:/*SETUPConnect led to analog pin 10, shorter end to ground.Potentiometer setup:(From left to right)1. +5V2. Analog pin 0 (A0)3. Ground*///Set LED pin to analog pin 10.const int led_pin = 10;//Set potentio meter pin to analog pin 0 (A0).const int pot_met = 0;void setup(){  //Set led_pin to be the output.  pinMode(led_pin,OUTPUT);  //Initiate serial (optional).  Serial.begin(9600);}void loop(){  //Use the readPotentioMeterBrightness method to read brightness value.  int sensor_val = readPotentioMeterBrightness();  //Output the value to serial (optional).  Serial.println(sensor_val);  //Set LED brightness to value that we acquired from potentiometer.  analogWrite(led_pin,sensor_val);  //Pause for 250 ms.  delay(250);}int readPotentioMeterBrightness(){  //Read analog input and store value in integer variable val  int val = analogRead(pot_met);  //Adjust the value to be in range 0 to 255.  val = map(val,0,1023,0,255);  //Ensure value is between 0 and 255.   val = constrain(val,0,255);  //Return value  return val;} ",
        "url": "/2013/06/26/controlling-led-brightness-using-potentiometer-and-arduino/"
      }
      ,
    
      "2013-06-20-copying-symbolic-links-unixlinux": {
        "title": "Copying symbolic links (Unix/Linux)",
        "author": "Manthan Dave",
        "category": "",
        "content": "At some point, you might come across a situation where you have some files and a bunch of symlinks (symbolic links) in a folder and you wish to copy the data that is pointed at by the symlinks and not just blank links. If you are using Redhat Enterprise Linux, this should automatically happen when you call the cp command. However, on UNIX platforms or Linux distributions, this is not the case by default. In those cases, you will have to issue the following command:cp -L {file(s)} {destination folder}The -L option in cp command forces it to follow (dereference) symlinks and copy the data that is pointed at by the symlink.In case you do not want to follow symlinks, you can issue the following command instead:cp -P {file(s)} {destination folder} ",
        "url": "/2013/06/20/copying-symbolic-links-unixlinux/"
      }
      ,
    
      "2013-06-19-mounting-iso-images-on-linux": {
        "title": "Mounting ISO images on Linux via command line",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I learnt this today and decided to share it with you guys. If you have an iso image and want to mount it on Linux, simply execute the following command in terminal:sudo mount -o loop mydisk.iso myfoldernameSo in my case, I had iso file called 2012photos.iso on my desktop and I wanted to mount it on folder called 2012pics which was on Desktop as well. I executed:sudo mount -o loop /home/manthan/Desktop/2012photos.iso /home/manthan/Desktop/2012pics",
        "url": "/2013/06/19/mounting-iso-images-on-linux/"
      }
      ,
    
      "2013-06-16-the-application-was-unable-to-start-correctly-0xc000007b": {
        "title": "Fixing &amp;#8220;The application was unable to start correctly (0xc000007b)&amp;#8221; error",
        "author": "Manthan Dave",
        "category": "",
        "content": "So I found this error when I started Lotus Notes this morning. After looking through some forums, I found that I needed to re-install Microsoft Visual C++ 2010 (Redistributable) from here.This error is usually caused by C:\\Windows\\System32\\MSVCR100.dll being corrupted. Reinstalling VC++  fixes it.",
        "url": "/2013/06/16/the-application-was-unable-to-start-correctly-0xc000007b/"
      }
      ,
    
      "2013-05-15-heres-to-100-years-of-ibm": {
        "title": "Here&amp;#8217;s to 100 years of IBM!",
        "author": "Manthan Dave",
        "category": "",
        "content": "",
        "url": "/2013/05/15/heres-to-100-years-of-ibm/"
      }
      ,
    
      "2013-05-11-managing-password-expiry-for-a-user-account-unixlinux": {
        "title": "Managing password expiry for a user account (Unix/Linux)",
        "author": "Manthan Dave",
        "category": "",
        "content": "I find this very useful on several occasions so here we go. Login as root and execute the following:passwd -x -1 So if you want to remove password expiry from bob’s account, type:passwd -x -1 bobNotice that -1 parameter after -x represents number of days before the password expires. Since we do not want the password to expire, we have set it to -1. However, you can set it to any value you want. So, if you want your password to expire after 90 days (i.e. 3 months) for bob’s account, type:passwd -x 90 bob",
        "url": "/2013/05/11/managing-password-expiry-for-a-user-account-unixlinux/"
      }
      ,
    
      "2013-05-07-checking-memory-ram-size-on-solaris": {
        "title": "Checking memory (RAM) size on Solaris",
        "author": "Manthan Dave",
        "category": "",
        "content": "If you want to check how much RAM your Solaris machine has, here’s how:/usr/sbin/prtdiag -v | grep -i memoryYou can also try:/usr/sbin/prtconf | grep -i memory",
        "url": "/2013/05/07/checking-memory-ram-size-on-solaris/"
      }
      ,
    
      "2013-05-03-writing-code-building-a-chat-application-in-java-session-1": {
        "title": "Writing Code &amp;#8211; Building a chat application in Java &amp;#8211; Session 1",
        "author": "Manthan Dave",
        "category": "",
        "content": "Hey Guys! In this series, we will be building a chat application in java. I think this will be a great chance for you all guys out there wanting to do something hands on in Java.",
        "url": "/2013/05/03/writing-code-building-a-chat-application-in-java-session-1/"
      }
      ,
    
      "2013-05-02-removing-password-expiry-from-your-unix-machine-linux-and-solaris": {
        "title": "Removing password expiry from your Linux/Unix machine",
        "author": "Manthan Dave",
        "category": "",
        "content": "Here’s a quick one. If you have a server and you do not want the password for a user to expire (as it can screw some things up while its active), you need to execute the following commands as root:passwd -x -1 where is the username whose password expiry you wish to remove. For instance, in my case, if username is dm014, I executed:passwd -x -1 dm014 I have tested this and it works flawlessly on most Linux and Solaris operating systems.",
        "url": "/2013/05/02/removing-password-expiry-from-your-unix-machine-linux-and-solaris/"
      }
      ,
    
      "2013-05-01-compiling-c-easily-on-linux-easy-c-build-system": {
        "title": "Setting up C++ build system on Linux",
        "author": "Manthan Dave",
        "category": "",
        "content": "I was developing some C++ stuff on my Linux virtual machine and wanted to create something that would make it easy for me to compile C++ code. So I wrote up some bash code and added it into .bashrc as a function. Here’s the code:function cpprun(){echo \"Checking if output directory exists...\"if [ ! -d \"./output\" ]; thenecho \"Creating output directory...\"mkdir \"./output\"echo \"Output directory created.\"elif [ -f \"./output/cpprun.o\" ]; thenecho \"Backing up previous output...\"mv ./output/cpprun.o ./output/cpprun.bak.oecho \"Backup finished.\"fiecho \"Running g++ compile...\"g++ -o ./output/cpprun.o *.cppif [ -f \"./output/cpprun.o\" ]; thenecho \"Compile finished.\"echo \"Running output...\"echo \"-----------------\"./output/cpprun.oecho \"-----------------\"echo \"Run finished.\"elseecho \"Compile failed.\"fi}Add this to your .bashrc script and then you should be able to compile your C++ code by typing cpprun in the base directory.",
        "url": "/2013/05/01/compiling-c-easily-on-linux-easy-c-build-system/"
      }
      ,
    
      "2013-04-20-changing-mysql-username-and-password-used-by-phpmyadmin": {
        "title": "Changing MySQL username and password used by PHPMyAdmin",
        "author": "Manthan Dave",
        "category": "",
        "content": "So, I had to do this the other day and I got a bit confused. After poking around a bit, I finally found it. If you have xampp installed, go to the install directory. In my case, this directory is C:xampp which is the default directory. Locate the phpmyadmin folder and open it. Locate the config.inc.php file within the phpmyadmin folder. Open up the file. You need to make change in the following two lines:$cfg['Servers'][$i]['user'] = 'root'; $cfg['Servers'][$i]['password'] = 'password' The underlined parts are the ones that you’ve got to change. This file contains a lot of admin stuff so make sure you look around a bit and understand what it does.If you don’t know what you are doing, ensure you take a backup of the file before changing things.",
        "url": "/2013/04/20/changing-mysql-username-and-password-used-by-phpmyadmin/"
      }
      ,
    
      "2013-04-14-how-to-prevent-class-redefinition-errors-in-c": {
        "title": "Preventing class redefinition errors in C++",
        "author": "Manthan Dave",
        "category": "",
        "content": "I have just started learning C++ and have been getting a lot of these errors. I finally found a solution to this after spending some time on the internet.Say for instance you have a class called “Animal” and g++ complains that this class is being redefined. Just add the following two lines at the very top of the Animal.h file:#ifndef ANIMAL_H#define ANIMAL_Hand add the following at the very bottom of the file:#endifSo, if your class is called SomeClass, then it will be SOMECLASS_H instead of ANIMAL_H. You should have these in every header file. It prevents that header file from being redefined more than once.",
        "url": "/2013/04/14/how-to-prevent-class-redefinition-errors-in-c/"
      }
      ,
    
      "2013-04-13-how-to-tell-which-linux-partition-a-directory-is-on": {
        "title": "How to tell which Linux partition a directory is on?",
        "author": "Manthan Dave",
        "category": "",
        "content": "This is really simple. First of all, you need to know the full path to that directory. “cd” into that directory and type:pwdand it will display full path of the directory you are in. Remember that or even better, copy it to the clipboard. Type the following command to know the name of the partition:dfwhere replace with the full path of your directory. This in my case becomes “df /data/android/sample”.If you are already in the directory, you can type “df .” and it will give details about the partition you are currently on.The output should be something like:Filesystem    1024-blocks      Free %Used    Iused %Iused Mounted on /dev/hd4            32768     16016   52%     2271    14% /The highlighted part is the name of your partition.",
        "url": "/2013/04/13/how-to-tell-which-linux-partition-a-directory-is-on/"
      }
      ,
    
      "2013-01-12-pizza-ordering-system-source-session-2": {
        "title": "Pizza Ordering System source (Session 2)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Hey Guys! Thanks for watching my video. In case you haven’t, the video is down below:&lt;iframe width=\"560\" height=\"315\" src=\"http://www.youtube.com/embed/TnJdzucKbxw\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;&lt;iframe width=\"560\" height=\"315\" src=\"http://www.youtube.com/embed/MouRpkoLlmM\" frameborder=\"0\" allowfullscreen&gt;&lt;/iframe&gt;I hope the video as helpful to you. Please comment below the video or this post for any suggestions/queries. You can download the source code by following the link below:https://www.box.com/s/civvyq1d2wzz69xydqi4",
        "url": "/2013/01/12/pizza-ordering-system-source-session-2/"
      }
      ,
    
      "2012-12-30-job-execution-api-v0-1b-is-now-available": {
        "title": "Job Execution API v0.1b is now available!",
        "author": "Manthan Dave",
        "category": "",
        "content": "I have been working on this for a while and now I have finally released it. This is 0.1 beta so it may contain bugs. However, I am working on it constantly and am trying to improve the API.Job Execution API is all about job execution in Java. You have jobs which comprise of tasks. Each task is a class implementing the ITask interface. For now, the task is not Async so tasks are executed sequentially, in linear pattern. I intend to provide basic tasks which you can use to create standard jobs. You can later extend these tasks and make your own.Download the API here and let me know how it goes:https://www.box.com/s/6cnn93gkrjmuf39sp12d",
        "url": "/2012/12/30/job-execution-api-v0-1b-is-now-available/"
      }
      ,
    
      "2012-08-27-api-for-recognizing-gestures-in-android": {
        "title": "API for recognizing gestures in Android",
        "author": "Manthan Dave",
        "category": "",
        "content": "I have been busy lately. For most of the time, I have been working on gesture API for android. Android already has a built in gesture API but to actually use it, you have to apply mathematics which makes it a bit comlicated for a normal Android developer. AGAPI (Accelerometer Gesture API) adds a layer of abstraction over the existing gesture API to make it easier to recognize gestures. Using AGAPI is very simple. Implement the ShakeListener or any gesture listener class. Register your listener to ShakeManager or corresponding gesture manager instance. Write your code in OnShake or in corresponding event methods and BAM! Your gesture rich application is ready! At this stage of development, the Shake gesture is ready to use. You can get the API here.Note: This API is licensed under Apache License 2.0.",
        "url": "/2012/08/27/api-for-recognizing-gestures-in-android/"
      }
      ,
    
      "2012-04-27-linux-windows-comparison": {
        "title": "Linux vs Windows comparison",
        "author": "Manthan Dave",
        "category": "",
        "content": "Well, I wrote the following article a year ago. A YEAR AGO! I’m actually amazed that I wrote such a thing. Well, so… Here it goes…Linux, as you all know, is a free-open source operating system. Inspire of having number of advantages over Windows, Linux is still free and of course open source. Now, you might be thinking why I’m using terms ‘free’ and ‘open’ source separately. ‘Free’ means that Linux is freely available and downloadable from internet. Some of the vendors like ‘Ubuntu’ offer CDs loaded with Linux absolutely free of cost. Open source means that if you know kernel programming and mechanics of operating system, then you can actually tweak Linux and also program Linux from a scratch. That’s open source and Linux provides that flexibility. Unlike Linux, Windows is not open source. It does not allow user to modify its kernel or components. This is one of the reasons why Linux is hot favourite among young developers.What separates them?The main difference between Linux and windows is their build. Practically, Windows is created only by Microsoft Corporation while Linux has been created and tweaked by entire world! Why entire world? Because, it’s open source. If I have Linux and I find a bug, then I can tweak and remove that thing myself. I also tell the organization about the bug which I fixed. So, they verify it and tell their rest of the consumers about the fix. So, that’s how it all works. With windows, it is not like that. I must appreciate Microsoft’s efforts in improving windows because in new versions of windows – The Vista Series – they have put an improved version of error reporting like Problems Report and Solutions. With that you can just submit the error report. You cannot tell them how to fix that even if you know. Of course, you cannot fix grave errors in windows even if you think because it’s not open source!Who’s better on performance?Practically, on considering stable releases of Linux offered by several organizations like Ubuntu, Debian, Fedora, etc., no doubt Linux outperforms Windows. Windows has always been a bulky operating system taking several minutes to boot up, then a minute to get stable and so on. While with Linux, running on an average PC, booting up is a child’s play for Linux, 10 – 15 seconds is a limit. Even if ‘background processing’ indicator on mouse cursor is on, you can still get your stuff by clicking. Exploring your computer is just with double-click of a button. You’ll never find this speed on Windows!Also, shutting down your PC is a matter of 5-6 seconds for Linux. And what’s more? Linux works on a 1 GHz PC with 128 MB RAM and even 10 GB Hard Drive!What’s your colour today?Unlike Windows, Linux provides a lot of customization features. It is so flexible that you can customize almost everything. From changing your wallpaper to behaviour and animation of minimizing/maximizing a window, you can change and tweak everything. Linux also provides various desktop effects – some of which even require a 3D card – which are super cool.For Linux, wallpapers are billions, themes are millions, and yet there are thousands of programs that help even more to customize Linux. Also various types of desktop environment are available on Linux like GNOME (Most common with cool GUI), KDE (High end graphical interface), and Xfce (Fastest of all). These desktop environments not only handle the look and feel of the Linux environment but also provide you with lot of other flexibilities. Instead of trying out new desktop environments, I prefer to GNOME and its cool Compiz-Fusion Desktop effects which tweak almost all aspects of the operating system. In customization, Linux easily overcomes Windows.Applications?This is where Linux gets outperformed. Almost 91% of the applications available in the market run on windows. 5% work on Mac OS X, 3% of it in Linux and the rest are for other OS. Windows has its roots wide-spread all over the world because it was the first OS to offer the idea of multi-tasking and a lot more new features that were not even thought of at that time. While Linux – which is not that old but still is not as old as windows – has relatively very less number of applications available worldwide. Linux is rapidly getting wide spread in the world of computing because it is a lot more flexible and offers a lot more possibilities than Windows. Linux offers its developers great flexibility of working with applications. Previously, Linux was called non-user friendly because it was difficult to operate but now that it has been tweaked and is available in several forms, Linux has removed this badge of dishonour from itself.Organizations like Ubuntu etc. have made Linux user friendly and easy to use operating system that provides its users with number of options which makes it easy to use.Running Windows on Linux?Also, now you can run Windows applications on Linux. With ‘Wine’ – an application for Linux – you can run windows applications and programs on Linux. Wine uses its DLL library to run Windows applications. If you dual boot Windows and Linux, then it’ll be better. Wine uses Windows library and its own library in combination to run the application smoothly. Wine also has configuration screen where, you can select in which mode Wine should work. There are several modes available based on Windows OS Released. On my Linux, there are following modes:• Windows Vista• Windows XP• Windows Me• Windows 98• Windows 3.0And several others including server editions which I cannot recall right now.Each of these modes create a separate environment for the application to run smoothly. Here, environment does not mean that your Linux look would change. You won’t even notice that! So, are you thinking of running Windows Games on your Linux? Don’t dream that! It won’t run. Games are called as hefty softwares that require large amount of resources and also they have some of functions which are accessible to them in Windows but not in Linux. However, these people are trying a lot to make each and every application of Windows run on Linux. Good luck to them!Where’s the security?Linux has security from viruses. Don’t think that an antivirus system has been integrated to it. It isn’t in this case. Because, Linux does not support viruses! Yes, Viruses don’t run on Linux because they find it hard to harass you while you are logged in. Surprised? Ok, let’s be frank. There’s one virus on Linux and that’s – Bliss. But just think of it. Linux has just one virus to support while Windows is entirely surrounded by viruses. Viruses for Windows – maybe billions – Viruses for Linux – Just One!! And that too is not going to do any big thing to your computer! So, here we have arrived to the point where we must say that Linux is too stout to respond to viruses. Consider a case where a burglar has entered a bank, he shows a gun, fires in the air and expects people to panic. But what happens? People ignore him! He shoots at the cashier but finds that his guns do not work against him. Bullets just fall down touching the cashier! This is the case when virus enters into Linux. Virus would try hard to do harm but Linux won’t execute any operations that virus has told. Virus will try to infiltrate your files but that also would not work because unlike windows, Linux is not having applications chained.Little things that matter!Yet, there are a lot of little things offered by Linux that provide great advantage over Windows. First of all, Linux has a ‘Force Quit’ feature. If you find any misbehaving application (Though you’ll rarely find) that’s taking too long to respond, you can use Force Quit. Force quit is a utility provided in Linux which immediately exits a non-responding application. So, you won’t need to wait minutes for a non-responding application to end. Sometimes, you’ll find that in windows, when an application does not respond, then your computer will also not respond. This happens because in windows, all applications are chained. When running applications are chained, they all get affected if one of them stops responding. This is not the case in Linux, In Linux, applications work independently, meaning, that they are not chained. So, if any damn application gets ‘hanged’ your computer and other applications will continue working in full swing. Second advantage in Linux, that it has an in-built archive manager. With archive manager, you can handle multiple types of archives without installing a separate program for it. Linux also has a highly advanced calculator with three extended modes which are ’Advanced’, ‘Financial’ and ’Programming’. Linux also comes with Password and Encryption key manager which allows you to create passwords for applications and folders. It also allows you to create encryption keys whose strength ranges from 2048 bits to 4096 bits. With that, you can encrypt archives, files and folders to ensure that they are safe. Many Linux editions also allow you to change the way login screen looks and behaviour of the boot loader and its appearance. Linux also allows you to scale CPU usage. Meaning, you can set how much of the CPU power Linux should use. This helps you a lot in saving power. Suppose, you are reading an eBook on your Linux PC during a journey, if you were using windows, and even in power saver mode, you’ll drain a lot of battery. But in Linux, it dims the display automatically. Now, reading an eBook does not require processing power. So, you can set the value to least mode. In this way, Linux will stop most of background processes which drain battery and will ensure that the battery is saved. Now, as were discussing about battery, it reminds me of an amazing feature that Linux provides. Linux has an in-built system that provides detailed information on the battery installed on your laptop. It includes details like your battery model, version, status, and vendor. These details are common in windows. Excluding these, it also tells you details about battery technology, serial number, current charge capacity of battery(in %), current charge(in watt-hour), last full charge(in watt-hour) and design charge(in watt-hour). It also provides you with information based on graphs too! I’m sure that these features are not available on windows.If you refer to new versions of windows, in Vista line, you’ll find that they are having preview panes. The moment, you select an item, its preview (if available) is shown in the preview pane. Previews are available for pictures, music, videos and some other types. Suppose, I click a music file. If my computer or rather Windows media player has that plugin, then it will show a mini media player in the preview pane with a play button. If I click play button, it will search the plugin in Windows Media library and then it will play the file. On my VAIO BZ, windows music preview does not work correctly. For the first file I play on starting the computer, it takes a lot of time in loading the preview pane. After that it plays the file. Now, Ubuntu Linux 9.04 has something new to offer. Now, unlike windows, Ubuntu has a ‘Movie Player’ as default player. While exploring my computer, if I stumble upon a music file which I want to preview, I simply hover my mouse over it and immediately, it’ll start playing the file. But unfortunately in this technique, pausing is not possible. If I move down my mouse from that music file, the file immediately stops playing. Ubuntu also does not offer a seek bar!Also, there’s one thing where Linux surrenders. It’s Silverlight Experience! At Windows, you have Silverlight integrated in Windows Internet Explorer as well as many other web browsers like Firefox. But, in Ubuntu (or I assume in most of Linux operating systems) Silverlight isn’t available. Yes, if you do visit a website featuring Silverlight, it’ll tell you to download an alternative version of Silverlight as moonlight but even after downloading moon light, I can’t get Silverlight experience – It still says that Silverlight is missing.So, what do you learn from this? Most of all high-end Microsoft technologies aren’t shared by Microsoft.For all Photoshop editors, Linux is a disappointment because there’s probably no Linux edition for adobe Photoshop.Now, Linux also has an in built application called Disk Analyser. It is an application which tells you what folder has been eating, rather reserving your disk space. It is very useful application in that way because it lets you know what to delete to free your space. It not only tells you about your file system but tells you the ‘free space sharing’ on any other device that you connect. In this case, windows gets defeated. It’s not having an in built application for this, so, you’ll have to wander-on-the-web to get one!I’ve heard that Linux produces outstanding graphics but I haven’t seen it in action and therefore I tell you about that so sure. But I think that it would be able to produce out-standing graphics because it’s swift. Windows and other operating system themselves allocate very high memory and therefore they have relatively less to dedicate for graphics. Whereas in Linux, this case is totally different. Linux itself uses very less of your RAM as well as your hard drive. So, it can dedicate more graphics to your games and all that stuff. This is just a guess and I am not so sure about it. Built-in games of Linux are very standard – sort of only two-dimensional and therefore, you cannot judge its actual capacity. However, some of real 3D games are available but aren’t satisfactory because they aren’t developed that passionately. I purely do not mean to discourage or hurt the Linux game developers but despite of such nice capabilities of Linux to handle the resources you won’t get any game on Linux that can compete the one on windows. This might be because the real ‘expert’ game developers are looking for money and hence they are moving towards development of games for windows as most of the high-end games in windows aren’t free. Now, you might be wondering about 3D modelling software like Maya? As of now, I haven’t encountered any such software but I am still looking for it. There must be some sort of software available on Linux. Don’t worry, you’ll obviously find one.Speed with appearance?This is very easy question to answer – You compromise speed, you’ll get speed. But after you use Linux, you’ll probably disagree with this. Even after using themes and effects, you’ll observe that your working speed has been little affected. Shocked? I was too but I soon discovered this myself on my computer. Of course, you might suffer from a sort of decrease in speed at first but this decrease in speed will be rather unnoticed by you unless you are using a computer that’s got a configuration which is just on the minimum system requirements of Linux. Now, if you know this, then there are different window managers in Linux. Most popular and used is GNOME. Still, there are two other types called KDE and Xfce. GNOME is for a computer user who wants both speed and appearance. KDE is for one who wants very cool appearance, mostly for high-fi users. Xfce us mostly used by users who have very low system requirements and want speed. Now, this is not always true because some users who just don’t care about appearance even after having a high-end system use Xfce. Another one, which is not a window manager (Though most of users think it is), is Compiz Fusion. It accelerates the appearance of your system and allows you to perform a lot more tweaks like changing the open/close/minimize/maximize animations, window borders, various effects and things like modifying the application switcher method. All these will be better understood by you if you use hands on approach and get at least a Live CD of Ubuntu Linux. Different window managers provide their own options of modifying the appearance on Linux while on windows? You know, you’ll have to stick to the like or not Windows own window manager. Now this is mostly attracting to users but if you consider old versions, then they weren’t attractive. But now, considering Windows Vista and Windows 7, they seem to be cool. Still, you don’t get options of really ‘modifying’ them. Don’t tell me about the ‘themes’ option. That’s of no use. If you want to change desktop background and other sorts of things without affecting the real appearance, then it’s better to do it yourself rather than depending on it.Still thinking of Windows? Think again! Because Linux is used by huge servers and supercomputers. It’s their choice because they rely on the usability, flexibility and resource management of Linux. Shouldn’t you rely on it?",
        "url": "/2012/04/27/linux-windows-comparison/"
      }
      ,
    
      "2012-04-17-how-to-use-timers-in-java": {
        "title": "Using Timer in Java",
        "author": "Manthan Dave",
        "category": "",
        "content": "Well, this post will be similar to the one about Threading. However, it is an important concept and thus needs to be covered. First of all, to time a certain task, you will have to make a Timer object. However, a timer object needs to have a TimerTask object. Now, TimerTask is an abstract class and has a run method which gets called on set frequency. So, for instance, if the frequency of the task has been set to 2000 ms, then the run method gets called every 2000 ms. To use Timers, you will have to create your own class which extends the TimerTask class. This would be something like: public class SomeTask extends TimerTask{@Overridepublic void run() {//Something to do here...}}If you are going to access UI elements of a particular JFrame from your SomeTask class, then you will have to pass a reference of that JFrame in its class constructor. This is because all your UI elements are private and thus cannot be accessed by an external class. You will also have to make methods which would access your private UI elements in the NewJFrame. For a JFrame called NewJFrame, the code would look like:public class SomeTask extends TimerTask{NewJFrame parent;public SomeTask(NewJFrame frame){parent = frame;}@Overridepublic void run() {//Do work here}}&lt;/code&gt;If you want to type Hello every five seconds to a JTextArea in NewJFrame, then you’ll first have to create a method that would do that in your NewJFrame. This method can be as simple as:public void type(){jTextArea1.append(\"Hellon\");}Now, access this method from the run() method of your timer task simply by:parent.type();To run this task, you will have to make a timer by:Timer t = new Timer();Then, create a new object of SomeTask class that you just created.SomeTask st = new SomeTask(this);Notice ‘this’ in the constructor. It is because we asked for it in the class constructor as the task needs to access the UI elements. Now schedule that task by:t.schedule(st,0,2000);The above line takes three parameters. First is the TimerTask object, which in this case is the SomeTask object called st. Second is the intial delay (in ms) which is concerned with how long will it take to start the task for the first time and the third parameter is the frequency of the task which in this case is 2000 ms.Thats it! Now you have scheduled your task and it will run almost instantly (0 ms delay). It will keep running every 2 seconds (2000 ms) until someone calls:st.cancel();Which will stop the task from running.Watch the video tutorial here:",
        "url": "/2012/04/17/how-to-use-timers-in-java/"
      }
      ,
    
      "2012-04-15-threading-in-java": {
        "title": "Threading in Java",
        "author": "Manthan Dave",
        "category": "",
        "content": "Every thread in Java needs a Runnable. To make a runnable, you must first create a class which implements the Runnable interface. Runnable interface has a method called run() which gets called by the Thread. Your class will need to have this method. Put your code in this run() method. Your Runnable class should look similar to:public class Work implements Runnable{@Overridepublic void run() {//do some work..}}Now, you can create a thread in multiple ways but the most appropriate way is to pass the runnable object in its constructor.Thread thread1 = new Thread(new Work());Work work = new Work();Thread thread2 = new Thread(work);thread1.start();thread2.start();Make sure that you say thread1.start() and not thread1.run(). thread1.run() simply calls the run method within the runnable while thread1.start() runs the run method in separate thread.That’s it! Now you know the basics of threading in Java!Check out the video tutorial here:",
        "url": "/2012/04/15/threading-in-java/"
      }
      ,
    
      "2012-02-24-using-sclc-source-code-line-counter": {
        "title": "Using SCLC (Source Code Line Counter)",
        "author": "Manthan Dave",
        "category": "",
        "content": "Well, it is really very simple. Here you go:\tDownload the file. You can get it from here.\tDouble click it. It should open as any other normal application.\tIf it doesn't open as expected or opens WinRAR/WinZIP/7-Zip or any other archiving software, follow here:\tRight click the file and point to Open With.\tClick on Select Default Program.\tJava should be here in the list of programs. If it isn't then click browse and locate your java installation. It should be in C:Program FilesJavajre7binjava.exe\tIf you dont have any folder called Java in Program files, then go to www.java.com and download and install it first.\tOnce Java is installed, follow step 2.\tClick OK. Follow step 2.\tClick on 'Add File' button to add files to it. A dialog box will be presented.\tLocate your source code files. You can select more than one file by selecting one file and then holding the 'Ctrl' key and selecting other files. Alternatively, if you wish to select all files, you can press 'Ctrl + A' to select all files in the directory you are in.\tMake sure that you do not select any executable files. This may cause an error.\tClick OK/Open.\tYou should see list of files in the list above 'Add File' button. If you wish to add more files, follow step 3.\tOnce you are done, click Start. It will start counting lines in the files you have selected.\tYou're done! The table will display number of code lines in each file and below the table you'll find total line count and line density.I hope you like it. Feel free to shout out any suggestions/feedback you have regarding this small application. Have fun!",
        "url": "/2012/02/24/using-sclc-source-code-line-counter/"
      }
      ,
    
      "2012-02-13-conditions-in-linux-shell-script": {
        "title": "If statements in Linux Shell Script",
        "author": "Manthan Dave",
        "category": "",
        "content": "If statements are important in any programming language. I had a bit of hard time while doing if statements in linux since they are different than those in a conventional programming language. First of all, an if statement to compare whether a value in a variable val1 is equal to 5 is like:val1=5if [ $val1 -eq 5 ]then     Code here to do some stuffelse     If above condition is false, then code here gets executedfiNote that spaces are important when defining condition. $val1 refers to the variable and -eq simply implies equal to operator to compare if the $val1 is equal to 5.Seems pretty easy? It is. Other conditional operators are:Conditional OperatorLinux EquivalentGreater than-gtLess than-ltEqual to-eqNot equal to-neNow, if I want to compare strings, for instance, if variable answer is equal to “yes” then the code will be as follows:if [ \"$val\" = \"yes\" ]then     Code here to do some stufffiAgain, spaces are important here.In here, ‘=’ can easily be replaced by conventional conditional operators like ‘&gt;’ or ‘&lt;’ or ‘!=’ etc.",
        "url": "/2012/02/13/conditions-in-linux-shell-script/"
      }
      ,
    
      "2012-02-13-variables-in-linux-shell-script": {
        "title": "Variables in Linux Shell",
        "author": "Manthan Dave",
        "category": "",
        "content": "Well, this has been a bit tricky for me so I decided to put it on my blog so that it can be helpful to you guys.First of all, How to define a variable? count=0There! I’ve just defined a variable count with 0 as its initial value.Now, How do you print value of a variable?echo \"$count\"orecho $countNext. How do you perform calculations on a variable?Okay. So assume that you have two variables called val1 and val2 with values 3 and 4 respectively. Now, you want val3 to have the addition of val1 and val2. So, val1=3val2=4val3=`expr $val1 + $val2`Similarly, to do subtraction:val4=`expr $val1 - $val2`Note that spaces are very important here.Now, if you wish to assign value returned by a command to a variable, then it works by doing:wcount=`wc -w myfile.txt`anything between ` and ` will be executed as linux command and the value returned by that command will be assigned back to the variable to left.",
        "url": "/2012/02/13/variables-in-linux-shell-script/"
      }
      ,
    
      "2012-01-31-how-to-extract-words-from-a-string-in-c-split-string": {
        "title": "How to extract words from a string in C# (Split string)",
        "author": "Manthan Dave",
        "category": "",
        "content": "This is incredibly easy in C#. First of all, you need to have a string from which you want to extract words. A word is a string separated by spaces. To do that, just say:string s = \"Welcome to my blog!\";Now, when I use the Split command, it will return an array of individual strings. So, the implementation will be:string[] words = s.Split(\" \",StringSplitOptions.RemoveEmptyEntries);As a result, words array will contain “Welcome”, “to”, “my”, “blog!”.",
        "url": "/2012/01/31/how-to-extract-words-from-a-string-in-c-split-string/"
      }
      ,
    
      "2012-01-26-google-chrome-versus-mozilla-firefox": {
        "title": "Google Chrome vs Mozilla Firefox",
        "author": "Manthan Dave",
        "category": "",
        "content": "Well, they said that Mozilla firefox is the most efficient browser. I wasn’t satisfied. Hence, I went to check. I am a big fan of Google Chrome. However, I recently learnt that chrome collects browsing information. That didn’t sound good to me and hence I switched to Mozilla Firefox. On the download page, it said that it is the most efficient browser. I downloaded it, but initially I had some issues. It lagged a bit when I used it on my laptop. However, when I used it on my university’s mac (with Windows 7 installed) it was smooth. Hence I was suspicious. I had to check. I closed all applications, refreshed the page few times and fired up Mozilla Firefox and Google Chrome.I opened up www.sciencedaily.com in both the browsers. I fired up Task Manager only to find that Google chrome had three simultaneous processes and firefox had one. This probably means that it is doing multi-threading a lot and has a lot of external handlers. It is a good practice though when you are programming something like a web browser. However, I counted the memory allocated and firefox had like 88,000 and Google Chrome had in total something like 50,000. I was surprised.Then, to satisfy myself that Firefox is better, I opened two tabs in both chrome and firefox. In one tab, I opened the google home page and in another I opened up www.sciencedaily.com. This time, I hoped chrome to jump up. The task manager showed that chrome had four processes, one more than last time I had seen and firefox still had one process. In terms of memory, firefox allocated something like 120,000 and chrome took something like 90,000. I was even more surprised. My quick analysis at that point was that the memory print of chrome increases rapidly with increase in number of tabs while in same conditions, that of firefox increases at relatively slower rate.In the third test, in which I seriously hoped firefox to win, I opened up four tabs as follows:  Tab 1: www.google.com  Tab 2: www.sciencedaily.com  Tab 3: www.bing.com  Tab 4: news.google.comThis time, firefox really won. Its memory print was around 143,000 while at the same time, that of chrome was around 150,000. Yayy! Victory at last!So, well, what does this tell to average computer user? I will still be using firefox. However, if I quickly want to check something like a definition or address, I’d use chrome. However, for tab intensive stuff like research and like that, I will be using firefox.FYI:Chrome: 12.0.742.122Firefox: 5.0",
        "url": "/2012/01/26/google-chrome-versus-mozilla-firefox/"
      }
      ,
    
      "2012-01-24-how-do-i-do-threading-in-c": {
        "title": "Writing multi-threaded code in C#",
        "author": "Manthan Dave",
        "category": "",
        "content": "Threading in C# is no big deal. It has the easiest implementation I’ve ever seen. First of all, you need to have a method that you want to execute on a different thread. For this instance, I’ll put method called Calculate(). Now, this method has to be a void. However, it can take any amount of parameters. So, it can be:public void Calculate()orpublic void Calculate(int x, int y)Now that we know what to execute, we need to create a thread with this method. This is done by:Thread t = new Thread(Calculate());I can now run the thread by typing:t.Start();See Also: How do I do threaded programming in Java?",
        "url": "/2012/01/24/how-do-i-do-threading-in-c/"
      }
      ,
    
      "2012-01-24-how-do-i-do-threaded-programming-in-java": {
        "title": "Multi-threaded programming in Java",
        "author": "Manthan Dave",
        "category": "",
        "content": "My favorite concept in any programming language is Threading. It is awesome. This time, I’ll get started straight away!First of all, before making a thread in java, you need a runnable. Runnable is basically like a piece of code that you want to execute as a separate thread. There are two ways to create a runnable. You can do it by making an anonymous class such as:Runnable r = new Runnable(){public void run(){ //Your code goes here... }}However, this is not a good practice. Hence, it is advised to create a separate class which implements the Runnable interface like:public class MyNewRunnable implements Runnable{ MyNewRunnable(){ } public void run(){ //Your code here... }} In this manner, you can pass any parameters for processing the information using the class constructor. For instance, if you are making a runnable for downloading files, you can pass url of the file through the class constructor. Now, once you have created your runnable, you need a thread in which you have to put the runnable. Then you run the thread, the run method of runnable gets executed. For runnable r, you can make a thread as follows:Thread t = new Thread(r);Then, you can start the thread by typing:t.start();I’d say that threading would help make your program more responsive. This is solely because of the fact that all the heavy processor hungry tasks are carried out in a separate thread and the UI thread is kept clean.See Also: How do I do threading in C#?",
        "url": "/2012/01/24/how-do-i-do-threaded-programming-in-java/"
      }
      ,
    
      "2012-01-22-how-to-make-a-login-in-asp-net-c": {
        "title": "How to make a login page in ASP .NET C#",
        "author": "Manthan Dave",
        "category": "",
        "content": "In this post, I will teach you guys about making a simple login using common page controls. First of all, you need two textboxes, one for username and another for password. I’ll call them usernameTextBox and passwordTextBox for this example. Drag down a button and call it loginButton. Now, in the OnClick event of loginButton, you need to check if the provided combination of username and password is correct or not. You can only do this if you have username and passwords stored somewhere. It is recommended to use a database to do this. Create a AccountDetails table in your database. The table should have username and password fields, both as text (VARCHAR for SQL). The way login is going to work is that you select records from AccountDetails table who have the provided username and password. This query should return one row if the combination matches. If not, then it should return nothing. Here’s the query you need to pass:string query = \"SELECT * FROM AccountDetails WHERE username='\"+usernameTextBox.Text + \"' AND pass='\" + passwordTextBox.Text + \"'\";Supply this query to the command object. This can be OleDbCommand or SqlCommand depending on your type of database. Invoke the ExecuteReader method of the command object and then check if the Read method of the DataReader object returns true. If it does then this means that the login is successful. If not, then login is invalid. Here’s a sample code.string connectionString = \"...\";OleDbConnection conn = new OleDbConnection(connectionString);conn.Open();string queryString = \"SELECT * FROM AccountDetails WHERE username='\"+usernameTextBox.Text + \"' AND pass='\" + passwordTextBox.Text + \"'\";OleDbCommand command = new OleDbCommand(queryString, conn);OleDbDataReader reader = command.ExecuteReader();if(reader.Read()){//Login is successful.}else{//Login failed.}Thats it! Your ASP website should now have a wonderful login to allow geniune users. To extend its capabilities, store the username in a session and then in PageLoad event of rest of the pages which require login, check if the session contains something. If it is null, then the page should redirect to login page. Same if for log-out. In the PageLoad event of the login page, check if the session username contains something. If it does, then clear out the session and display some notification saying that the user has been logged out. Now, in your page, to which the user gets redirected after successful login, put a simple link to login.Now, initially when user lands on the login page, the session username will be empty. However, on typing correct username and password, this session gets filled with the username of the logged user and he/she is redirected to some sort of dashboard page. When the user clicks on the log-out link, he/she gets redirected to the login page which first checks if there is anything in the session. Since the user has previously logged in, there will be a username inside that session. Login page clears it out and displays a message saying that the user has been logged out. I hope that was easy.If you have any queries or doubts, feel free to comment below.If you want to know how session works, click here to see my post on Sessions.",
        "url": "/2012/01/22/how-to-make-a-login-in-asp-net-c/"
      }
      ,
    
      "2012-01-20-using-sessions-in-asp-net": {
        "title": "Using Sessions in ASP .NET",
        "author": "Manthan Dave",
        "category": "",
        "content": "Well, believe it or not, Session is the most important thing when it comes to web design. I used to hate it but now I am a big fan of it. Trust me. You will like it once you get hang of it. So, in this post, I will be teaching you about using them. First of all, a session is like a small memory pit where you can store values. Values in session are lost once the browser closes. However, they stay the same even if you change the page or open new tab. Hence, sessions are used to pass values from one page to another.You can store an object in session. It can be int, double, string or any other FooClass object that you have defined. This is the main benefit of it. However, when taking values back, you need to make sure that you cast them back to its same original form. Here’s how you store value in a session. Let me define some variables.int i=2;string s=\"Hello\";FooClass foo = new FooClass();Now, I will store each of them in session. To store a value, you need to give a session name. This must be unique. You must remember this because you will have to use it to extract value out of session. Here’s how you do it:Session[\"someIntegerVariable\"] = i;Session[\"someStringVariable\"] = s;Session[\"someFooClassObject\"] = foo;Now that I have all of them in the session, I will extract them. Remember, I can only extract variable ‘i’ from ‘someIntegerVariable’ session and not ‘someStringVariable’ session.int newI = (int)Session[\"someIntegerVariable\"];string newS = (string)Session[\"someStringVariable\"];FooClass = (FooClass)Session[\"someFooClassObject\"];Since session stores object only, you will have to cast the object back to its original form. In the code above, I have casted the object stored in ‘someIntegerVariable’ session back to its original form which is int.That’s basically how you use sessions. It is widely used in tracking pages and variety of other purposes, especially when you have to keep something consistent between pages.",
        "url": "/2012/01/20/using-sessions-in-asp-net/"
      }
      ,
    
      "2012-01-20-best-way-to-query-database-in-visual-c": {
        "title": "Best way to query database in Visual C#",
        "author": "Manthan Dave",
        "category": "",
        "content": "Best way to query database in C#Well, this is actually not an official way but my personal choice. In this instance, I will use an access database as an example.While querying the database is a long process in C#, it becomes simplified once you use object orientation. First of all, here’s the conventional way of querying the database:string connectionString = \"...\";OleDbConnection dbConnection = new OleDbConnection(connectionString);dbConnection.Open();string queryString = \"SELECT * FROM Customers\";OleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection);//since this is select query...OleDbDataReader dr = dbCommand.ExecuteReader();//and then you do the reading process...Now, if you have a lot of queries to run, then this would be really painful process to do again and again. Hence, it is better to put the process in a method like this:public OleDbDataReader runSelectQuery(string queryString){string connectionString = \"...\";OleDbConnection dbConnection = new OleDbConnection(connectionString);dbConnection.Open();//using the query string...OleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection);//since this is select query...OleDbDataReader dr = dbCommand.ExecuteReader();//and then you do the reading process...return dr;}The method takes in the select query string and returns the data reader which can then be used to extract the data. To make this more efficient, make a class and put the above method as static. Also, make the connectionString a global variable so that it if you need to change it, then you only have to change it in one place. In the end, the code would look like this:public class QueryClass{private static string connectionString = \"...\";public static OleDbDataReader runSelectQuery(string queryString){OleDbConnection dbConnection = new OleDbConnection(connectionString);dbConnection.Open();//using the query string...OleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection);//since this is select query...OleDbDataReader dr = dbCommand.ExecuteReader();return dr;}}Later, to access the method, you’ll just have to do:OleDbDataReader reader = QueryClass.runSelectQuery(\"SELECT * FROM Customers\");Comment below if you have any queries regarding the code.",
        "url": "/2012/01/20/best-way-to-query-database-in-visual-c/"
      }
      ,
    
      "2012-01-19-displaying-tabular-data-in-visual-c-using-listview": {
        "title": "Displaying tabular data in Visual C# using ListView",
        "author": "Manthan Dave",
        "category": "",
        "content": "ListView is extremely useful in displaying tabular data in C#. Yet, it is an easy to use control.\tDrag the control onto your form and note its name. In this instance, I am going to use ListView1 as name.\tYou will have to add columns to it now. This is how you do it.ListView1.Columns.Add(\"ColumnName\");If you have 5 columns, then you will have to do it five times.\tNow, you will have to have each row that you want to insert as a single dimension string array. Number of items in this array should be equal to number of added columns. If it's not, then rest of string elements will not show up.\tAdd the string array to a ListViewItem. For that, you will have to create a ListViewItem first. ListViewItem's constructor would allow you to pass the string array to it. Do it by typing:ListViewItem li = new ListViewItem(stringarray);\tNow add the ListViewItem to the ListView by typing:ListView1.Items.Add(li);That’s it! Run your application and you’ll have your table beautifully displayed in tabular format.",
        "url": "/2012/01/19/displaying-tabular-data-in-visual-c-using-listview/"
      }
      ,
    
      "2012-01-19-introduction-to-code-efficiency-and-the-big-o-notation": {
        "title": "Code efficiency and The Big O Notation",
        "author": "Manthan Dave",
        "category": "",
        "content": "The big O notation explains how efficient the code is. It is like measurement scale for code efficiency. If you go in details, it can become complex. However, in this post, I’ll explain how to get a rough idea of your code’s efficiency.To roughly calculate it, you need to know size of your problem which is ‘n’ in this case. Look through your code, and analyze the loops that you have. Basically, loops are the determining factor. If your code has 100 loops (not nested) then the efficiency is O(n). No matter what the number is, if your code has no nested loops, then the efficiency is still O(n). However, if you have one single nested loop, then efficiency is n2. Nested loops decrease efficiency of code. This is simply depicted by the fact that n2 is greater than n.To write efficient code, it should be your goal to reduce number of nested loops. To do this, the best way is to reuse values that have been previously generated. My way to do this is by using connected methods. I will be discussing more about this in next posts. You can also exit loops when you have met your aim. In this case, while loop is preferred but if you have more than one condition, you should exit the loop when they are met just to save iterations.If you wish to know more about the big O notation, read the article written by Rob Bell",
        "url": "/2012/01/19/introduction-to-code-efficiency-and-the-big-o-notation/"
      }
      ,
    
      "2012-01-18-welcome": {
        "title": "Welcome!",
        "author": "Manthan Dave",
        "category": "",
        "content": "Welcome to Code Ninjutsu.Code Ninjutsu is about art of writing efficient code. In this blog, we shall discuss methods to write efficient code. I shall also teach tricks that you can perform in different programming languages to get the most out of your code. We shall also see how algorithm works and methods to make it more efficient. We have a lot to talk about. Till then, you can watch videos on my youtube channel:http://www.youtube.com/user/themanthandaveLater then!Manthan Dave",
        "url": "/2012/01/18/welcome/"
      }
      
    
  };
</script>
<script src="https://unpkg.com/lunr/lunr.js"></script>
<script src="/js/search.js"></script>
      </div>
    </main></body>
</html>