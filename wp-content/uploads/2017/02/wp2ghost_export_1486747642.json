{"data":{"posts":[{"id":55,"title":"Welcome!","slug":"welcome","markdown":"\nWelcome to Code Ninjutsu.\n\nCode Ninjutsu is about art of writing efficient code. In this blog, we shall discuss methods to write efficient code. I shall also teach tricks that you can perform in different programming languages to get the most out of your code. We shall also see how algorithm works and methods to make it more efficient. We have a lot to talk about. Till then, you can watch videos on my youtube channel:\n\n[http:\/\/www.youtube.com\/user\/themanthandave](http:\/\/www.youtube.com\/user\/themanthandave)\n\nLater then!\n\nManthan Dave\n\n\n","html":"<p>Welcome to Code Ninjutsu.<\/p>\n<p>Code Ninjutsu is about art of writing efficient code. In this blog, we shall discuss methods to write efficient code. I shall also teach tricks that you can perform in different programming languages to get the most out of your code. We shall also see how algorithm works and methods to make it more efficient. We have a lot to talk about. Till then, you can watch videos on my youtube channel:<\/p>\n<p><a href=\"http:\/\/www.youtube.com\/user\/themanthandave\">http:\/\/www.youtube.com\/user\/themanthandave<\/a><\/p>\n<p>Later then!<\/p>\n<p>Manthan Dave<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 18 Jan 2012 10:06:00 +0000","created_by":1,"updated_at":"Wed, 18 Jan 2012 10:06:00 +0000","updated_by":1,"published_at":"Wed, 18 Jan 2012 10:06:00 +0000","published_by":1},{"id":54,"title":"Code efficiency and The Big O Notation","slug":"introduction-to-code-efficiency-and-the-big-o-notation","markdown":"\nThe big O notation explains how efficient the code is. It is like measurement scale for code efficiency. If you go in details, it can become complex. However, in this post, I\u2019ll explain how to get a rough idea of your code\u2019s efficiency.\n\nTo roughly calculate it, you need to know size of your problem which is \u2018n\u2019 in this case. Look through your code, and analyze the loops that you have. Basically, loops are the determining factor. If your code has 100 loops (not nested) then the efficiency is O(n). No matter what the number is, if your code has no nested loops, then the efficiency is still O(n). However, if you have one single nested loop, then efficiency is n2. Nested loops decrease efficiency of code. This is simply depicted by the fact that n2 is greater than n.\n\nTo write efficient code, it should be your goal to reduce number of nested loops. To do this, the best way is to reuse values that have been previously generated. My way to do this is by using connected methods. I will be discussing more about this in next posts. You can also exit loops when you have met your aim. In this case, while loop is preferred but if you have more than one condition, you should exit the loop when they are met just to save iterations.\n\nIf you wish to know more about the big O notation, read the [article written by Rob Bell](http:\/\/rob-bell.net\/2009\/06\/a-beginners-guide-to-big-o-notation\/)\n\n\n","html":"<p>The big O notation explains how efficient the code is. It is like measurement scale for code efficiency. If you go in details, it can become complex. However, in this post, I&#8217;ll explain how to get a rough idea of your code&#8217;s efficiency.<\/p>\n<p>To roughly calculate it, you need to know size of your problem which is &#8216;n&#8217; in this case. Look through your code, and analyze the loops that you have. Basically, loops are the determining factor. If your code has 100 loops (not nested) then the efficiency is O(n). No matter what the number is, if your code has no nested loops, then the efficiency is still O(n). However, if you have one single nested loop, then efficiency is n2. Nested loops decrease efficiency of code. This is simply depicted by the fact that n2 is greater than n.<\/p>\n<p>To write efficient code, it should be your goal to reduce number of nested loops. To do this, the best way is to reuse values that have been previously generated. My way to do this is by using connected methods. I will be discussing more about this in next posts. You can also exit loops when you have met your aim. In this case, while loop is preferred but if you have more than one condition, you should exit the loop when they are met just to save iterations.<\/p>\n<p>If you wish to know more about the big O notation, read the <a href=\"http:\/\/rob-bell.net\/2009\/06\/a-beginners-guide-to-big-o-notation\/\">article written by Rob Bell<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 19 Jan 2012 17:56:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:59:23 +0000","updated_by":1,"published_at":"Thu, 19 Jan 2012 17:56:00 +0000","published_by":1},{"id":53,"title":"Displaying tabular data in Visual C# using ListView","slug":"displaying-tabular-data-in-visual-c-using-listview","markdown":"\nListView is extremely useful in displaying tabular data in C#. Yet, it is an easy to use control.\n\n1. Drag the control onto your form and note its name. In this instance, I am going to use ListView1 as name.\n2. You will have to add columns to it now. This is how you do it.  \n`ListView1.Columns.Add(\"ColumnName\");`  \n If you have 5 columns, then you will have to do it five times.\n3. Now, you will have to have each row that you want to insert as a single dimension string array. Number of items in this array should be equal to number of added columns. If it\u2019s not, then rest of string elements will not show up.\n4. Add the string array to a ListViewItem. For that, you will have to create a ListViewItem first. ListViewItem\u2019s constructor would allow you to pass the string array to it. Do it by typing:  \n`ListViewItem li = new ListViewItem(stringarray);`\n5. Now add the ListViewItem to the ListView by typing:  \n`ListView1.Items.Add(li);`\n\nThat\u2019s it! Run your application and you\u2019ll have your table beautifully displayed in tabular format.\n\n\n","html":"<p>ListView is extremely useful in displaying tabular data in C#. Yet, it is an easy to use control.<\/p>\n<ol>\n<li>Drag the control onto your form and note its name. In this instance, I am going to use ListView1 as name.<\/li>\n<li>You will have to add columns to it now. This is how you do it.<br \/>\n<code>ListView1.Columns.Add(\"ColumnName\");<\/code><br \/>\nIf you have 5 columns, then you will have to do it five times.<\/li>\n<li>Now, you will have to have each row that you want to insert as a single dimension string array. Number of items in this array should be equal to number of added columns. If it&#8217;s not, then rest of string elements will not show up.<\/li>\n<li>Add the string array to a ListViewItem. For that, you will have to create a ListViewItem first. ListViewItem&#8217;s constructor would allow you to pass the string array to it. Do it by typing:<br \/>\n<code>ListViewItem li = new ListViewItem(stringarray);<\/code><\/li>\n<li>Now add the ListViewItem to the ListView by typing:<br \/>\n<code>ListView1.Items.Add(li);<\/code><\/li>\n<\/ol>\n<p>That&#8217;s it! Run your application and you&#8217;ll have your table beautifully displayed in tabular format.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 19 Jan 2012 22:30:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:57:34 +0000","updated_by":1,"published_at":"Thu, 19 Jan 2012 22:30:00 +0000","published_by":1},{"id":52,"title":"Best way to query database in Visual C#","slug":"best-way-to-query-database-in-visual-c","markdown":"\nBest way to query database in C#  \n Well, this is actually not an official way but my personal choice. In this instance, I will use an access database as an example.  \n While querying the database is a long process in C#, it becomes simplified once you use object orientation. First of all, here\u2019s the conventional way of querying the database:\n\nstring connectionString = \"...\"; OleDbConnection dbConnection = new OleDbConnection(connectionString); dbConnection.Open(); string queryString = \"SELECT * FROM Customers\"; OleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection); \/\/since this is select query... OleDbDataReader dr = dbCommand.ExecuteReader(); \/\/and then you do the reading process...\n\nNow, if you have a lot of queries to run, then this would be really painful process to do again and again. Hence, it is better to put the process in a method like this:\n\npublic OleDbDataReader runSelectQuery(string queryString){ string connectionString = \"...\"; OleDbConnection dbConnection = new OleDbConnection(connectionString); dbConnection.Open(); \/\/using the query string... OleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection); \/\/since this is select query... OleDbDataReader dr = dbCommand.ExecuteReader(); \/\/and then you do the reading process... return dr; }\n\nThe method takes in the select query string and returns the data reader which can then be used to extract the data. To make this more efficient, make a class and put the above method as static. Also, make the connectionString a global variable so that it if you need to change it, then you only have to change it in one place. In the end, the code would look like this:\n\npublic class QueryClass{ private static string connectionString = \"...\"; public static OleDbDataReader runSelectQuery(string queryString){ OleDbConnection dbConnection = new OleDbConnection(connectionString); dbConnection.Open(); \/\/using the query string... OleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection); \/\/since this is select query... OleDbDataReader dr = dbCommand.ExecuteReader(); return dr; } }\n\nLater, to access the method, you\u2019ll just have to do:\n\n`OleDbDataReader reader = QueryClass.runSelectQuery(\"SELECT * FROM Customers\");<br><\/br>`  \n Comment below if you have any queries regarding the code.\n\n\n","html":"<p>Best way to query database in C#<br \/>\nWell, this is actually not an official way but my personal choice. In this instance, I will use an access database as an example.<br \/>\nWhile querying the database is a long process in C#, it becomes simplified once you use object orientation. First of all, here&#8217;s the conventional way of querying the database:<\/p>\n<pre class=\"lang:c#\">string connectionString = \"...\";\r\nOleDbConnection dbConnection = new OleDbConnection(connectionString);\r\ndbConnection.Open();\r\nstring queryString = \"SELECT * FROM Customers\";\r\nOleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection);\r\n\/\/since this is select query...\r\nOleDbDataReader dr = dbCommand.ExecuteReader();\r\n\/\/and then you do the reading process...<\/pre>\n<p>Now, if you have a lot of queries to run, then this would be really painful process to do again and again. Hence, it is better to put the process in a method like this:<\/p>\n<pre class=\"lang:c#\">public OleDbDataReader runSelectQuery(string queryString){\r\nstring connectionString = \"...\";\r\nOleDbConnection dbConnection = new OleDbConnection(connectionString);\r\ndbConnection.Open();\r\n\/\/using the query string...\r\nOleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection);\r\n\/\/since this is select query...\r\nOleDbDataReader dr = dbCommand.ExecuteReader();\r\n\/\/and then you do the reading process...\r\nreturn dr;\r\n}<\/pre>\n<p>The method takes in the select query string and returns the data reader which can then be used to extract the data. To make this more efficient, make a class and put the above method as static. Also, make the connectionString a global variable so that it if you need to change it, then you only have to change it in one place. In the end, the code would look like this:<\/p>\n<pre class=\"lang:c#\">public class QueryClass{\r\n\r\nprivate static string connectionString = \"...\";\r\n\r\npublic static OleDbDataReader runSelectQuery(string queryString){\r\nOleDbConnection dbConnection = new OleDbConnection(connectionString);\r\ndbConnection.Open();\r\n\/\/using the query string...\r\nOleDbCommand dbCommand = new OleDbCommand(queryString,dbConnection);\r\n\/\/since this is select query...\r\nOleDbDataReader dr = dbCommand.ExecuteReader();\r\nreturn dr;\r\n}\r\n}<\/pre>\n<p>Later, to access the method, you&#8217;ll just have to do:<\/p>\n<p><code>OleDbDataReader reader = QueryClass.runSelectQuery(\"SELECT * FROM Customers\");<br \/>\n<\/code><br \/>\nComment below if you have any queries regarding the code.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 20 Jan 2012 15:44:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:56:22 +0000","updated_by":1,"published_at":"Fri, 20 Jan 2012 15:44:00 +0000","published_by":1},{"id":51,"title":"Using Sessions in ASP .NET","slug":"using-sessions-in-asp-net","markdown":"\nWell, believe it or not, Session is the most important thing when it comes to web design. I used to hate it but now I am a big fan of it. Trust me. You will like it once you get hang of it. So, in this post, I will be teaching you about using them. First of all, a session is like a small memory pit where you can store values. Values in session are lost once the browser closes. However, they stay the same even if you change the page or open new tab. Hence, sessions are used to pass values from one page to another.\n\nYou can store an object in session. It can be int, double, string or any other FooClass object that you have defined. This is the main benefit of it. However, when taking values back, you need to make sure that you cast them back to its same original form. Here\u2019s how you store value in a session. Let me define some variables.\n\n```\nint i=2;<br><\/br>\nstring s=\"Hello\";<br><\/br>\nFooClass foo = new FooClass();<br><\/br>```\n\nNow, I will store each of them in session. To store a value, you need to give a session name. This must be unique. You must remember this because you will have to use it to extract value out of session. Here\u2019s how you do it:\n\n```\nSession[\"someIntegerVariable\"] = i;<br><\/br>\nSession[\"someStringVariable\"] = s;<br><\/br>\nSession[\"someFooClassObject\"] = foo;<br><\/br>```\n\nNow that I have all of them in the session, I will extract them. Remember, I can only extract variable \u2018i\u2019 from \u2018someIntegerVariable\u2019 session and not \u2018someStringVariable\u2019 session.\n\n```\nint newI = (int)Session[\"someIntegerVariable\"];<br><\/br>\nstring newS = (string)Session[\"someStringVariable\"];<br><\/br>\nFooClass = (FooClass)Session[\"someFooClassObject\"];<br><\/br>```\n\nSince session stores object only, you will have to cast the object back to its original form. In the code above, I have casted the object stored in \u2018someIntegerVariable\u2019 session back to its original form which is int.\n\nThat\u2019s basically how you use sessions. It is widely used in tracking pages and variety of other purposes, especially when you have to keep something consistent between pages.\n\n\n","html":"<p>Well, believe it or not, Session is the most important thing when it comes to web design. I used to hate it but now I am a big fan of it. Trust me. You will like it once you get hang of it. So, in this post, I will be teaching you about using them. First of all, a session is like a small memory pit where you can store values. Values in session are lost once the browser closes. However, they stay the same even if you change the page or open new tab. Hence, sessions are used to pass values from one page to another.<\/p>\n<p>You can store an object in session. It can be int, double, string or any other FooClass object that you have defined. This is the main benefit of it. However, when taking values back, you need to make sure that you cast them back to its same original form. Here&#8217;s how you store value in a session. Let me define some variables.<\/p>\n<p><code>int i=2;<br \/>\nstring s=\"Hello\";<br \/>\nFooClass foo = new FooClass();<br \/>\n<\/code><\/p>\n<p>Now, I will store each of them in session. To store a value, you need to give a session name. This must be unique. You must remember this because you will have to use it to extract value out of session. Here&#8217;s how you do it:<\/p>\n<p><code>Session[\"someIntegerVariable\"] = i;<br \/>\nSession[\"someStringVariable\"] = s;<br \/>\nSession[\"someFooClassObject\"] = foo;<br \/>\n<\/code><\/p>\n<p>Now that I have all of them in the session, I will extract them. Remember, I can only extract variable &#8216;i&#8217; from &#8216;someIntegerVariable&#8217; session and not &#8216;someStringVariable&#8217; session.<\/p>\n<p><code>int newI = (int)Session[\"someIntegerVariable\"];<br \/>\nstring newS = (string)Session[\"someStringVariable\"];<br \/>\nFooClass = (FooClass)Session[\"someFooClassObject\"];<br \/>\n<\/code><\/p>\n<p>Since session stores object only, you will have to cast the object back to its original form. In the code above, I have casted the object stored in &#8216;someIntegerVariable&#8217; session back to its original form which is int.<\/p>\n<p>That&#8217;s basically how you use sessions. It is widely used in tracking pages and variety of other purposes, especially when you have to keep something consistent between pages.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 20 Jan 2012 22:50:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:45:25 +0000","updated_by":1,"published_at":"Fri, 20 Jan 2012 22:50:00 +0000","published_by":1},{"id":50,"title":"How to make a login page in ASP .NET C#","slug":"how-to-make-a-login-in-asp-net-c","markdown":"\nIn this post, I will teach you guys about making a simple login using common page controls. First of all, you need two textboxes, one for username and another for password. I\u2019ll call them usernameTextBox and passwordTextBox for this example. Drag down a button and call it loginButton. Now, in the OnClick event of loginButton, you need to check if the provided combination of username and password is correct or not. You can only do this if you have username and passwords stored somewhere. It is recommended to use a database to do this. Create a AccountDetails table in your database. The table should have username and password fields, both as text (VARCHAR for SQL). The way login is going to work is that you select records from AccountDetails table who have the provided username and password. This query should return one row if the combination matches. If not, then it should return nothing. Here\u2019s the query you need to pass:\n\n`string query = \"SELECT * FROM AccountDetails WHERE username='\"+usernameTextBox.Text + \"' AND pass='\" + passwordTextBox.Text + \"'\";<br><\/br>`Supply this query to the command object. This can be OleDbCommand or SqlCommand depending on your type of database. Invoke the ExecuteReader method of the command object and then check if the Read method of the DataReader object returns true. If it does then this means that the login is successful. If not, then login is invalid. Here\u2019s a sample code.\n\n```\nstring connectionString = \"...\";<br><\/br>\nOleDbConnection conn = new OleDbConnection(connectionString);<br><\/br>\nconn.Open();<br><\/br>\nstring queryString = \"SELECT * FROM AccountDetails WHERE username='\"+usernameTextBox.Text + \"' AND pass='\" + passwordTextBox.Text + \"'\";<br><\/br>\nOleDbCommand command = new OleDbCommand(queryString, conn);<br><\/br>\nOleDbDataReader reader = command.ExecuteReader();<br><\/br>\nif(reader.Read()){<br><\/br>\n\/\/Login is successful.<br><\/br>\n}else{<br><\/br>\n\/\/Login failed.<br><\/br>\n}<br><\/br>```\nThats it! Your ASP website should now have a wonderful login to allow geniune users. To extend its capabilities, store the username in a session and then in PageLoad event of rest of the pages which require login, check if the session contains something. If it is null, then the page should redirect to login page. Same if for log-out. In the PageLoad event of the login page, check if the session username contains something. If it does, then clear out the session and display some notification saying that the user has been logged out. Now, in your page, to which the user gets redirected after successful login, put a simple link to login.\n\nNow, initially when user lands on the login page, the session username will be empty. However, on typing correct username and password, this session gets filled with the username of the logged user and he\/she is redirected to some sort of dashboard page. When the user clicks on the log-out link, he\/she gets redirected to the login page which first checks if there is anything in the session. Since the user has previously logged in, there will be a username inside that session. Login page clears it out and displays a message saying that the user has been logged out. I hope that was easy.\n\nIf you have any queries or doubts, feel free to comment below.  \n If you want to know how session works, click [here](http:\/\/codeninjutsu.blogspot.com\/2012\/01\/using-sessions-in-asp-net.html) to see my post on Sessions.\n\n\n","html":"<p>In this post, I will teach you guys about making a simple login using common page controls. First of all, you need two textboxes, one for username and another for password. I&#8217;ll call them usernameTextBox and passwordTextBox for this example. Drag down a button and call it loginButton. Now, in the OnClick event of loginButton, you need to check if the provided combination of username and password is correct or not. You can only do this if you have username and passwords stored somewhere. It is recommended to use a database to do this. Create a AccountDetails table in your database. The table should have username and password fields, both as text (VARCHAR for SQL). The way login is going to work is that you select records from AccountDetails table who have the provided username and password. This query should return one row if the combination matches. If not, then it should return nothing. Here&#8217;s the query you need to pass:<\/p>\n<p><code>string query = \"SELECT * FROM AccountDetails WHERE username='\"+usernameTextBox.Text + \"' AND pass='\" + passwordTextBox.Text + \"'\";<br \/>\n<\/code>Supply this query to the command object. This can be OleDbCommand or SqlCommand depending on your type of database. Invoke the ExecuteReader method of the command object and then check if the Read method of the DataReader object returns true. If it does then this means that the login is successful. If not, then login is invalid. Here&#8217;s a sample code.<\/p>\n<p><code>string connectionString = \"...\";<br \/>\nOleDbConnection conn = new OleDbConnection(connectionString);<br \/>\nconn.Open();<br \/>\nstring queryString = \"SELECT * FROM AccountDetails WHERE username='\"+usernameTextBox.Text + \"' AND pass='\" + passwordTextBox.Text + \"'\";<br \/>\nOleDbCommand command = new OleDbCommand(queryString, conn);<br \/>\nOleDbDataReader reader = command.ExecuteReader();<br \/>\nif(reader.Read()){<br \/>\n\/\/Login is successful.<br \/>\n}else{<br \/>\n\/\/Login failed.<br \/>\n}<br \/>\n<\/code>Thats it! Your ASP website should now have a wonderful login to allow geniune users. To extend its capabilities, store the username in a session and then in PageLoad event of rest of the pages which require login, check if the session contains something. If it is null, then the page should redirect to login page. Same if for log-out. In the PageLoad event of the login page, check if the session username contains something. If it does, then clear out the session and display some notification saying that the user has been logged out. Now, in your page, to which the user gets redirected after successful login, put a simple link to login.<\/p>\n<p>Now, initially when user lands on the login page, the session username will be empty. However, on typing correct username and password, this session gets filled with the username of the logged user and he\/she is redirected to some sort of dashboard page. When the user clicks on the log-out link, he\/she gets redirected to the login page which first checks if there is anything in the session. Since the user has previously logged in, there will be a username inside that session. Login page clears it out and displays a message saying that the user has been logged out. I hope that was easy.<\/p>\n<p>If you have any queries or doubts, feel free to comment below.<br \/>\nIf you want to know how session works, click <a href=\"http:\/\/codeninjutsu.blogspot.com\/2012\/01\/using-sessions-in-asp-net.html\">here<\/a> to see my post on Sessions.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 22 Jan 2012 01:30:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:44:24 +0000","updated_by":1,"published_at":"Sun, 22 Jan 2012 01:30:00 +0000","published_by":1},{"id":49,"title":"Multi-threaded programming in Java","slug":"how-do-i-do-threaded-programming-in-java","markdown":"\nMy favorite concept in any programming language is Threading. It is awesome. This time, I\u2019ll get started straight away!\n\nFirst of all, before making a thread in java, you need a runnable. Runnable is basically like a piece of code that you want to execute as a separate thread. There are two ways to create a runnable. You can do it by making an anonymous class such as:\n\n`Runnable r = new Runnable(){`  \n```\n<br><\/br>\npublic void run(){<br><\/br>```\n  \n` \/\/Your code goes here...<span style=\"white-space: pre;\"><br><\/br><\/span>`  \n```\n }<br><\/br>\n}<br><\/br>```\nHowever, this is not a good practice. Hence, it is advised to create a separate class which implements the Runnable interface like:\n\n``  \n`public class MyNewRunnable implements Runnable{`  \n`<br><\/br>`  \n` MyNewRunnable(){`  \n`<br><\/br>`  \n` }`  \n`<br><\/br>`  \n` public void run(){`  \n` \/\/Your code here...`  \n` }`  \n`}`\n\n<div>`\u00a0`<\/div>``In this manner, you can pass any parameters for processing the information using the class constructor. For instance, if you are making a runnable for downloading files, you can pass url of the file through the class constructor. Now, once you have created your runnable, you need a thread in which you have to put the runnable. Then you run the thread, the run method of runnable gets executed. For runnable r, you can make a thread as follows:\n\n`Thread t = new Thread(r);<br><\/br>`  \n Then, you can start the thread by typing:\n\n`t.start();<br><\/br>`  \n I\u2019d say that threading would help make your program more responsive. This is solely because of the fact that all the heavy processor hungry tasks are carried out in a separate thread and the UI thread is kept clean.\n\nSee Also: [How do I do threading in C#?](http:\/\/codeninjutsu.blogspot.com\/2012\/01\/how-do-i-do-threading-in-c.html)\n\n\n","html":"<p>My favorite concept in any programming language is Threading. It is awesome. This time, I&#8217;ll get started straight away!<\/p>\n<p>First of all, before making a thread in java, you need a runnable. Runnable is basically like a piece of code that you want to execute as a separate thread. There are two ways to create a runnable. You can do it by making an anonymous class such as:<\/p>\n<p><code>Runnable r = new Runnable(){<\/code><br \/>\n<code><br \/>\npublic void run(){<br \/>\n<\/code><br \/>\n<code> \/\/Your code goes here...<span style=\"white-space: pre;\"><br \/>\n<\/span><\/code><br \/>\n<code> }<br \/>\n}<br \/>\n<\/code>However, this is not a good practice. Hence, it is advised to create a separate class which implements the Runnable interface like:<\/p>\n<p><code><\/code><br \/>\n<code>public class MyNewRunnable implements Runnable{<\/code><br \/>\n<code><br \/>\n<\/code><br \/>\n<code> MyNewRunnable(){<\/code><br \/>\n<code><br \/>\n<\/code><br \/>\n<code> }<\/code><br \/>\n<code><br \/>\n<\/code><br \/>\n<code> public void run(){<\/code><br \/>\n<code> \/\/Your code here...<\/code><br \/>\n<code> }<\/code><br \/>\n<code>}<\/code><\/p>\n<div><code>\u00a0<\/code><\/div>\n<p><code><\/code>In this manner, you can pass any parameters for processing the information using the class constructor. For instance, if you are making a runnable for downloading files, you can pass url of the file through the class constructor. Now, once you have created your runnable, you need a thread in which you have to put the runnable. Then you run the thread, the run method of runnable gets executed. For runnable r, you can make a thread as follows:<\/p>\n<p><code>Thread t = new Thread(r);<br \/>\n<\/code><br \/>\nThen, you can start the thread by typing:<\/p>\n<p><code>t.start();<br \/>\n<\/code><br \/>\nI&#8217;d say that threading would help make your program more responsive. This is solely because of the fact that all the heavy processor hungry tasks are carried out in a separate thread and the UI thread is kept clean.<\/p>\n<p>See Also: <a href=\"http:\/\/codeninjutsu.blogspot.com\/2012\/01\/how-do-i-do-threading-in-c.html\">How do I do threading in C#?<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 24 Jan 2012 20:55:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:25:21 +0000","updated_by":1,"published_at":"Tue, 24 Jan 2012 20:55:00 +0000","published_by":1},{"id":48,"title":"Writing multi-threaded code in C#","slug":"how-do-i-do-threading-in-c","markdown":"\nThreading in C# is no big deal. It has the easiest implementation I\u2019ve ever seen. First of all, you need to have a method that you want to execute on a different thread. For this instance, I\u2019ll put method called Calculate(). Now, this method has to be a void. However, it can take any amount of parameters. So, it can be:\n\n`public void Calculate()<br><\/br>`  \n or\n\n`public void Calculate(int x, int y)<br><\/br>`  \n Now that we know what to execute, we need to create a thread with this method. This is done by:\n\n`Thread t = new Thread(Calculate());<br><\/br>`  \n I can now run the thread by typing:\n\n`t.Start();<br><\/br>`  \n See Also: [How do I do threaded programming in Java?](http:\/\/codeninjutsu.blogspot.com\/2012\/01\/how-do-i-do-threaded-programming-in.html)\n\n\n","html":"<p>Threading in C# is no big deal. It has the easiest implementation I&#8217;ve ever seen. First of all, you need to have a method that you want to execute on a different thread. For this instance, I&#8217;ll put method called Calculate(). Now, this method has to be a void. However, it can take any amount of parameters. So, it can be:<\/p>\n<p><code>public void Calculate()<br \/>\n<\/code><br \/>\nor<\/p>\n<p><code>public void Calculate(int x, int y)<br \/>\n<\/code><br \/>\nNow that we know what to execute, we need to create a thread with this method. This is done by:<\/p>\n<p><code>Thread t = new Thread(Calculate());<br \/>\n<\/code><br \/>\nI can now run the thread by typing:<\/p>\n<p><code>t.Start();<br \/>\n<\/code><br \/>\nSee Also: <a href=\"http:\/\/codeninjutsu.blogspot.com\/2012\/01\/how-do-i-do-threaded-programming-in.html\">How do I do threaded programming in Java?<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 24 Jan 2012 22:01:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:52:48 +0000","updated_by":1,"published_at":"Tue, 24 Jan 2012 22:01:00 +0000","published_by":1},{"id":47,"title":"Google Chrome vs Mozilla Firefox","slug":"google-chrome-versus-mozilla-firefox","markdown":"\nWell, they said that Mozilla firefox is the most efficient browser. I wasn\u2019t satisfied. Hence, I went to check. I am a big fan of Google Chrome. However, I recently learnt that chrome collects browsing information. That didn\u2019t sound good to me and hence I switched to Mozilla Firefox. On the download page, it said that it is the most efficient browser. I downloaded it, but initially I had some issues. It lagged a bit when I used it on my laptop. However, when I used it on my university\u2019s mac (with Windows 7 installed) it was smooth. Hence I was suspicious. I had to check. I closed all applications, refreshed the page few times and fired up Mozilla Firefox and Google Chrome.\n\nI opened up www.sciencedaily.com in both the browsers. I fired up Task Manager only to find that Google chrome had three simultaneous processes and firefox had one. This probably means that it is doing multi-threading a lot and has a lot of external handlers. It is a good practice though when you are programming something like a web browser. However, I counted the memory allocated and firefox had like 88,000 and Google Chrome had in total something like 50,000. I was surprised.\n\nThen, to satisfy myself that Firefox is better, I opened two tabs in both chrome and firefox. In one tab, I opened the google home page and in another I opened up www.sciencedaily.com. This time, I hoped chrome to jump up. The task manager showed that chrome had four processes, one more than last time I had seen and firefox still had one process. In terms of memory, firefox allocated something like 120,000 and chrome took something like 90,000. I was even more surprised. My quick analysis at that point was that the memory print of chrome increases rapidly with increase in number of tabs while in same conditions, that of firefox increases at relatively slower rate.\n\nIn the third test, in which I seriously hoped firefox to win, I opened up four tabs as follows:  \n 1. Tab 1: www.google.com  \n 2. Tab 2: www.sciencedaily.com  \n 3. Tab 3: www.bing.com  \n 4. Tab 4: news.google.com\n\nThis time, firefox really won. Its memory print was around 143,000 while at the same time, that of chrome was around 150,000. Yayy! Victory at last!\n\nSo, well, what does this tell to average computer user? I will still be using firefox. However, if I quickly want to check something like a definition or address, I\u2019d use chrome. However, for tab intensive stuff like research and like that, I will be using firefox.\n\n**FYI:**  \n Chrome: 12.0.742.122  \n Firefox: 5.0\n\n\n","html":"<p>Well, they said that Mozilla firefox is the most efficient browser. I wasn&#8217;t satisfied. Hence, I went to check. I am a big fan of Google Chrome. However, I recently learnt that chrome collects browsing information. That didn&#8217;t sound good to me and hence I switched to Mozilla Firefox. On the download page, it said that it is the most efficient browser. I downloaded it, but initially I had some issues. It lagged a bit when I used it on my laptop. However, when I used it on my university&#8217;s mac (with Windows 7 installed) it was smooth. Hence I was suspicious. I had to check. I closed all applications, refreshed the page few times and fired up Mozilla Firefox and Google Chrome.<\/p>\n<p>I opened up www.sciencedaily.com in both the browsers. I fired up Task Manager only to find that Google chrome had three simultaneous processes and firefox had one. This probably means that it is doing multi-threading a lot and has a lot of external handlers. It is a good practice though when you are programming something like a web browser. However, I counted the memory allocated and firefox had like 88,000 and Google Chrome had in total something like 50,000. I was surprised.<\/p>\n<p>Then, to satisfy myself that Firefox is better, I opened two tabs in both chrome and firefox. In one tab, I opened the google home page and in another I opened up www.sciencedaily.com. This time, I hoped chrome to jump up. The task manager showed that chrome had four processes, one more than last time I had seen and firefox still had one process. In terms of memory, firefox allocated something like 120,000 and chrome took something like 90,000. I was even more surprised. My quick analysis at that point was that the memory print of chrome increases rapidly with increase in number of tabs while in same conditions, that of firefox increases at relatively slower rate.<\/p>\n<p>In the third test, in which I seriously hoped firefox to win, I opened up four tabs as follows:<br \/>\n1. Tab 1: www.google.com<br \/>\n2. Tab 2: www.sciencedaily.com<br \/>\n3. Tab 3: www.bing.com<br \/>\n4. Tab 4: news.google.com<\/p>\n<p>This time, firefox really won. Its memory print was around 143,000 while at the same time, that of chrome was around 150,000. Yayy! Victory at last!<\/p>\n<p>So, well, what does this tell to average computer user? I will still be using firefox. However, if I quickly want to check something like a definition or address, I&#8217;d use chrome. However, for tab intensive stuff like research and like that, I will be using firefox.<\/p>\n<p><b>FYI:<\/b><br \/>\nChrome: 12.0.742.122<br \/>\nFirefox: 5.0<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 26 Jan 2012 12:43:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:39:01 +0000","updated_by":1,"published_at":"Thu, 26 Jan 2012 12:43:00 +0000","published_by":1},{"id":46,"title":"How to extract words from a string in C# (Split string)","slug":"how-to-extract-words-from-a-string-in-c-split-string","markdown":"\nThis is incredibly easy in C#. First of all, you need to have a string from which you want to extract words. A word is a string separated by spaces. To do that, just say:\n\n`string s = \"Welcome to my blog!\";`\n\nNow, when I use the `Split` command, it will return an array of individual strings. So, the implementation will be:\n\n`string[] words = s.Split(\" \",StringSplitOptions.RemoveEmptyEntries);`\n\nAs a result, `words` array will contain \u201cWelcome\u201d, \u201cto\u201d, \u201cmy\u201d, \u201cblog!\u201d.\n\n\n","html":"<p>This is incredibly easy in C#. First of all, you need to have a string from which you want to extract words. A word is a string separated by spaces. To do that, just say:<\/p>\n<p><code>string s = \"Welcome to my blog!\";<\/code><\/p>\n<p>Now, when I use the <code>Split<\/code> command, it will return an array of individual strings. So, the implementation will be:<\/p>\n<p><code>string[] words = s.Split(\" \",StringSplitOptions.RemoveEmptyEntries);<\/code><\/p>\n<p>As a result, <code>words<\/code> array will contain &#8220;Welcome&#8221;, &#8220;to&#8221;, &#8220;my&#8221;, &#8220;blog!&#8221;.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 31 Jan 2012 15:48:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:51:53 +0000","updated_by":1,"published_at":"Tue, 31 Jan 2012 15:48:00 +0000","published_by":1},{"id":44,"title":"Variables in Linux Shell","slug":"variables-in-linux-shell-script","markdown":"\nWell, this has been a bit tricky for me so I decided to put it on my blog so that it can be helpful to you guys.\n\nFirst of all, **How to define a variable?**\n\n``  \n`count=0`\n\nThere! I\u2019ve just defined a variable count with 0 as its initial value.  \n Now, **How do you print value of a variable?**\n\n`echo \"$count\"`\n\nor\n\n`echo $count`\n\nNext. **How do you perform calculations on a variable?**  \n Okay. So assume that you have two variables called val1 and val2 with values 3 and 4 respectively. Now, you want val3 to have the addition of val1 and val2. So,\n\n``  \n`val1=3`  \n`val2=4`  \n`val3=`expr $val1 + $val2``\n\nSimilarly, to do subtraction:\n\n`val4=`expr $val1 - $val2``\n\nNote that spaces are very important here.\n\nNow, if you wish to assign value returned by a command to a variable, then it works by doing:\n\n`wcount=`wc -w myfile.txt``\n\nanything between ` and ` will be executed as linux command and the value returned by that command will be assigned back to the variable to left.\n\n\n","html":"<p>Well, this has been a bit tricky for me so I decided to put it on my blog so that it can be helpful to you guys.<\/p>\n<p>First of all, <b>How to define a variable?<\/b><\/p>\n<p><code> <\/code><br \/>\n<code>count=0<\/code><\/p>\n<p>There! I&#8217;ve just defined a variable count with 0 as its initial value.<br \/>\nNow, <b>How do you print value of a variable?<\/b><\/p>\n<p><code>echo \"$count\"<\/code><\/p>\n<p>or<\/p>\n<p><code>echo $count<\/code><\/p>\n<p>Next. <b>How do you perform calculations on a variable?<\/b><br \/>\nOkay. So assume that you have two variables called val1 and val2 with values 3 and 4 respectively. Now, you want val3 to have the addition of val1 and val2. So,<\/p>\n<p><code> <\/code><br \/>\n<code>val1=3<\/code><br \/>\n<code>val2=4<\/code><br \/>\n<code>val3=`expr $val1 + $val2`<\/code><\/p>\n<p>Similarly, to do subtraction:<\/p>\n<p><code>val4=`expr $val1 - $val2`<\/code><\/p>\n<p>Note that spaces are very important here.<\/p>\n<p>Now, if you wish to assign value returned by a command to a variable, then it works by doing:<\/p>\n<p><code>wcount=`wc -w myfile.txt`<\/code><\/p>\n<p>anything between ` and ` will be executed as linux command and the value returned by that command will be assigned back to the variable to left.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 13 Feb 2012 18:16:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:16:19 +0000","updated_by":1,"published_at":"Mon, 13 Feb 2012 18:16:00 +0000","published_by":1},{"id":43,"title":"If statements in Linux Shell Script","slug":"conditions-in-linux-shell-script","markdown":"\nIf statements are important in any programming language. I had a bit of hard time while doing if statements in linux since they are different than those in a conventional programming language. First of all, an if statement to compare whether a value in a variable val1 is equal to 5 is like:  \n`val1=5`  \n`if [ $val1 -eq 5 ]`  \n`then`  \n`\u00a0\u00a0\u00a0\u00a0 <i>Code here to do some stuff<\/i>`  \n`else`  \n`\u00a0\u00a0\u00a0\u00a0 <i>If above condition is false, then code here gets executed<\/i>`  \n`fi`\n\nNote that **spaces are important **when defining condition. $val1 refers to the variable and -eq simply implies equal to operator to compare if the $val1 is equal to 5.\n\nSeems pretty easy? It is. Other conditional operators are:\n\n<table border=\"1\"><tbody><tr align=\"center\"><td>**Conditional Operator**<\/td><td>**Linux Equivalent**<\/td><\/tr><tr><td>Greater than<\/td><td>-gt<\/td><\/tr><tr><td>Less than<\/td><td>-lt<\/td><\/tr><tr><td>Equal to<\/td><td>-eq<\/td><\/tr><tr><td>Not equal to<\/td><td>-ne<\/td><\/tr><\/tbody><\/table>Now, if I want to compare strings, for instance, if variable answer is equal to \u201cyes\u201d then the code will be as follows:  \n`if [ \"$val\" = \"yes\" ]`  \n`then`  \n`\u00a0\u00a0\u00a0\u00a0 <i>Code here to do some stuff<\/i>`  \n`fi`\n\nAgain, **spaces are important **here.  \n In here, \u2018=\u2019 can easily be replaced by conventional conditional operators like \u2018>\u2019 or \u2018<\u2018 or \u2018!=\u2019 etc.\n\n\n","html":"<p>If statements are important in any programming language. I had a bit of hard time while doing if statements in linux since they are different than those in a conventional programming language. First of all, an if statement to compare whether a value in a variable val1 is equal to 5 is like:<br \/>\n<code>val1=5<\/code><br \/>\n<code>if [ $val1 -eq 5 ]<\/code><br \/>\n<code>then<\/code><br \/>\n<code>\u00a0\u00a0\u00a0\u00a0 <i>Code here to do some stuff<\/i><\/code><br \/>\n<code>else<\/code><br \/>\n<code>\u00a0\u00a0\u00a0\u00a0 <i>If above condition is false, then code here gets executed<\/i><\/code><br \/>\n<code>fi<\/code><\/p>\n<p>Note that <b>spaces are important <\/b>when defining condition. $val1 refers to the variable and -eq simply implies equal to operator to compare if the $val1 is equal to 5.<\/p>\n<p>Seems pretty easy? It is. Other conditional operators are:<\/p>\n<table border=\"1\">\n<tbody>\n<tr align=\"center\">\n<td><b>Conditional Operator<\/b><\/td>\n<td><b>Linux Equivalent<\/b><\/td>\n<\/tr>\n<tr>\n<td>Greater than<\/td>\n<td>-gt<\/td>\n<\/tr>\n<tr>\n<td>Less than<\/td>\n<td>-lt<\/td>\n<\/tr>\n<tr>\n<td>Equal to<\/td>\n<td>-eq<\/td>\n<\/tr>\n<tr>\n<td>Not equal to<\/td>\n<td>-ne<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n<p>Now, if I want to compare strings, for instance, if variable answer is equal to &#8220;yes&#8221; then the code will be as follows:<br \/>\n<code>if [ \"$val\" = \"yes\" ]<\/code><br \/>\n<code>then<\/code><br \/>\n<code>\u00a0\u00a0\u00a0\u00a0 <i>Code here to do some stuff<\/i><\/code><br \/>\n<code>fi<\/code><\/p>\n<p>Again, <b>spaces are important <\/b>here.<br \/>\nIn here, &#8216;=&#8217; can easily be replaced by conventional conditional operators like &#8216;&gt;&#8217; or &#8216;&lt;&#8216; or &#8216;!=&#8217; etc.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 13 Feb 2012 18:32:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:15:49 +0000","updated_by":1,"published_at":"Mon, 13 Feb 2012 18:32:00 +0000","published_by":1},{"id":41,"title":"Using SCLC (Source Code Line Counter)","slug":"using-sclc-source-code-line-counter","markdown":"\nWell, it is really very simple. Here you go:\n\n1. Download the file. You can get it from [here](http:\/\/min.us\/mXvNL53WB#1o).\n2. Double click it. It should open as any other normal application. 1. If it doesn\u2019t open as expected or opens WinRAR\/WinZIP\/7-Zip or any other archiving software, follow here: 1. Right click the file and point to Open With.\n2. Click on Select Default Program.\n3. Java should be here in the list of programs. If it isn\u2019t then click browse and locate your java installation. It should be in C:Program FilesJavajre7binjava.exe 1. If you dont have any folder called Java in Program files, then go to www.java.com and download and install it first.\n2. Once Java is installed, follow step 2.\n4. Click OK. Follow step 2.\n3. Click on \u2018Add File\u2019 button to add files to it. A dialog box will be presented.\n4. Locate your source code files. You can select more than one file by selecting one file and then holding the \u2018Ctrl\u2019 key and selecting other files. Alternatively, if you wish to select all files, you can press \u2018Ctrl + A\u2019 to select all files in the directory you are in. 1. Make sure that you do not select any executable files. This may cause an error.\n5. Click OK\/Open.\n6. You should see list of files in the list above \u2018Add File\u2019 button. If you wish to add more files, follow step 3.\n7. Once you are done, click Start. It will start counting lines in the files you have selected.\n8. You\u2019re done! The table will display number of code lines in each file and below the table you\u2019ll find total line count and line density.\n\nI hope you like it. Feel free to shout out any suggestions\/feedback you have regarding this small application. Have fun!\n\n\n","html":"<p>Well, it is really very simple. Here you go:<\/p>\n<ol>\n<li>Download the file. You can get it from <a href=\"http:\/\/min.us\/mXvNL53WB#1o\" target=\"_blank\">here<\/a>.<\/li>\n<li>Double click it. It should open as any other normal application.\n<ol>\n<li>If it doesn&#8217;t open as expected or opens WinRAR\/WinZIP\/7-Zip or any other archiving software, follow here:\n<ol>\n<li>Right click the file and point to Open With.<\/li>\n<li>Click on Select Default Program.<\/li>\n<li>Java should be here in the list of programs. If it isn&#8217;t then click browse and locate your java installation. It should be in C:Program FilesJavajre7binjava.exe\n<ol>\n<li>If you dont have any folder called Java in Program files, then go to www.java.com and download and install it first.<\/li>\n<li>Once Java is installed, follow step 2.<\/li>\n<\/ol>\n<\/li>\n<li>Click OK. Follow step 2.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<\/li>\n<li>Click on &#8216;Add File&#8217; button to add files to it. A dialog box will be presented.<\/li>\n<li>Locate your source code files. You can select more than one file by selecting one file and then holding the &#8216;Ctrl&#8217; key and selecting other files. Alternatively, if you wish to select all files, you can press &#8216;Ctrl + A&#8217; to select all files in the directory you are in.\n<ol>\n<li>Make sure that you do not select any executable files. This may cause an error.<\/li>\n<\/ol>\n<\/li>\n<li>Click OK\/Open.<\/li>\n<li>You should see list of files in the list above &#8216;Add File&#8217; button. If you wish to add more files, follow step 3.<\/li>\n<li>Once you are done, click Start. It will start counting lines in the files you have selected.<\/li>\n<li>You&#8217;re done! The table will display number of code lines in each file and below the table you&#8217;ll find total line count and line density.<\/li>\n<\/ol>\n<p>I hope you like it. Feel free to shout out any suggestions\/feedback you have regarding this small application. Have fun!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 24 Feb 2012 00:01:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:05:46 +0000","updated_by":1,"published_at":"Fri, 24 Feb 2012 00:01:00 +0000","published_by":1},{"id":39,"title":"Threading in Java","slug":"threading-in-java","markdown":"\nEvery thread in Java needs a Runnable. To make a runnable, you must first create a class which implements the Runnable interface. Runnable interface has a method called `run()` which gets called by the Thread. Your class will need to have this method. Put your code in this `run()` method. Your Runnable class should look similar to:\n\npublic class Work implements Runnable{ @Override public void run() { \/\/do some work.. } }\n\nNow, you can create a thread in multiple ways but the most appropriate way is to pass the runnable object in its constructor.\n\nThread thread1 = new Thread(new Work()); Work work = new Work(); Thread thread2 = new Thread(work);\n\nthread1.start(); thread2.start();\n\nMake sure that you say `thread1.start()` and not `thread1.run()`. `thread1.run()` simply calls the run method within the runnable while `thread1.start()` runs the run method in separate thread.\n\nThat\u2019s it! Now you know the basics of threading in Java!\n\nCheck out the video tutorial here:  \n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"360\" src=\"http:\/\/www.youtube.com\/embed\/videoseries?list=PL4DD8CFBE563F29AC&hl=en_US\" width=\"640\"><\/iframe>\n\n\n","html":"<p>Every thread in Java needs a Runnable. To make a runnable, you must first create a class which implements the Runnable interface. Runnable interface has a method called <code>run()<\/code> which gets called by the Thread. Your class will need to have this method. Put your code in this <code>run()<\/code> method. Your Runnable class should look similar to:<\/p>\n<pre class=\"lang:java\">public class Work implements Runnable{\r\n\r\n@Override\r\npublic void run() {\r\n\/\/do some work..\r\n}\r\n}\r\n<\/pre>\n<p>Now, you can create a thread in multiple ways but the most appropriate way is to pass the runnable object in its constructor.<\/p>\n<pre class=\"lang:java\">Thread thread1 = new Thread(new Work());\r\nWork work = new Work();\r\nThread thread2 = new Thread(work);<\/pre>\n<pre class=\"lang:java\">thread1.start();\r\nthread2.start();\r\n<\/pre>\n<p>Make sure that you say <code>thread1.start()<\/code> and not <code>thread1.run()<\/code>. <code>thread1.run()<\/code> simply calls the run method within the runnable while <code>thread1.start()<\/code> runs the run method in separate thread.<\/p>\n<p>That&#8217;s it! Now you know the basics of threading in Java!<\/p>\n<p>Check out the video tutorial here:<br \/>\n<iframe width=\"640\" height=\"360\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\" src=\"http:\/\/www.youtube.com\/embed\/videoseries?list=PL4DD8CFBE563F29AC&amp;hl=en_US\"><\/iframe><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 15 Apr 2012 19:38:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:24:06 +0000","updated_by":1,"published_at":"Sun, 15 Apr 2012 19:38:00 +0000","published_by":1},{"id":38,"title":"Using Timer in Java","slug":"how-to-use-timers-in-java","markdown":"\nWell, this post will be similar to the one about Threading. However, it is an important concept and thus needs to be covered. First of all, to time a certain task, you will have to make a Timer object. However, a timer object needs to have a TimerTask object. Now, TimerTask is an abstract class and has a run method which gets called on set frequency. So, for instance, if the frequency of the task has been set to 2000 ms, then the run method gets called every 2000 ms. To use Timers, you will have to create your own class which extends the TimerTask class. This would be something like:\n\n```\n\u00a0public class SomeTask extends TimerTask{<br><\/br>\n@Override<br><\/br>\npublic void run() {<br><\/br>\n\/\/Something to do here...<br><\/br>\n}<br><\/br>\n}```\n\nIf you are going to access UI elements of a particular JFrame from your SomeTask class, then you will have to pass a reference of that JFrame in its class constructor. This is because all your UI elements are private and thus cannot be accessed by an external class. You will also have to make methods which would access your private UI elements in the NewJFrame. For a JFrame called NewJFrame, the code would look like:\n\n`public class SomeTask extends TimerTask{`\n\nNewJFrame parent;  \n public SomeTask(NewJFrame frame){  \n parent = frame;  \n }\n\n@Override  \n public void run() {  \n \/\/Do work here  \n }  \n }\n\nIf you want to type Hello every five seconds to a JTextArea in NewJFrame, then you\u2019ll first have to create a method that would do that in your NewJFrame. This method can be as simple as:\n\n```\npublic void type(){<br><\/br>\njTextArea1.append(\"Hellon\");<br><\/br>\n}```\n\nNow, access this method from the run() method of your timer task simply by:\n\n`parent.type();`\n\nTo run this task, you will have to make a timer by:\n\n`Timer t = new Timer();`\n\nThen, create a new object of SomeTask class that you just created.\n\n`SomeTask st = new SomeTask(this);`\n\nNotice \u2018this\u2019 in the constructor. It is because we asked for it in the class constructor as the task needs to access the UI elements. Now schedule that task by:\n\n`t.schedule(st,0,2000);`\n\nThe above line takes three parameters. First is the TimerTask object, which in this case is the SomeTask object called st. Second is the intial delay (in ms) which is concerned with how long will it take to start the task for the first time and the third parameter is the frequency of the task which in this case is 2000 ms.\n\nThats it! Now you have scheduled your task and it will run almost instantly (0 ms delay). It will keep running every 2 seconds (2000 ms) until someone calls:\n\n`st.cancel();`\n\nWhich will stop the task from running.\n\nWatch the video tutorial here:\n\n<center><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"360\" src=\"http:\/\/www.youtube.com\/embed\/-loYJQsE3TY\" width=\"480\"><\/iframe><\/center>\n","html":"<p>Well, this post will be similar to the one about Threading. However, it is an important concept and thus needs to be covered. First of all, to time a certain task, you will have to make a Timer object. However, a timer object needs to have a TimerTask object. Now, TimerTask is an abstract class and has a run method which gets called on set frequency. So, for instance, if the frequency of the task has been set to 2000 ms, then the run method gets called every 2000 ms. To use Timers, you will have to create your own class which extends the TimerTask class. This would be something like:<\/p>\n<p><code>\u00a0public class SomeTask extends TimerTask{<br \/>\n@Override<br \/>\npublic void run() {<br \/>\n\/\/Something to do here...<br \/>\n}<br \/>\n}<\/code><\/p>\n<p>If you are going to access UI elements of a particular JFrame from your SomeTask class, then you will have to pass a reference of that JFrame in its class constructor. This is because all your UI elements are private and thus cannot be accessed by an external class. You will also have to make methods which would access your private UI elements in the NewJFrame. For a JFrame called NewJFrame, the code would look like:<\/p>\n<p><code>public class SomeTask extends TimerTask{<\/p>\n<p>NewJFrame parent;<br \/>\npublic SomeTask(NewJFrame frame){<br \/>\nparent = frame;<br \/>\n}<\/p>\n<p>@Override<br \/>\npublic void run() {<br \/>\n\/\/Do work here<br \/>\n}<br \/>\n}<\/code><\/p>\n<p>If you want to type Hello every five seconds to a JTextArea in NewJFrame, then you&#8217;ll first have to create a method that would do that in your NewJFrame. This method can be as simple as:<\/p>\n<p><code>public void type(){<br \/>\njTextArea1.append(\"Hellon\");<br \/>\n}<\/code><\/p>\n<p>Now, access this method from the run() method of your timer task simply by:<\/p>\n<p><code>parent.type();<\/code><\/p>\n<p>To run this task, you will have to make a timer by:<\/p>\n<p><code>Timer t = new Timer();<\/code><\/p>\n<p>Then, create a new object of SomeTask class that you just created.<\/p>\n<p><code>SomeTask st = new SomeTask(this);<\/code><\/p>\n<p>Notice &#8216;this&#8217; in the constructor. It is because we asked for it in the class constructor as the task needs to access the UI elements. Now schedule that task by:<\/p>\n<p><code>t.schedule(st,0,2000);<\/code><\/p>\n<p>The above line takes three parameters. First is the TimerTask object, which in this case is the SomeTask object called st. Second is the intial delay (in ms) which is concerned with how long will it take to start the task for the first time and the third parameter is the frequency of the task which in this case is 2000 ms.<\/p>\n<p>Thats it! Now you have scheduled your task and it will run almost instantly (0 ms delay). It will keep running every 2 seconds (2000 ms) until someone calls:<\/p>\n<p><code>st.cancel();<\/code><\/p>\n<p>Which will stop the task from running.<\/p>\n<p>Watch the video tutorial here:<\/p>\n<p><center><iframe width=\"480\" height=\"360\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\" src=\"http:\/\/www.youtube.com\/embed\/-loYJQsE3TY\"><\/iframe><\/center><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 17 Apr 2012 10:03:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:22:05 +0000","updated_by":1,"published_at":"Tue, 17 Apr 2012 10:03:00 +0000","published_by":1},{"id":36,"title":"Linux vs Windows comparison","slug":"linux-windows-comparison","markdown":"\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i2.wp.com\/4.bp.blogspot.com\/-6AZtjxIA7T4\/T5pVHechtVI\/AAAAAAAAABM\/aLap_UYPpA8\/s320\/windows_tux.jpg?resize=320%2C173)](https:\/\/i2.wp.com\/4.bp.blogspot.com\/-6AZtjxIA7T4\/T5pVHechtVI\/AAAAAAAAABM\/aLap_UYPpA8\/s1600\/windows_tux.jpg)<\/div>Well, I wrote the following article a year ago. A YEAR AGO! I\u2019m actually amazed that I wrote such a thing. Well, so\u2026 Here it goes\u2026\n\nLinux, as you all know, is a free-open source operating system. Inspire of having number of advantages over Windows, Linux is still free and of course open source. Now, you might be thinking why I\u2019m using terms \u2018free\u2019 and \u2018open\u2019 source separately. \u2018Free\u2019 means that Linux is freely available and downloadable from internet. Some of the vendors like \u2018Ubuntu\u2019 offer CDs loaded with Linux absolutely free of cost. Open source means that if you know kernel programming and mechanics of operating system, then you can actually tweak Linux and also program Linux from a scratch. That\u2019s open source and Linux provides that flexibility. Unlike Linux, Windows is not open source. It does not allow user to modify its kernel or components. This is one of the reasons why Linux is hot favourite among young developers.\n\n**<span style=\"font-size: large;\">What separates them?<\/span>**  \n The main difference between Linux and windows is their build. Practically, Windows is created only by Microsoft Corporation while Linux has been created and tweaked by entire world! Why entire world? Because, it\u2019s open source. If I have Linux and I find a bug, then I can tweak and remove that thing myself. I also tell the organization about the bug which I fixed. So, they verify it and tell their rest of the consumers about the fix. So, that\u2019s how it all works. With windows, it is not like that. I must appreciate Microsoft\u2019s efforts in improving windows because in new versions of windows \u2013 The Vista Series \u2013 they have put an improved version of error reporting like\u00a0Problems Report and Solutions. With that you can just submit the error report. You cannot tell them how to fix that even if you know. Of course, you cannot fix grave errors in windows even if you think because it\u2019s not open source!\n\n<div style=\"clear: both; text-align: center;\"><\/div>[  \n](http:\/\/2.bp.blogspot.com\/-mOMRW4ouf7Y\/T5pWKF5b9HI\/AAAAAAAAABs\/CHSczwEImQU\/s1600\/debian.png)  \n**<span style=\"font-size: large;\">Who\u2019s better on performance?<\/span>**  \n[![](https:\/\/i2.wp.com\/4.bp.blogspot.com\/-ofHO8h4jRIo\/T5pXXKotT0I\/AAAAAAAAAB8\/NoBlGPqy6zM\/s1600\/linuxdistros.jpg?w=700)](https:\/\/i2.wp.com\/4.bp.blogspot.com\/-ofHO8h4jRIo\/T5pXXKotT0I\/AAAAAAAAAB8\/NoBlGPqy6zM\/s1600\/linuxdistros.jpg)Practically, on considering stable releases of Linux offered by several organizations like Ubuntu, Debian, Fedora, etc., no doubt Linux outperforms Windows. Windows has always been a bulky operating system taking several minutes to boot up, then a minute to get stable and so on. While with Linux, running on an average PC, booting up is a child\u2019s play for Linux, 10 \u2013 15 seconds is a limit. Even if \u2018background processing\u2019 indicator on mouse cursor is on, you can still get your stuff by clicking. Exploring your computer is just with double-click of a button. You\u2019ll never find this speed on Windows!  \n Also, shutting down your PC is a matter of 5-6 seconds for Linux. And what\u2019s more? Linux works on a 1 GHz PC with 128 MB RAM and even 10 GB Hard Drive!\n\n**<span style=\"font-size: large;\">What\u2019s your colour today?<\/span>**  \n Unlike Windows, Linux provides a lot of customization features. It is so flexible that you can customize almost everything. From changing your wallpaper to behaviour and animation of minimizing\/maximizing a window, you can change and tweak everything. Linux also provides various desktop effects \u2013 some of which even require a 3D card \u2013 which are super cool.  \n For Linux, wallpapers are billions, themes are millions, and yet there are thousands of programs that help even more to customize Linux. Also various types of desktop environment are available on Linux like GNOME (Most common with cool GUI), KDE (High end graphical interface), and Xfce (Fastest of all). These desktop environments not only handle the look and feel of the Linux environment but also provide you with lot of other flexibilities. Instead of trying out new desktop environments, I prefer to GNOME and its cool Compiz-Fusion Desktop effects which tweak almost all aspects of the operating system. In customization, Linux easily overcomes Windows.\n\n**<span style=\"font-size: large;\">Applications?<\/span>**  \n This is where Linux gets outperformed. Almost 91% of the applications available in the market run on windows. 5% work on Mac OS X, 3% of it in Linux and the rest are for other OS. Windows has its roots wide-spread all over the world because it was the first OS to offer the idea of multi-tasking and a lot more new features that were not even thought of at that time. While Linux \u2013 which is not that old but still is not as old as windows \u2013 has relatively very less number of applications available worldwide. Linux is rapidly getting wide spread in the world of computing because it is a lot more flexible and offers a lot more possibilities than Windows. Linux offers its developers great flexibility of working with applications. Previously, Linux was called non-user friendly because it was difficult to operate but now that it has been tweaked and is available in several forms, Linux has removed this badge of dishonour from itself.  \n Organizations like Ubuntu etc. have made Linux user friendly and easy to use operating system that provides its users with number of options which makes it easy to use.\n\n**<span style=\"font-size: large;\">Running Windows on Linux?<\/span>**  \n[![](https:\/\/i2.wp.com\/3.bp.blogspot.com\/-6VLcWKZuVcE\/T5pX_QdaB4I\/AAAAAAAAACE\/yPpkNoerhOQ\/s1600\/wine-windows.png?w=700)](https:\/\/i2.wp.com\/3.bp.blogspot.com\/-6VLcWKZuVcE\/T5pX_QdaB4I\/AAAAAAAAACE\/yPpkNoerhOQ\/s1600\/wine-windows.png)Also, now you can run Windows applications on Linux. With \u2018Wine\u2019 \u2013 an application for Linux \u2013 you can run windows applications and programs on Linux. Wine uses its DLL library to run Windows applications. If you dual boot Windows and Linux, then it\u2019ll be better. Wine uses Windows library and its own library in combination to run the application smoothly. Wine also has configuration screen where, you can select in which mode Wine should work. There are several modes available based on Windows OS Released. On my Linux, there are following modes:  \n \u2022 Windows Vista  \n \u2022\u00a0Windows XP  \n \u2022\u00a0Windows Me  \n \u2022 Windows 98  \n \u2022 Windows 3.0  \n And several others including server editions which I cannot recall right now.  \n Each of these modes create a separate environment for the application to run smoothly. Here, environment does not mean that your Linux look would change. You won\u2019t even notice that! So, are you thinking of running Windows Games on your Linux? Don\u2019t dream that! It won\u2019t run. Games are called as hefty softwares that require large amount of resources and also they have some of functions which are accessible to them in Windows but not in Linux. However, these people are trying a lot to make each and every application of Windows run on Linux. Good luck to them!\n\n**<span style=\"font-size: large;\">Where\u2019s the security?<\/span>**  \n Linux has security from viruses. Don\u2019t think that an antivirus system has been integrated to it. It isn\u2019t in this case. Because, Linux does not support viruses! Yes, Viruses don\u2019t run on Linux because they find it hard to harass you while you are logged in. Surprised? Ok, let\u2019s be frank. There\u2019s one virus on Linux and that\u2019s \u2013 Bliss. But just think of it. Linux has just one virus to support while Windows is entirely surrounded by viruses. Viruses for Windows \u2013 maybe billions \u2013 Viruses for Linux \u2013 Just One!! And that too is not going to do any big thing to your computer! So, here we have arrived to the point where we must say that Linux is too stout to respond to viruses. Consider a case where a burglar has entered a bank, he shows a gun, fires in the air and expects people to panic. But what happens? People ignore him! He shoots at the cashier but finds that his guns do not work against him. Bullets just fall down touching the cashier! This is the case when virus enters into Linux. Virus would try hard to do harm but Linux won\u2019t execute any operations that virus has told. Virus will try to infiltrate your files but that also would not work because unlike windows, Linux is not having applications chained.\n\n**<span style=\"font-size: large;\">Little things that matter!<\/span>**  \n Yet, there are a lot of little things offered by Linux that provide great advantage over Windows. First of all, Linux has a \u2018Force Quit\u2019 feature. If you find any misbehaving application (Though you\u2019ll rarely find) that\u2019s taking too long to respond, you can use Force Quit. Force quit is a utility provided in Linux which immediately exits a non-responding application. So, you won\u2019t need to wait minutes for a non-responding application to end. Sometimes, you\u2019ll find that in windows, when an application does not respond, then your computer will also not respond. This happens because in windows, all applications are chained. When running applications are chained, they all get affected if one of them stops responding. This is not the case in Linux, In Linux, applications work independently, meaning, that they are not chained. So, if any damn application gets \u2018hanged\u2019 your computer and other applications will continue working in full swing. Second advantage in Linux, that it has an in-built archive manager. With archive manager, you can handle multiple types of archives without installing a separate program for it. Linux also has a highly advanced calculator with three extended modes which are\u00a0\u2018Advanced\u2019, \u2018Financial\u2019\u00a0and\u00a0\u2018Programming\u2019. Linux also comes with Password and Encryption key manager which allows you to create passwords for applications and folders. It also allows you to create encryption keys whose strength ranges from 2048 bits to 4096 bits. With that, you can encrypt archives, files and folders to ensure that they are safe. Many Linux editions also allow you to change the way login screen looks and behaviour of the boot loader and its appearance. Linux also allows you to scale CPU usage. Meaning, you can set how much of the CPU power Linux should use. This helps you a lot in saving power. Suppose, you are reading an eBook on your Linux PC during a journey, if you were using windows, and even in power saver mode, you\u2019ll drain a lot of battery. But in Linux, it dims the display automatically. Now, reading an eBook does not require processing power. So, you can set the value to least mode. In this way, Linux will stop most of background processes which drain battery and will ensure that the battery is saved. Now, as were discussing about battery, it reminds me of an amazing feature that Linux provides. Linux has an in-built system that provides detailed information on the battery installed on your laptop. It includes details like your battery model, version, status, and vendor. These details are common in windows. Excluding these, it also tells you details about battery technology, serial number, current charge capacity of battery(in %), current charge(in watt-hour), last full charge(in watt-hour) and design charge(in watt-hour). It also provides you with information based on graphs too! I\u2019m sure that these features are not available on windows.  \n If you refer to new versions of windows, in Vista line, you\u2019ll find that they are having preview panes. The moment, you select an item, its preview (if available) is shown in the preview pane. Previews are available for pictures, music, videos and some other types. Suppose, I click a music file. If my computer or rather Windows media player has that plugin, then it will show a mini media player in the preview pane with a play button. If I click play button, it will search the plugin in Windows Media library and then it will play the file. On my VAIO BZ, windows music preview does not work correctly. For the first file I play on starting the computer, it takes a lot of time in loading the preview pane. After that it plays the file. Now, Ubuntu Linux 9.04 has something new to offer. Now, unlike windows, Ubuntu has a \u2018Movie Player\u2019 as default player. While exploring my computer, if I stumble upon a music file which I want to preview, I simply hover my mouse over it and immediately, it\u2019ll start playing the file. But unfortunately in this technique, pausing is not possible. If I move down my mouse from that music file, the file immediately stops playing. Ubuntu also does not offer a seek bar!  \n Also, there\u2019s one thing where Linux surrenders. It\u2019s Silverlight Experience! At Windows, you have Silverlight integrated in Windows Internet Explorer as well as many other web browsers like Firefox. But, in Ubuntu (or I assume in most of Linux operating systems) Silverlight isn\u2019t available. Yes, if you do visit a website featuring Silverlight, it\u2019ll tell you to download an alternative version of Silverlight as moonlight but even after downloading moon light, I can\u2019t get Silverlight experience \u2013 It still says that Silverlight is missing.  \n So, what do you learn from this? Most of all high-end Microsoft technologies aren\u2019t shared by Microsoft.  \n For all Photoshop editors, Linux is a disappointment because there\u2019s probably no Linux edition for adobe Photoshop.  \n Now, Linux also has an in built application called Disk Analyser. It is an application which tells you what folder has been eating, rather reserving your disk space. It is very useful application in that way because it lets you know what to delete to free your space. It not only tells you about your file system but tells you the \u2018free space sharing\u2019 on any other device that you connect. In this case, windows gets defeated. It\u2019s not having an in built application for this, so, you\u2019ll have to wander-on-the-web to get one!  \n I\u2019ve heard that Linux produces outstanding graphics but I haven\u2019t seen it in action and therefore I tell you about that so sure. But I think that it would be able to produce out-standing graphics because it\u2019s swift. Windows and other operating system themselves allocate very high memory and therefore they have relatively less to dedicate for graphics. Whereas in Linux, this case is totally different. Linux itself uses very less of your RAM as well as your hard drive. So, it can dedicate more graphics to your games and all that stuff. This is just a guess and I am not so sure about it. Built-in games of Linux are very standard \u2013 sort of only two-dimensional and therefore, you cannot judge its actual capacity. However, some of real 3D games are available but aren\u2019t satisfactory because they aren\u2019t developed that passionately. I purely do not mean to discourage or hurt the Linux game developers but despite of such nice capabilities of Linux to handle the resources you won\u2019t get any game on Linux that can compete the one on windows. This might be because the real \u2018expert\u2019 game developers are looking for money and hence they are moving towards development of games for windows as most of the high-end games in windows aren\u2019t free. Now, you might be wondering about 3D modelling software like Maya? As of now, I haven\u2019t encountered any such software but I am still looking for it. There must be some sort of software available on Linux. Don\u2019t worry, you\u2019ll obviously find one.\n\n**<span style=\"font-size: large;\">Speed with appearance?<\/span>**  \n This is very easy question to answer \u2013\u00a0You compromise speed, you\u2019ll get speed.\u00a0But after you use Linux, you\u2019ll probably disagree with this. Even after using themes and effects, you\u2019ll observe that your working speed has been little affected. Shocked? I was too but I soon discovered this myself on my computer. Of course, you might suffer from a sort of decrease in speed at first but this decrease in speed will be rather unnoticed by you unless you are using a computer that\u2019s got a configuration which is just on the minimum system requirements of Linux. Now, if you know this, then there are different window managers in Linux. Most popular and used is GNOME. Still, there are two other types called KDE and Xfce. GNOME is for a computer user who wants both speed and appearance. KDE is for one who wants very cool appearance, mostly for high-fi users. Xfce us mostly used by users who have very low system requirements and want speed. Now, this is not always true because some users who just don\u2019t care about appearance even after having a high-end system use Xfce. Another one, which is not a window manager (Though most of users think it is), is Compiz Fusion. It accelerates the appearance of your system and allows you to perform a lot more tweaks like changing the open\/close\/minimize\/maximize animations, window borders, various effects and things like modifying the application switcher method. All these will be better understood by you if you use hands on approach and get at least a Live CD of Ubuntu Linux. Different window managers provide their own options of modifying the appearance on Linux while on windows? You know, you\u2019ll have to stick to the like or not Windows own window manager. Now this is mostly attracting to users but if you consider old versions, then they weren\u2019t attractive. But now, considering Windows Vista and Windows 7, they seem to be cool. Still, you don\u2019t get options of really \u2018modifying\u2019 them. Don\u2019t tell me about the \u2018themes\u2019 option. That\u2019s of no use. If you want to change desktop background and other sorts of things without affecting the real appearance, then it\u2019s better to do it yourself rather than depending on it.  \n Still thinking of Windows? Think again! Because Linux is used by huge servers and supercomputers. It\u2019s their choice because they rely on the usability, flexibility and resource management of Linux. Shouldn\u2019t you rely on it?\n\n\n","html":"<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i2.wp.com\/4.bp.blogspot.com\/-6AZtjxIA7T4\/T5pVHechtVI\/AAAAAAAAABM\/aLap_UYPpA8\/s1600\/windows_tux.jpg\"><img src=\"https:\/\/i2.wp.com\/4.bp.blogspot.com\/-6AZtjxIA7T4\/T5pVHechtVI\/AAAAAAAAABM\/aLap_UYPpA8\/s320\/windows_tux.jpg?resize=320%2C173\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<p>Well, I wrote the following article a year ago. A YEAR AGO! I&#8217;m actually amazed that I wrote such a thing. Well, so&#8230; Here it goes&#8230;<\/p>\n<p>Linux, as you all know, is a free-open source operating system. Inspire of having number of advantages over Windows, Linux is still free and of course open source. Now, you might be thinking why I&#8217;m using terms &#8216;free&#8217; and &#8216;open&#8217; source separately. &#8216;Free&#8217; means that Linux is freely available and downloadable from internet. Some of the vendors like &#8216;Ubuntu&#8217; offer CDs loaded with Linux absolutely free of cost. Open source means that if you know kernel programming and mechanics of operating system, then you can actually tweak Linux and also program Linux from a scratch. That&#8217;s open source and Linux provides that flexibility. Unlike Linux, Windows is not open source. It does not allow user to modify its kernel or components. This is one of the reasons why Linux is hot favourite among young developers.<\/p>\n<p><strong><span style=\"font-size: large;\">What separates them?<\/span><\/strong><br \/>\nThe main difference between Linux and windows is their build. Practically, Windows is created only by Microsoft Corporation while Linux has been created and tweaked by entire world! Why entire world? Because, it&#8217;s open source. If I have Linux and I find a bug, then I can tweak and remove that thing myself. I also tell the organization about the bug which I fixed. So, they verify it and tell their rest of the consumers about the fix. So, that&#8217;s how it all works. With windows, it is not like that. I must appreciate Microsoft\u2019s efforts in improving windows because in new versions of windows \u2013 The Vista Series \u2013 they have put an improved version of error reporting like\u00a0Problems Report and Solutions. With that you can just submit the error report. You cannot tell them how to fix that even if you know. Of course, you cannot fix grave errors in windows even if you think because it&#8217;s not open source!<\/p>\n<div style=\"clear: both; text-align: center;\"><\/div>\n<p><a style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\" href=\"http:\/\/2.bp.blogspot.com\/-mOMRW4ouf7Y\/T5pWKF5b9HI\/AAAAAAAAABs\/CHSczwEImQU\/s1600\/debian.png\"><br \/>\n<\/a><br \/>\n<strong><span style=\"font-size: large;\">Who&#8217;s better on performance?<\/span><\/strong><br \/>\n<a style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\" href=\"https:\/\/i2.wp.com\/4.bp.blogspot.com\/-ofHO8h4jRIo\/T5pXXKotT0I\/AAAAAAAAAB8\/NoBlGPqy6zM\/s1600\/linuxdistros.jpg\"><img src=\"https:\/\/i2.wp.com\/4.bp.blogspot.com\/-ofHO8h4jRIo\/T5pXXKotT0I\/AAAAAAAAAB8\/NoBlGPqy6zM\/s1600\/linuxdistros.jpg?w=700\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a>Practically, on considering stable releases of Linux offered by several organizations like Ubuntu, Debian, Fedora, etc., no doubt Linux outperforms Windows. Windows has always been a bulky operating system taking several minutes to boot up, then a minute to get stable and so on. While with Linux, running on an average PC, booting up is a child&#8217;s play for Linux, 10 \u2013 15 seconds is a limit. Even if &#8216;background processing&#8217; indicator on mouse cursor is on, you can still get your stuff by clicking. Exploring your computer is just with double-click of a button. You&#8217;ll never find this speed on Windows!<br \/>\nAlso, shutting down your PC is a matter of 5-6 seconds for Linux. And what&#8217;s more? Linux works on a 1 GHz PC with 128 MB RAM and even 10 GB Hard Drive!<\/p>\n<p><strong><span style=\"font-size: large;\">What&#8217;s your colour today?<\/span><\/strong><br \/>\nUnlike Windows, Linux provides a lot of customization features. It is so flexible that you can customize almost everything. From changing your wallpaper to behaviour and animation of minimizing\/maximizing a window, you can change and tweak everything. Linux also provides various desktop effects \u2013 some of which even require a 3D card \u2013 which are super cool.<br \/>\nFor Linux, wallpapers are billions, themes are millions, and yet there are thousands of programs that help even more to customize Linux. Also various types of desktop environment are available on Linux like GNOME (Most common with cool GUI), KDE (High end graphical interface), and Xfce (Fastest of all). These desktop environments not only handle the look and feel of the Linux environment but also provide you with lot of other flexibilities. Instead of trying out new desktop environments, I prefer to GNOME and its cool Compiz-Fusion Desktop effects which tweak almost all aspects of the operating system. In customization, Linux easily overcomes Windows.<\/p>\n<p><strong><span style=\"font-size: large;\">Applications?<\/span><\/strong><br \/>\nThis is where Linux gets outperformed. Almost 91% of the applications available in the market run on windows. 5% work on Mac OS X, 3% of it in Linux and the rest are for other OS. Windows has its roots wide-spread all over the world because it was the first OS to offer the idea of multi-tasking and a lot more new features that were not even thought of at that time. While Linux \u2013 which is not that old but still is not as old as windows \u2013 has relatively very less number of applications available worldwide. Linux is rapidly getting wide spread in the world of computing because it is a lot more flexible and offers a lot more possibilities than Windows. Linux offers its developers great flexibility of working with applications. Previously, Linux was called non-user friendly because it was difficult to operate but now that it has been tweaked and is available in several forms, Linux has removed this badge of dishonour from itself.<br \/>\nOrganizations like Ubuntu etc. have made Linux user friendly and easy to use operating system that provides its users with number of options which makes it easy to use.<\/p>\n<p><strong><span style=\"font-size: large;\">Running Windows on Linux?<\/span><\/strong><br \/>\n<a style=\"clear: right; float: right; margin-bottom: 1em; margin-left: 1em;\" href=\"https:\/\/i2.wp.com\/3.bp.blogspot.com\/-6VLcWKZuVcE\/T5pX_QdaB4I\/AAAAAAAAACE\/yPpkNoerhOQ\/s1600\/wine-windows.png\"><img src=\"https:\/\/i2.wp.com\/3.bp.blogspot.com\/-6VLcWKZuVcE\/T5pX_QdaB4I\/AAAAAAAAACE\/yPpkNoerhOQ\/s1600\/wine-windows.png?w=700\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a>Also, now you can run Windows applications on Linux. With &#8216;Wine&#8217; \u2013 an application for Linux \u2013 you can run windows applications and programs on Linux. Wine uses its DLL library to run Windows applications. If you dual boot Windows and Linux, then it&#8217;ll be better. Wine uses Windows library and its own library in combination to run the application smoothly. Wine also has configuration screen where, you can select in which mode Wine should work. There are several modes available based on Windows OS Released. On my Linux, there are following modes:<br \/>\n\u2022 Windows Vista<br \/>\n\u2022\u00a0Windows XP<br \/>\n\u2022\u00a0Windows Me<br \/>\n\u2022 Windows 98<br \/>\n\u2022 Windows 3.0<br \/>\nAnd several others including server editions which I cannot recall right now.<br \/>\nEach of these modes create a separate environment for the application to run smoothly. Here, environment does not mean that your Linux look would change. You won&#8217;t even notice that! So, are you thinking of running Windows Games on your Linux? Don&#8217;t dream that! It won&#8217;t run. Games are called as hefty softwares that require large amount of resources and also they have some of functions which are accessible to them in Windows but not in Linux. However, these people are trying a lot to make each and every application of Windows run on Linux. Good luck to them!<\/p>\n<p><strong><span style=\"font-size: large;\">Where&#8217;s the security?<\/span><\/strong><br \/>\nLinux has security from viruses. Don&#8217;t think that an antivirus system has been integrated to it. It isn&#8217;t in this case. Because, Linux does not support viruses! Yes, Viruses don&#8217;t run on Linux because they find it hard to harass you while you are logged in. Surprised? Ok, let\u2019s be frank. There&#8217;s one virus on Linux and that&#8217;s \u2013 Bliss. But just think of it. Linux has just one virus to support while Windows is entirely surrounded by viruses. Viruses for Windows \u2013 maybe billions \u2013 Viruses for Linux \u2013 Just One!! And that too is not going to do any big thing to your computer! So, here we have arrived to the point where we must say that Linux is too stout to respond to viruses. Consider a case where a burglar has entered a bank, he shows a gun, fires in the air and expects people to panic. But what happens? People ignore him! He shoots at the cashier but finds that his guns do not work against him. Bullets just fall down touching the cashier! This is the case when virus enters into Linux. Virus would try hard to do harm but Linux won&#8217;t execute any operations that virus has told. Virus will try to infiltrate your files but that also would not work because unlike windows, Linux is not having applications chained.<\/p>\n<p><strong><span style=\"font-size: large;\">Little things that matter!<\/span><\/strong><br \/>\nYet, there are a lot of little things offered by Linux that provide great advantage over Windows. First of all, Linux has a &#8216;Force Quit&#8217; feature. If you find any misbehaving application (Though you&#8217;ll rarely find) that&#8217;s taking too long to respond, you can use Force Quit. Force quit is a utility provided in Linux which immediately exits a non-responding application. So, you won&#8217;t need to wait minutes for a non-responding application to end. Sometimes, you&#8217;ll find that in windows, when an application does not respond, then your computer will also not respond. This happens because in windows, all applications are chained. When running applications are chained, they all get affected if one of them stops responding. This is not the case in Linux, In Linux, applications work independently, meaning, that they are not chained. So, if any damn application gets &#8216;hanged&#8217; your computer and other applications will continue working in full swing. Second advantage in Linux, that it has an in-built archive manager. With archive manager, you can handle multiple types of archives without installing a separate program for it. Linux also has a highly advanced calculator with three extended modes which are\u00a0&#8216;Advanced&#8217;, &#8216;Financial&#8217;\u00a0and\u00a0&#8216;Programming&#8217;. Linux also comes with Password and Encryption key manager which allows you to create passwords for applications and folders. It also allows you to create encryption keys whose strength ranges from 2048 bits to 4096 bits. With that, you can encrypt archives, files and folders to ensure that they are safe. Many Linux editions also allow you to change the way login screen looks and behaviour of the boot loader and its appearance. Linux also allows you to scale CPU usage. Meaning, you can set how much of the CPU power Linux should use. This helps you a lot in saving power. Suppose, you are reading an eBook on your Linux PC during a journey, if you were using windows, and even in power saver mode, you&#8217;ll drain a lot of battery. But in Linux, it dims the display automatically. Now, reading an eBook does not require processing power. So, you can set the value to least mode. In this way, Linux will stop most of background processes which drain battery and will ensure that the battery is saved. Now, as were discussing about battery, it reminds me of an amazing feature that Linux provides. Linux has an in-built system that provides detailed information on the battery installed on your laptop. It includes details like your battery model, version, status, and vendor. These details are common in windows. Excluding these, it also tells you details about battery technology, serial number, current charge capacity of battery(in %), current charge(in watt-hour), last full charge(in watt-hour) and design charge(in watt-hour). It also provides you with information based on graphs too! I&#8217;m sure that these features are not available on windows.<br \/>\nIf you refer to new versions of windows, in Vista line, you\u2019ll find that they are having preview panes. The moment, you select an item, its preview (if available) is shown in the preview pane. Previews are available for pictures, music, videos and some other types. Suppose, I click a music file. If my computer or rather Windows media player has that plugin, then it will show a mini media player in the preview pane with a play button. If I click play button, it will search the plugin in Windows Media library and then it will play the file. On my VAIO BZ, windows music preview does not work correctly. For the first file I play on starting the computer, it takes a lot of time in loading the preview pane. After that it plays the file. Now, Ubuntu Linux 9.04 has something new to offer. Now, unlike windows, Ubuntu has a &#8216;Movie Player&#8217; as default player. While exploring my computer, if I stumble upon a music file which I want to preview, I simply hover my mouse over it and immediately, it&#8217;ll start playing the file. But unfortunately in this technique, pausing is not possible. If I move down my mouse from that music file, the file immediately stops playing. Ubuntu also does not offer a seek bar!<br \/>\nAlso, there&#8217;s one thing where Linux surrenders. It&#8217;s Silverlight Experience! At Windows, you have Silverlight integrated in Windows Internet Explorer as well as many other web browsers like Firefox. But, in Ubuntu (or I assume in most of Linux operating systems) Silverlight isn&#8217;t available. Yes, if you do visit a website featuring Silverlight, it&#8217;ll tell you to download an alternative version of Silverlight as moonlight but even after downloading moon light, I can&#8217;t get Silverlight experience \u2013 It still says that Silverlight is missing.<br \/>\nSo, what do you learn from this? Most of all high-end Microsoft technologies aren&#8217;t shared by Microsoft.<br \/>\nFor all Photoshop editors, Linux is a disappointment because there&#8217;s probably no Linux edition for adobe Photoshop.<br \/>\nNow, Linux also has an in built application called Disk Analyser. It is an application which tells you what folder has been eating, rather reserving your disk space. It is very useful application in that way because it lets you know what to delete to free your space. It not only tells you about your file system but tells you the &#8216;free space sharing&#8217; on any other device that you connect. In this case, windows gets defeated. It&#8217;s not having an in built application for this, so, you&#8217;ll have to wander-on-the-web to get one!<br \/>\nI&#8217;ve heard that Linux produces outstanding graphics but I haven&#8217;t seen it in action and therefore I tell you about that so sure. But I think that it would be able to produce out-standing graphics because it&#8217;s swift. Windows and other operating system themselves allocate very high memory and therefore they have relatively less to dedicate for graphics. Whereas in Linux, this case is totally different. Linux itself uses very less of your RAM as well as your hard drive. So, it can dedicate more graphics to your games and all that stuff. This is just a guess and I am not so sure about it. Built-in games of Linux are very standard \u2013 sort of only two-dimensional and therefore, you cannot judge its actual capacity. However, some of real 3D games are available but aren&#8217;t satisfactory because they aren&#8217;t developed that passionately. I purely do not mean to discourage or hurt the Linux game developers but despite of such nice capabilities of Linux to handle the resources you won&#8217;t get any game on Linux that can compete the one on windows. This might be because the real &#8216;expert&#8217; game developers are looking for money and hence they are moving towards development of games for windows as most of the high-end games in windows aren&#8217;t free. Now, you might be wondering about 3D modelling software like Maya? As of now, I haven&#8217;t encountered any such software but I am still looking for it. There must be some sort of software available on Linux. Don&#8217;t worry, you&#8217;ll obviously find one.<\/p>\n<p><strong><span style=\"font-size: large;\">Speed with appearance?<\/span><\/strong><br \/>\nThis is very easy question to answer \u2013\u00a0You compromise speed, you&#8217;ll get speed.\u00a0But after you use Linux, you&#8217;ll probably disagree with this. Even after using themes and effects, you&#8217;ll observe that your working speed has been little affected. Shocked? I was too but I soon discovered this myself on my computer. Of course, you might suffer from a sort of decrease in speed at first but this decrease in speed will be rather unnoticed by you unless you are using a computer that&#8217;s got a configuration which is just on the minimum system requirements of Linux. Now, if you know this, then there are different window managers in Linux. Most popular and used is GNOME. Still, there are two other types called KDE and Xfce. GNOME is for a computer user who wants both speed and appearance. KDE is for one who wants very cool appearance, mostly for high-fi users. Xfce us mostly used by users who have very low system requirements and want speed. Now, this is not always true because some users who just don&#8217;t care about appearance even after having a high-end system use Xfce. Another one, which is not a window manager (Though most of users think it is), is Compiz Fusion. It accelerates the appearance of your system and allows you to perform a lot more tweaks like changing the open\/close\/minimize\/maximize animations, window borders, various effects and things like modifying the application switcher method. All these will be better understood by you if you use hands on approach and get at least a Live CD of Ubuntu Linux. Different window managers provide their own options of modifying the appearance on Linux while on windows? You know, you&#8217;ll have to stick to the like or not Windows own window manager. Now this is mostly attracting to users but if you consider old versions, then they weren&#8217;t attractive. But now, considering Windows Vista and Windows 7, they seem to be cool. Still, you don&#8217;t get options of really &#8216;modifying&#8217; them. Don&#8217;t tell me about the &#8216;themes&#8217; option. That&#8217;s of no use. If you want to change desktop background and other sorts of things without affecting the real appearance, then it&#8217;s better to do it yourself rather than depending on it.<br \/>\nStill thinking of Windows? Think again! Because Linux is used by huge servers and supercomputers. It&#8217;s their choice because they rely on the usability, flexibility and resource management of Linux. Shouldn&#8217;t you rely on it?<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 27 Apr 2012 01:11:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:37:30 +0000","updated_by":1,"published_at":"Fri, 27 Apr 2012 01:11:00 +0000","published_by":1},{"id":35,"title":"API for recognizing gestures in Android","slug":"api-for-recognizing-gestures-in-android","markdown":"\nI have been busy lately. For most of the time, I have been working on gesture API for android. Android already has a built in gesture API but to actually use it, you have to apply mathematics which makes it a bit comlicated for a normal Android developer. AGAPI (Accelerometer Gesture API) adds a layer of abstraction over the existing gesture API to make it easier to recognize gestures. Using AGAPI is very simple. Implement the ShakeListener or any gesture listener class. Register your listener to ShakeManager or corresponding gesture manager instance. Write your code in OnShake or in corresponding event methods and BAM! Your gesture rich application is ready! At this stage of development, the Shake gesture is ready to use. You can get the API [here](http:\/\/code.google.com\/p\/ag-api\/downloads\/list).\n\nNote: This API is licensed under Apache License 2.0.\n\n\n","html":"<p>I have been busy lately. For most of the time, I have been working on gesture API for android. Android already has a built in gesture API but to actually use it, you have to apply mathematics which makes it a bit comlicated for a normal Android developer. AGAPI (Accelerometer Gesture API) adds a layer of abstraction over the existing gesture API to make it easier to recognize gestures. Using AGAPI is very simple. Implement the ShakeListener or any gesture listener class. Register your listener to ShakeManager or corresponding gesture manager instance. Write your code in OnShake or in corresponding event methods and BAM! Your gesture rich application is ready! At this stage of development, the Shake gesture is ready to use. You can get the API <a href=\"http:\/\/code.google.com\/p\/ag-api\/downloads\/list\" target=\"_blank\">here<\/a>.<\/p>\n<p>Note: This API is licensed under Apache License 2.0.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 27 Aug 2012 15:48:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:28:40 +0000","updated_by":1,"published_at":"Mon, 27 Aug 2012 15:48:00 +0000","published_by":1},{"id":34,"title":"Job Execution API v0.1b is now available!","slug":"job-execution-api-v0-1b-is-now-available","markdown":"\nI have been working on this for a while and now I have finally released it. This is 0.1 beta so it may contain bugs. However, I am working on it constantly and am trying to improve the API.\n\nJob Execution API is all about job execution in Java. You have jobs which comprise of tasks. Each task is a class implementing the ITask interface. For now, the task is not Async so tasks are executed sequentially, in linear pattern. I intend to provide basic tasks which you can use to create standard jobs. You can later extend these tasks and make your own.\n\nDownload the API here and let me know how it goes:  \n[https:\/\/www.box.com\/s\/6cnn93gkrjmuf39sp12d](https:\/\/www.box.com\/s\/6cnn93gkrjmuf39sp12d)\n\n\n","html":"<p>I have been working on this for a while and now I have finally released it. This is 0.1 beta so it may contain bugs. However, I am working on it constantly and am trying to improve the API.<\/p>\n<p>Job Execution API is all about job execution in Java. You have jobs which comprise of tasks. Each task is a class implementing the ITask interface. For now, the task is not Async so tasks are executed sequentially, in linear pattern. I intend to provide basic tasks which you can use to create standard jobs. You can later extend these tasks and make your own.<\/p>\n<p>Download the API here and let me know how it goes:<br \/><a href=\"https:\/\/www.box.com\/s\/6cnn93gkrjmuf39sp12d\">https:\/\/www.box.com\/s\/6cnn93gkrjmuf39sp12d<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 30 Dec 2012 19:09:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:26:46 +0000","updated_by":1,"published_at":"Sun, 30 Dec 2012 19:09:00 +0000","published_by":1},{"id":33,"title":"Pizza Ordering System source (Session 2)","slug":"pizza-ordering-system-source-session-2","markdown":"\nHey Guys! Thanks for watching my video. In case you haven\u2019t, the video is down below:  \n<iframe allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"http:\/\/www.youtube.com\/embed\/TnJdzucKbxw\" width=\"560\"><\/iframe>  \n<iframe allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"http:\/\/www.youtube.com\/embed\/MouRpkoLlmM\" width=\"560\"><\/iframe>\n\nI hope the video as helpful to you. Please comment below the video or this post for any suggestions\/queries. You can download the source code by following the link below:  \n[https:\/\/www.box.com\/s\/civvyq1d2wzz69xydqi4](https:\/\/www.box.com\/s\/civvyq1d2wzz69xydqi4)\n\n\n","html":"<p>Hey Guys! Thanks for watching my video. In case you haven&#8217;t, the video is down below:<br \/><iframe width=\"560\" height=\"315\" src=\"http:\/\/www.youtube.com\/embed\/TnJdzucKbxw\" frameborder=\"0\" allowfullscreen><\/iframe><br \/><iframe width=\"560\" height=\"315\" src=\"http:\/\/www.youtube.com\/embed\/MouRpkoLlmM\" frameborder=\"0\" allowfullscreen><\/iframe><\/p>\n<p>I hope the video as helpful to you. Please comment below the video or this post for any suggestions\/queries. You can download the source code by following the link below:<br \/><a href=\"https:\/\/www.box.com\/s\/civvyq1d2wzz69xydqi4\">https:\/\/www.box.com\/s\/civvyq1d2wzz69xydqi4<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 12 Jan 2013 22:45:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:28:10 +0000","updated_by":1,"published_at":"Sat, 12 Jan 2013 22:45:00 +0000","published_by":1},{"id":32,"title":"How to tell which Linux partition a directory is on?","slug":"how-to-tell-which-linux-partition-a-directory-is-on","markdown":"\nThis is really simple. First of all, you need to know the full path to that directory. \u201ccd\u201d into that directory and type:  \n`pwd<br><\/br>`  \n and it will display full path of the directory you are in. Remember that or even better, copy it to the clipboard. Type the following command to know the name of the partition:  \n`df<br><\/br>`  \n where replace with the full path of your directory. This in my case becomes \u201c`df \/data\/android\/sample`\u201c.\n\nIf you are already in the directory, you can type \u201c`df .`\u201d and it will give details about the partition you are currently on.\n\nThe output should be something like:\n\n<span style=\"background-color: #f9f9f9;\">Filesystem <\/span><span style=\"background-color: #f9f9f9;\">1024<\/span><span style=\"background-color: #f9f9f9;\">-blocks Free <\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span><span style=\"background-color: #f9f9f9;\">Used Iused <\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span><span style=\"background-color: #f9f9f9;\">Iused Mounted on <\/span><span style=\"background-color: #bf9000;\"><span style=\"font-weight: bold;\">\/<\/span>dev<span style=\"font-weight: bold;\">\/<\/span>hd4<\/span><span style=\"background-color: #f9f9f9;\">32768<\/span><span style=\"background-color: #f9f9f9;\">16016<\/span><span style=\"background-color: #f9f9f9;\">52<\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span><span style=\"background-color: #f9f9f9;\">2271<\/span><span style=\"background-color: #f9f9f9;\">14<\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">\/<\/span>\n\nThe highlighted part is the name of your partition.\n\n\n","html":"<p>This is really simple. First of all, you need to know the full path to that directory. &#8220;cd&#8221; into that directory and type:<br \/>\n<code>pwd<br \/>\n<\/code><br \/>\nand it will display full path of the directory you are in. Remember that or even better, copy it to the clipboard. Type the following command to know the name of the partition:<br \/>\n<code>df<br \/>\n<\/code><br \/>\nwhere replace with the full path of your directory. This in my case becomes &#8220;<code>df \/data\/android\/sample<\/code>&#8220;.<\/p>\n<p>If you are already in the directory, you can type &#8220;<code>df .<\/code>&#8221; and it will give details about the partition you are currently on.<\/p>\n<p>The output should be something like:<\/p>\n<pre style=\"background-image: none; border: 0px none white; font-family: monospace, monospace; font-size: 13px; line-height: 1.2em; padding: 0px; vertical-align: top;\"><span style=\"background-color: #f9f9f9;\">Filesystem    <\/span><span style=\"background-color: #f9f9f9;\">1024<\/span><span style=\"background-color: #f9f9f9;\">-blocks      Free <\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span><span style=\"background-color: #f9f9f9;\">Used    Iused <\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span><span style=\"background-color: #f9f9f9;\">Iused Mounted on\r\n <\/span><span style=\"background-color: #bf9000;\"><span style=\"font-weight: bold;\">\/<\/span>dev<span style=\"font-weight: bold;\">\/<\/span>hd4<\/span>            <span style=\"background-color: #f9f9f9;\">32768<\/span>     <span style=\"background-color: #f9f9f9;\">16016<\/span>   <span style=\"background-color: #f9f9f9;\">52<\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span>     <span style=\"background-color: #f9f9f9;\">2271<\/span>    <span style=\"background-color: #f9f9f9;\">14<\/span><span style=\"background-color: #f9f9f9; font-weight: bold;\">%<\/span> <span style=\"background-color: #f9f9f9; font-weight: bold;\">\/<\/span><\/pre>\n<p>The highlighted part is the name of your partition.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 13 Apr 2013 23:01:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:20:35 +0000","updated_by":1,"published_at":"Sat, 13 Apr 2013 23:01:00 +0000","published_by":1},{"id":31,"title":"Preventing class redefinition errors in C++","slug":"how-to-prevent-class-redefinition-errors-in-c","markdown":"\nI have just started learning C++ and have been getting a lot of these errors. I finally found a solution to this after spending some time on the internet.\n\nSay for instance you have a class called \u201cAnimal\u201d and g++ complains that this class is being redefined. Just add the following two lines at the very top of the Animal.h file:\n\n```\n#ifndef ANIMAL_H<br><\/br>\n#define ANIMAL_H```\n\nand add the following at the very bottom of the file:  \n```\n<br><\/br>\n#endif```\n\nSo, if your class is called SomeClass, then it will be SOMECLASS_H instead of ANIMAL_H. You should have these in every header file. It prevents that header file from being redefined more than once.\n\n\n","html":"<p>I have just started learning C++ and have been getting a lot of these errors. I finally found a solution to this after spending some time on the internet.<\/p>\n<p>Say for instance you have a class called &#8220;Animal&#8221; and g++ complains that this class is being redefined. Just add the following two lines at the very top of the Animal.h file:<\/p>\n<p><code>#ifndef ANIMAL_H<br \/>\n#define ANIMAL_H<\/code><\/p>\n<p>and add the following at the very bottom of the file:<br \/>\n<code><br \/>\n#endif<\/code><\/p>\n<p>So, if your class is called SomeClass, then it will be SOMECLASS_H instead of ANIMAL_H. You should have these in every header file. It prevents that header file from being redefined more than once.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 14 Apr 2013 01:48:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:49:00 +0000","updated_by":1,"published_at":"Sun, 14 Apr 2013 01:48:00 +0000","published_by":1},{"id":29,"title":"Changing MySQL username and password used by PHPMyAdmin","slug":"changing-mysql-username-and-password-used-by-phpmyadmin","markdown":"\nSo, I had to do this the other day and I got a bit confused. After poking around a bit, I finally found it. If you have xampp installed, go to the install directory. In my case, this directory is C:xampp which is the default directory. Locate the phpmyadmin folder and open it. Locate the config.inc.php file within the phpmyadmin folder. Open up the file. You need to make change in the following two lines:\n\n`$cfg['Servers'][$i]['user'] = '<b><u>root<\/u><\/b>';\u00a0`  \n`$cfg['Servers'][$i]['password'] = '<b><u>password<\/u><\/b>' `\n\nThe underlined parts are the ones that you\u2019ve got to change. This file contains a lot of admin stuff so make sure you look around a bit and understand what it does.\n\nIf you don\u2019t know what you are doing, ensure you take a backup of the file before changing things.\n\n\n","html":"<p>So, I had to do this the other day and I got a bit confused. After poking around a bit, I finally found it. If you have xampp installed, go to the install directory. In my case, this directory is C:xampp which is the default directory. Locate the phpmyadmin folder and open it. Locate the config.inc.php file within the phpmyadmin folder. Open up the file. You need to make change in the following two lines:<\/p>\n<p><code>$cfg['Servers'][$i]['user'] = '<b><u>root<\/u><\/b>';\u00a0<\/code><br \/>\n<code>$cfg['Servers'][$i]['password'] = '<b><u>password<\/u><\/b>' <\/code><\/p>\n<p>The underlined parts are the ones that you&#8217;ve got to change. This file contains a lot of admin stuff so make sure you look around a bit and understand what it does.<\/p>\n<p>If you don&#8217;t know what you are doing, ensure you take a backup of the file before changing things.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 20 Apr 2013 10:09:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:08:47 +0000","updated_by":1,"published_at":"Sat, 20 Apr 2013 10:09:00 +0000","published_by":1},{"id":28,"title":"Setting up C++ build system on Linux","slug":"compiling-c-easily-on-linux-easy-c-build-system","markdown":"\nI was developing some C++ stuff on my Linux virtual machine and wanted to create something that would make it easy for me to compile C++ code. So I wrote up some bash code and added it into .bashrc as a function. Here\u2019s the code:\n\nfunction cpprun(){ echo \"Checking if output directory exists...\" if [ ! -d \".\/output\" ]; then echo \"Creating output directory...\" mkdir \".\/output\" echo \"Output directory created.\" elif [ -f \".\/output\/cpprun.o\" ]; then echo \"Backing up previous output...\" mv .\/output\/cpprun.o .\/output\/cpprun.bak.o echo \"Backup finished.\" fi echo \"Running g++ compile...\" g++ -o .\/output\/cpprun.o *.cpp if [ -f \".\/output\/cpprun.o\" ]; then echo \"Compile finished.\" echo \"Running output...\" echo \"-----------------\" .\/output\/cpprun.o echo \"-----------------\" echo \"Run finished.\" else echo \"Compile failed.\" fi }\n\n<div><\/div><div>Add this to your .bashrc script and then you should be able to compile your C++ code by typing cpprun in the base directory.<\/div>\n","html":"<p>I was developing some C++ stuff on my Linux virtual machine and wanted to create something that would make it easy for me to compile C++ code. So I wrote up some bash code and added it into .bashrc as a function. Here&#8217;s the code:<\/p>\n<pre>function cpprun(){\r\necho \"Checking if output directory exists...\"\r\nif [ ! -d \".\/output\" ]; then\r\necho \"Creating output directory...\"\r\nmkdir \".\/output\"\r\necho \"Output directory created.\"\r\nelif [ -f \".\/output\/cpprun.o\" ]; then\r\necho \"Backing up previous output...\"\r\nmv .\/output\/cpprun.o .\/output\/cpprun.bak.o\r\necho \"Backup finished.\"\r\nfi\r\n\r\necho \"Running g++ compile...\"\r\ng++ -o .\/output\/cpprun.o *.cpp\r\nif [ -f \".\/output\/cpprun.o\" ]; then\r\necho \"Compile finished.\"\r\necho \"Running output...\"\r\necho \"-----------------\"\r\n.\/output\/cpprun.o\r\necho \"-----------------\"\r\necho \"Run finished.\"\r\nelse\r\necho \"Compile failed.\"\r\nfi\r\n}\r\n<\/pre>\n<div><\/div>\n<div>Add this to your .bashrc script and then you should be able to compile your C++ code by typing cpprun in the base directory.<\/div>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 01 May 2013 18:20:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:47:49 +0000","updated_by":1,"published_at":"Wed, 01 May 2013 18:20:00 +0000","published_by":1},{"id":27,"title":"Removing password expiry from your Linux\/Unix machine","slug":"removing-password-expiry-from-your-unix-machine-linux-and-solaris","markdown":"\nHere\u2019s a quick one. If you have a server and you do not want the password for a user to expire (as it can screw some things up while its active), you need to execute the following commands as **`root`**:  \n`passwd -x -1 `\n\nwhere is the username whose password expiry you wish to remove. For instance, in my case, if username is dm014, I executed:  \n`passwd -x -1 dm014`  \n`<br><\/br>` I have tested this and it works flawlessly on most Linux and Solaris operating systems.\n\n\n","html":"<p>Here&#8217;s a quick one. If you have a server and you do not want the password for a user to expire (as it can screw some things up while its active), you need to execute the following commands as <b><code>root<\/code><\/b>:<br \/>\n<code>passwd -x -1 <\/code><\/p>\n<p>where is the username whose password expiry you wish to remove. For instance, in my case, if username is dm014, I executed:<br \/>\n<code>passwd -x -1 dm014<\/code><br \/>\n<code><br \/>\n<\/code> I have tested this and it works flawlessly on most Linux and Solaris operating systems.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 02 May 2013 17:47:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:31:03 +0000","updated_by":1,"published_at":"Thu, 02 May 2013 17:47:00 +0000","published_by":1},{"id":26,"title":"Writing Code - Building a chat application in Java - Session 1","slug":"writing-code-building-a-chat-application-in-java-session-1","markdown":"\n<center><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"360\" src=\"http:\/\/www.youtube.com\/embed\/Q-wzRVgqM-M\" width=\"480\"><\/iframe><\/center>  \n Hey Guys! In this series, we will be building a chat application in java. I think this will be a great chance for you all guys out there wanting to do something hands on in Java.\n","html":"<p><center><iframe width=\"480\" height=\"360\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\" src=\"http:\/\/www.youtube.com\/embed\/Q-wzRVgqM-M\"><\/iframe><\/center><br \/>\nHey Guys! In this series, we will be building a chat application in java. I think this will be a great chance for you all guys out there wanting to do something hands on in Java.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 03 May 2013 18:27:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:30:43 +0000","updated_by":1,"published_at":"Fri, 03 May 2013 18:27:00 +0000","published_by":1},{"id":25,"title":"Checking memory (RAM) size on Solaris","slug":"checking-memory-ram-size-on-solaris","markdown":"\nIf you want to check how much RAM your Solaris machine has, here\u2019s how:  \n`\/usr\/sbin\/prtdiag -v | grep -i memory`\n\nYou can also try:  \n`\/usr\/sbin\/prtconf | grep -i memory`\n\n\n","html":"<p>If you want to check how much RAM your Solaris machine has, here&#8217;s how:<br \/>\n<code>\/usr\/sbin\/prtdiag -v | grep -i memory<\/code><\/p>\n<p>You can also try:<br \/>\n<code>\/usr\/sbin\/prtconf | grep -i memory<\/code><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 07 May 2013 18:10:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:12:52 +0000","updated_by":1,"published_at":"Tue, 07 May 2013 18:10:00 +0000","published_by":1},{"id":24,"title":"Managing password expiry for a user account (Unix\/Linux)","slug":"managing-password-expiry-for-a-user-account-unixlinux","markdown":"\nI find this very useful on several occasions so here we go. Login as root and execute the following:  \n`passwd -x -1 `\n\nSo if you want to remove password expiry from bob\u2019s account, type:  \n`passwd -x -1 bob`\n\nNotice that -1 parameter after -x represents number of days before the password expires. Since we do not want the password to expire, we have set it to -1. However, you can set it to any value you want. So, if you want your password to expire after 90 days (i.e. 3 months) for bob\u2019s account, type:  \n`passwd -x 90 bob`\n\n\n","html":"<p>I find this very useful on several occasions so here we go. Login as root and execute the following:<br \/>\n<code>passwd -x -1 <\/code><\/p>\n<p>So if you want to remove password expiry from bob&#8217;s account, type:<br \/>\n<code>passwd -x -1 bob<\/code><\/p>\n<p>Notice that -1 parameter after -x represents number of days before the password expires. Since we do not want the password to expire, we have set it to -1. However, you can set it to any value you want. So, if you want your password to expire after 90 days (i.e. 3 months) for bob&#8217;s account, type:<br \/>\n<code>passwd -x 90 bob<\/code><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 11 May 2013 20:41:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:20:00 +0000","updated_by":1,"published_at":"Sat, 11 May 2013 20:41:00 +0000","published_by":1},{"id":23,"title":"Here's to 100 years of IBM!","slug":"heres-to-100-years-of-ibm","markdown":"\n<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"360\" src=\"http:\/\/www.youtube.com\/embed\/39jtNUGgmd4\" width=\"640\"><\/iframe>\n\n\n","html":"<p><iframe width=\"640\" height=\"360\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\" src=\"http:\/\/www.youtube.com\/embed\/39jtNUGgmd4\"><\/iframe><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 15 May 2013 17:38:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:04:01 +0000","updated_by":1,"published_at":"Wed, 15 May 2013 17:38:00 +0000","published_by":1},{"id":21,"title":"Fixing \"The application was unable to start correctly (0xc000007b)\" error","slug":"the-application-was-unable-to-start-correctly-0xc000007b","markdown":"\nSo I found this error when I started Lotus Notes this morning. After looking through some forums, I found that I needed to re-install Microsoft Visual C++ 2010 (Redistributable) from [here](http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=5555).\n\nThis error is usually caused by C:\\Windows\\System32\\MSVCR100.dll being corrupted. Reinstalling VC++ \u00a0fixes it.\n\n\n","html":"<p>So I found this error when I started Lotus Notes this morning. After looking through some forums, I found that I needed to re-install Microsoft Visual C++ 2010 (Redistributable) from <a href=\"http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=5555\" target=\"_blank\">here<\/a>.<\/p>\n<p>This error is usually caused by C:\\Windows\\System32\\MSVCR100.dll being corrupted. Reinstalling VC++ \u00a0fixes it.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 16 Jun 2013 11:11:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:19:29 +0000","updated_by":1,"published_at":"Sun, 16 Jun 2013 11:11:00 +0000","published_by":1},{"id":20,"title":"Mounting ISO images on Linux via command line","slug":"mounting-iso-images-on-linux","markdown":"\nSo I learnt this today and decided to share it with you guys. If you have an iso image and want to mount it on Linux, simply execute the following command in terminal:\n\n`sudo mount -o loop mydisk.iso myfoldername`\n\nSo in my case, I had iso file called 2012photos.iso on my desktop and I wanted to mount it on folder called 2012pics which was on Desktop as well. I executed:\n\n`sudo mount -o loop \/home\/manthan\/Desktop\/2012photos.iso \/home\/manthan\/Desktop\/2012pics`\n\n\n","html":"<p>So I learnt this today and decided to share it with you guys. If you have an iso image and want to mount it on Linux, simply execute the following command in terminal:<\/p>\n<p><code>sudo mount -o loop mydisk.iso myfoldername<\/code><\/p>\n<p>So in my case, I had iso file called 2012photos.iso on my desktop and I wanted to mount it on folder called 2012pics which was on Desktop as well. I executed:<\/p>\n<p><code>sudo mount -o loop \/home\/manthan\/Desktop\/2012photos.iso \/home\/manthan\/Desktop\/2012pics<\/code><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 19 Jun 2013 14:00:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:18:47 +0000","updated_by":1,"published_at":"Wed, 19 Jun 2013 14:00:00 +0000","published_by":1},{"id":19,"title":"Copying symbolic links (Unix\/Linux)","slug":"copying-symbolic-links-unixlinux","markdown":"\nAt some point, you might come across a situation where you have some files and a bunch of symlinks (symbolic links) in a folder and you wish to copy the data that is pointed at by the symlinks and not just blank links. If you are using Redhat Enterprise Linux, this should automatically happen when you call the cp command. However, on UNIX platforms or Linux distributions, this is not the case by default. In those cases, you will have to issue the following command:\n\n`cp -L {file(s)} {destination folder}`  \n`<br><\/br>`The -L option in cp command forces it to follow (dereference) symlinks and copy the data that is pointed at by the symlink.\n\nIn case you do not want to follow symlinks, you can issue the following command instead:  \n`<br><\/br>``cp -P {file(s)} {destination folder}`<span style=\"font-family: monospace;\">\u00a0<\/span>\n\n\n","html":"<p>At some point, you might come across a situation where you have some files and a bunch of symlinks (symbolic links) in a folder and you wish to copy the data that is pointed at by the symlinks and not just blank links. If you are using Redhat Enterprise Linux, this should automatically happen when you call the cp command. However, on UNIX platforms or Linux distributions, this is not the case by default. In those cases, you will have to issue the following command:<br \/>\n<!--more--><\/p>\n<p><code>cp -L {file(s)} {destination folder}<\/code><br \/>\n<code><br \/>\n<\/code>The -L option in cp command forces it to follow (dereference) symlinks and copy the data that is pointed at by the symlink.<\/p>\n<p>In case you do not want to follow symlinks, you can issue the following command instead:<br \/>\n<code><br \/>\n<\/code><code>cp -P {file(s)} {destination folder}<\/code><span style=\"font-family: monospace;\">\u00a0<\/span><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 20 Jun 2013 21:25:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:18:18 +0000","updated_by":1,"published_at":"Thu, 20 Jun 2013 21:25:00 +0000","published_by":1},{"id":18,"title":"Controlling LED brightness using potentiometer and arduino","slug":"controlling-led-brightness-using-potentiometer-and-arduino","markdown":"\nI was doing experiments with my Arduino today and this was one of them. I have commented the code well so it should be easy to understand. Here you go:\n\n\/* SETUP Connect led to analog pin 10, shorter end to ground. Potentiometer setup: (From left to right) 1. +5V 2. Analog pin 0 (A0) 3. Ground *\/ \/\/Set LED pin to analog pin 10. const int led_pin = 10; \/\/Set potentio meter pin to analog pin 0 (A0). const int pot_met = 0; void setup(){ \/\/Set led_pin to be the output. pinMode(led_pin,OUTPUT); \/\/Initiate serial (optional). Serial.begin(9600); } void loop(){ \/\/Use the readPotentioMeterBrightness method to read brightness value. int sensor_val = readPotentioMeterBrightness(); \/\/Output the value to serial (optional). Serial.println(sensor_val); \/\/Set LED brightness to value that we acquired from potentiometer. analogWrite(led_pin,sensor_val); \/\/Pause for 250 ms. delay(250); } int readPotentioMeterBrightness(){ \/\/Read analog input and store value in integer variable val int val = analogRead(pot_met); \/\/Adjust the value to be in range 0 to 255. val = map(val,0,1023,0,255); \/\/Ensure value is between 0 and 255. val = constrain(val,0,255); \/\/Return value return val; }\n\n\u00a0\n\n\n","html":"<p>I was doing experiments with my Arduino today and this was one of them. I have commented the code well so it should be easy to understand. Here you go:<!--more--><\/p>\n<pre>\/*\r\nSETUP\r\nConnect led to analog pin 10, shorter end to ground.\r\nPotentiometer setup:\r\n(From left to right)\r\n1. +5V\r\n2. Analog pin 0 (A0)\r\n3. Ground\r\n*\/\r\n\r\n\/\/Set LED pin to analog pin 10.\r\nconst int led_pin = 10;\r\n\/\/Set potentio meter pin to analog pin 0 (A0).\r\nconst int pot_met = 0;\r\n\r\nvoid setup(){\r\n  \/\/Set led_pin to be the output.\r\n  pinMode(led_pin,OUTPUT);\r\n  \/\/Initiate serial (optional).\r\n  Serial.begin(9600);\r\n}\r\n\r\nvoid loop(){\r\n  \/\/Use the readPotentioMeterBrightness method to read brightness value.\r\n  int sensor_val = readPotentioMeterBrightness();\r\n  \/\/Output the value to serial (optional).\r\n  Serial.println(sensor_val);\r\n  \/\/Set LED brightness to value that we acquired from potentiometer.\r\n  analogWrite(led_pin,sensor_val);\r\n  \/\/Pause for 250 ms.\r\n  delay(250);\r\n}\r\n\r\nint readPotentioMeterBrightness(){\r\n  \/\/Read analog input and store value in integer variable val\r\n  int val = analogRead(pot_met);\r\n  \/\/Adjust the value to be in range 0 to 255.\r\n  val = map(val,0,1023,0,255);\r\n  \/\/Ensure value is between 0 and 255. \r\n  val = constrain(val,0,255);\r\n  \/\/Return value\r\n  return val;\r\n}<\/pre>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 26 Jun 2013 12:13:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:43:25 +0000","updated_by":1,"published_at":"Wed, 26 Jun 2013 12:13:00 +0000","published_by":1},{"id":17,"title":"Using find command to search for a particular directory in unix","slug":"using-find-command-to-search-for-a-particular-directory-in-unix","markdown":"\nI was looking for a particular directory today on my computer and I didn\u2019t know how to do it. After spending some time searching for it, I found the following command:\n\n`find [parent directory] -type d -name [directory name]`\n\nSo in my case:\n\n`find \/home\/vader -type d -name wrapper_scripts`\n\nOptionally you can also use:\n\n`find \/home\/vader -type d | grep wrapper`\n\n\n","html":"<p>I was looking for a particular directory today on my computer and I didn&#8217;t know how to do it. After spending some time searching for it, I found the following command:<\/p>\n<p><code>find [parent directory] -type d -name [directory name]<\/code><\/p>\n<p>So in my case:<\/p>\n<p><code>find \/home\/vader -type d -name wrapper_scripts<\/code><\/p>\n<p>Optionally you can also use:<\/p>\n<p><code>find \/home\/vader -type d | grep wrapper<\/code><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 18 Jul 2013 07:53:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:17:43 +0000","updated_by":1,"published_at":"Thu, 18 Jul 2013 07:53:00 +0000","published_by":1},{"id":16,"title":"Google Now shows you bus times!","slug":"google-now-shows-you-bus-times","markdown":"\nSo today I learnt that Google Now can show you bus times. I think this is cool as a student using public transport.\n\nAlthough, you have to be at or near the bus stop to view its times. It can pick that up from a calendar entry\/email\/search as well. It would\u2019ve been\u00a0 nice if I could search for a bus stop to view its times but as it turns out, you can\u2019t.\n\nSo I guess this isn\u2019t convincing enough to make me uninstall my bus checker app but it\u2019s a start.\n\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/Screenshot_2013-09-03-13-56-55-169x300.png?resize=169%2C300)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/Screenshot_2013-09-03-13-56-55.png)<\/div>\n","html":"<p dir=\"ltr\">So today I learnt that Google Now can show you bus times. I think this is cool as a student using public transport.<\/p>\n<p dir=\"ltr\">Although, you have to be at or near the bus stop to view its times. It can pick that up from a calendar entry\/email\/search as well. It would&#8217;ve been\u00a0 nice if I could search for a bus stop to view its times but as it turns out, you can&#8217;t.<\/p>\n<p dir=\"ltr\">So I guess this isn&#8217;t convincing enough to make me uninstall my bus checker app but it&#8217;s a start.<\/p>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/Screenshot_2013-09-03-13-56-55.png\"> <img src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/Screenshot_2013-09-03-13-56-55-169x300.png?resize=169%2C300\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/> <\/a><\/div>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/Screenshot_2013-09-03-13-56-55.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 03 Sep 2013 15:29:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:03:30 +0000","updated_by":1,"published_at":"Tue, 03 Sep 2013 15:29:00 +0000","published_by":1},{"id":15,"title":"Fixing HDMI display underscan\/overscan","slug":"how-to-fix-the-display-issue-where-edges-of-your-screen-are-cut-off-every-time-you-connect-your-pc-to-a-hdtv","markdown":"\n<div style=\"text-align: justify;\">So I have this problem every time I connect my laptop to my Samsung HDTV. I thought this was an issue with driver\/video card but I wasn\u2019t convinced because I wanted a solution. Finally, today, I managed to find a solution for this. Here\u2019s how you can solve it:<\/div><div style=\"text-align: justify;\">Go to display driver settings. In my case, since my laptop has two graphics cards \u2013 Nvidia 640m and Intel HD 4000 and since the primary display driver is Intel, I went to Intel HD Graphics Control Panel.<\/div><div style=\"clear: both; text-align: center;\">[![](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr012-300x241.png?resize=320%2C257)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr012.png)<\/div><div style=\"clear: both; text-align: left;\"><\/div><div style=\"clear: both; text-align: justify;\">Click on Display to view display settings.<\/div><div style=\"clear: both; text-align: center;\">[![](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr022-300x160.png?resize=640%2C340)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr022.png)<\/div><div style=\"clear: both; text-align: center;\"><\/div><div style=\"clear: both; text-align: justify;\">Ensure that you have connected your computer to TV using a HDMI cable. If you have, you should see something like \u201cDigital television SAMSUNG\u201d in the combo box under Select Display.<\/div><div style=\"clear: both; text-align: center;\">[![](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr031-300x163.png?resize=640%2C346)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr031.png)<\/div><div style=\"clear: both; text-align: center;\"><\/div><div style=\"clear: both; text-align: justify;\">Make sure that you have selected \u201cDisplay Settings\u201d tab and \u201cBasic\u201d twisty has been expanded. Under \u201cScaling\u201d, select \u201cCustomize Aspect Ratio\u201d. In the preview section, you will see two bars (horizontal and vertical) appear. You can adjust the aspect ratio using those. As you can see from the screenshot below, 63 vertical and 65 horizontal works perfect for me. Once you\u2019re done experimenting, hit apply.<\/div><div style=\"clear: both; text-align: center;\">[![](https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr041-300x163.png?resize=640%2C346)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr041.png)<\/div><div style=\"clear: both; text-align: left;\"><\/div><div style=\"clear: both; text-align: justify;\">Additionally, you might want to save those settings under a different profile. You can click on save profile button to do that. Give it a meaningful name and you\u2019re done. That way, you can keep different profiles for different monitors\/TVs.<\/div>\n","html":"<div style=\"text-align: justify;\">So I have this problem every time I connect my laptop to my Samsung HDTV. I thought this was an issue with driver\/video card but I wasn&#8217;t convinced because I wanted a solution. Finally, today, I managed to find a solution for this. Here&#8217;s how you can solve it:<\/div>\n<p><!--more--><\/p>\n<div style=\"text-align: justify;\">Go to display driver settings. In my case, since my laptop has two graphics cards &#8211; Nvidia 640m and Intel HD 4000 and since the primary display driver is Intel, I went to Intel HD Graphics Control Panel.<\/div>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr012.png\"><img src=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr012-300x241.png?resize=320%2C257\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: left;\"><\/div>\n<div style=\"clear: both; text-align: justify;\">Click on Display to view display settings.<\/div>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr022.png\"><img src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr022-300x160.png?resize=640%2C340\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: center;\"><\/div>\n<div style=\"clear: both; text-align: justify;\">Ensure that you have connected your computer to TV using a HDMI cable. If you have, you should see something like &#8220;Digital television SAMSUNG&#8221; in the combo box under Select Display.<\/div>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr031.png\"><img src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr031-300x163.png?resize=640%2C346\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: center;\"><\/div>\n<div style=\"clear: both; text-align: justify;\">Make sure that you have selected &#8220;Display Settings&#8221; tab and &#8220;Basic&#8221; twisty has been expanded. Under &#8220;Scaling&#8221;, select &#8220;Customize Aspect Ratio&#8221;. In the preview section, you will see two bars (horizontal and vertical) appear. You can adjust the aspect ratio using those. As you can see from the screenshot below, 63 vertical and 65 horizontal works perfect for me. Once you&#8217;re done experimenting, hit apply.<\/div>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr041.png\"><img src=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr041-300x163.png?resize=640%2C346\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: left;\"><\/div>\n<div style=\"clear: both; text-align: justify;\">Additionally, you might want to save those settings under a different profile. You can click on save profile button to do that. Give it a meaningful name and you&#8217;re done. That way, you can keep different profiles for different monitors\/TVs.<\/div>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr012.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 04 Sep 2013 13:35:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:00:50 +0000","updated_by":1,"published_at":"Wed, 04 Sep 2013 13:35:00 +0000","published_by":1},{"id":14,"title":"Assigning static IP address to your Raspberry Pi (WiFi)","slug":"assigning-static-ip-address-to-your-raspberry-pi-wifi","markdown":"\nWhen I first bought my Raspberry Pi, I had this problem. My router and TV are in different rooms and I don\u2019t have a ethernet cable. This restricts networking ability of my Pi which is quite annoying. I searched for articles on assigning static IP addresses to Raspberry Pi but most of them were talking about assigning it for eth0. I, on the other hand wanted wlan0 static IP.\n\nNow, I could have just got a long ethernet cable and solve this whole problem but I was just annoyed and was constantly asking myself \u201cWhy isn\u2019t this working over wifi\u201d. So finally, after significant digging around, I found the solution. Here it goes:  \n  \n First of all, take a backup of \/etc\/network\/interfaces file:  \n```\npi@raspberrypi ~ $ sudo su -<br><\/br>\nroot@raspberrypi:~# cp \/etc\/network\/interfaces .<br><\/br>```\n  \n`<br><\/br>`Now, in order to make this work, we need some information first. We need to find out the new static ip address that you want, gateway, mask, network and broadcast address. Assuming that you have a working wifi connection on your Raspberry Pi, type:  \n`root@raspberrypi:~# ifconfig`\n\nFrom the output, the bit that we are interested in is:  \n```\nwlan0 \u00a0 \u00a0 Link encap:Ethernet \u00a0HWaddr 00:00:00:00:00:00<br><\/br><b>inet addr:192.168.1.99 \u00a0Bcast:192.168.1.255 \u00a0Mask:255.255.255.0<\/b><br><\/br>\nUP BROADCAST RUNNING MULTICAST \u00a0MTU:1500 \u00a0Metric:1<br><\/br>\nRX packets:19153 errors:0 dropped:24512 overruns:0 frame:0<br><\/br>\nTX packets:25553 errors:0 dropped:0 overruns:0 carrier:0<br><\/br>\ncollisions:0 txqueuelen:1000<br><\/br>\nRX bytes:3724822 (3.5 MiB) \u00a0TX bytes:31437802 (29.9 MiB)```\n  \n`<br><\/br>`The highlighted line will give you information about your current IP, broadcast and mask address. Take a note of the last two. So, in my case:  \n**address**:\u00a0(this is the IP address you wish to reserve as static eg. 192.168.1.69)  \n**mask**: 255.255.255.0  \n**broadcast**: 192.168.1.255\n\nTo find out information about the last two bits, type:  \n```\nroot@raspberrypi:~# netstat -nr<br><\/br>\nKernel IP routing table<br><\/br>\nDestination \u00a0 \u00a0 Gateway \u00a0 \u00a0 \u00a0 \u00a0 Genmask \u00a0 \u00a0 \u00a0 \u00a0 Flags \u00a0 MSS Window \u00a0irtt Iface<br><\/br>\n0.0.0.0 \u00a0 \u00a0 \u00a0 \u00a0 <b>192.168.1.1<\/b> \u00a0 \u00a0 0.0.0.0 \u00a0 \u00a0 \u00a0 \u00a0 UG \u00a0 \u00a0 \u00a0 \u00a00 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 wlan0<br><\/br><b>192.168.1.0<\/b> \u00a0 \u00a0 0.0.0.0 \u00a0 \u00a0 \u00a0 \u00a0 255.255.255.0 \u00a0 U \u00a0 \u00a0 \u00a0 \u00a0 0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 wlan0```\n  \n`<br><\/br>`In my case:  \n**gateway**: 192.168.1.1  \n**network (a.k.a destination)**: 192.168.1.0\n\nNow that we have all the information that we need, edit the \/etc\/network\/interfaces file using nano. Putting all the bits and pieces together, it should look like:\n\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr011-300x189.png?resize=320%2C201)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr011.png)<\/div>Press Ctrl + O to save.  \n Now, since I was doing this whole process over SSH, I had to write a script to restart wifi because if I didn\u2019t, as soon as I turn it off, it would also take away my SSH access rendering me powerless. You can do the following in either case. Create a new file called restartwifi.sh using nano in root home directory:  \n`root@raspberrypi:~# nano restartwifi.sh`\n\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr021-300x189.png?resize=320%2C201)](https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr021.png)<\/div>Now, I am aware that this script can be improved but this is what I had before and it has worked without any problems. Press Ctrl + O to save the script. Then press Ctrl + X to exit.\n\nMake sure you edit permissions to make the script executable. Run:  \n`root@raspberrypi:~# chmod +x restartwifi.sh`\n\n<div><\/div><div>If you\u2019re not root, you\u2019ll have to prefix above command with \u201csudo\u201d.<\/div><div><\/div><div>Now comes the moment of truth. Run the script to reset your wifi by typing following command:<\/div><div>`root@raspberrypi:~# nohup .\/restartwifi.sh &`<\/div><div><\/div><div>If you\u2019re in putty session, you\u2019ll immediately go offline. If you\u2019ve done everything correctly, you should be able to ssh into your Pi, after couple of minutes, on your chosen IP address.<\/div>\n","html":"<p>When I first bought my Raspberry Pi, I had this problem. My router and TV are in different rooms and I don&#8217;t have a ethernet cable. This restricts networking ability of my Pi which is quite annoying. I searched for articles on assigning static IP addresses to Raspberry Pi but most of them were talking about assigning it for eth0. I, on the other hand wanted wlan0 static IP.<\/p>\n<p>Now, I could have just got a long ethernet cable and solve this whole problem but I was just annoyed and was constantly asking myself &#8220;Why isn&#8217;t this working over wifi&#8221;. So finally, after significant digging around, I found the solution. Here it goes:<br \/>\n<!--more--><br \/>\nFirst of all, take a backup of \/etc\/network\/interfaces file:<br \/>\n<code>pi@raspberrypi ~ $ sudo su -<br \/>\nroot@raspberrypi:~# cp \/etc\/network\/interfaces .<br \/>\n<\/code><br \/>\n<code><br \/>\n<\/code>Now, in order to make this work, we need some information first. We need to find out the new static ip address that you want, gateway, mask, network and broadcast address. Assuming that you have a working wifi connection on your Raspberry Pi, type:<br \/>\n<code>root@raspberrypi:~# ifconfig<\/code><\/p>\n<p>From the output, the bit that we are interested in is:<br \/>\n<code>wlan0 \u00a0 \u00a0 Link encap:Ethernet \u00a0HWaddr 00:00:00:00:00:00<br \/>\n<b>inet addr:192.168.1.99 \u00a0Bcast:192.168.1.255 \u00a0Mask:255.255.255.0<\/b><br \/>\nUP BROADCAST RUNNING MULTICAST \u00a0MTU:1500 \u00a0Metric:1<br \/>\nRX packets:19153 errors:0 dropped:24512 overruns:0 frame:0<br \/>\nTX packets:25553 errors:0 dropped:0 overruns:0 carrier:0<br \/>\ncollisions:0 txqueuelen:1000<br \/>\nRX bytes:3724822 (3.5 MiB) \u00a0TX bytes:31437802 (29.9 MiB)<\/code><br \/>\n<code><br \/>\n<\/code>The highlighted line will give you information about your current IP, broadcast and mask address. Take a note of the last two. So, in my case:<br \/>\n<b>address<\/b>:\u00a0(this is the IP address you wish to reserve as static eg. 192.168.1.69)<br \/>\n<b>mask<\/b>: 255.255.255.0<br \/>\n<b>broadcast<\/b>: 192.168.1.255<\/p>\n<p>To find out information about the last two bits, type:<br \/>\n<code>root@raspberrypi:~# netstat -nr<br \/>\nKernel IP routing table<br \/>\nDestination \u00a0 \u00a0 Gateway \u00a0 \u00a0 \u00a0 \u00a0 Genmask \u00a0 \u00a0 \u00a0 \u00a0 Flags \u00a0 MSS Window \u00a0irtt Iface<br \/>\n0.0.0.0 \u00a0 \u00a0 \u00a0 \u00a0 <b>192.168.1.1<\/b> \u00a0 \u00a0 0.0.0.0 \u00a0 \u00a0 \u00a0 \u00a0 UG \u00a0 \u00a0 \u00a0 \u00a00 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 wlan0<br \/>\n<b>192.168.1.0<\/b> \u00a0 \u00a0 0.0.0.0 \u00a0 \u00a0 \u00a0 \u00a0 255.255.255.0 \u00a0 U \u00a0 \u00a0 \u00a0 \u00a0 0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a00 wlan0<\/code><br \/>\n<code><br \/>\n<\/code>In my case:<br \/>\n<b>gateway<\/b>: 192.168.1.1<br \/>\n<b>network (a.k.a destination)<\/b>: 192.168.1.0<\/p>\n<p>Now that we have all the information that we need, edit the \/etc\/network\/interfaces file using nano. Putting all the bits and pieces together, it should look like:<\/p>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr011.png\"><img src=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr011-300x189.png?resize=320%2C201\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<p>Press Ctrl + O to save.<br \/>\nNow, since I was doing this whole process over SSH, I had to write a script to restart wifi because if I didn&#8217;t, as soon as I turn it off, it would also take away my SSH access rendering me powerless. You can do the following in either case. Create a new file called restartwifi.sh using nano in root home directory:<br \/>\n<code>root@raspberrypi:~# nano restartwifi.sh<\/code><\/p>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr021.png\"><img src=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr021-300x189.png?resize=320%2C201\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<p>Now, I am aware that this script can be improved but this is what I had before and it has worked without any problems. Press Ctrl + O to save the script. Then press Ctrl + X to exit.<\/p>\n<p>Make sure you edit permissions to make the script executable. Run:<br \/>\n<code>root@raspberrypi:~# chmod +x restartwifi.sh<\/code><\/p>\n<div><\/div>\n<div>If you&#8217;re not root, you&#8217;ll have to prefix above command with &#8220;sudo&#8221;.<\/div>\n<div><\/div>\n<div>Now comes the moment of truth. Run the script to reset your wifi by typing following command:<\/div>\n<div><code>root@raspberrypi:~# nohup .\/restartwifi.sh &amp;<\/code><\/div>\n<div><\/div>\n<div>If you&#8217;re in putty session, you&#8217;ll immediately go offline. If you&#8217;ve done everything correctly, you should be able to ssh into your Pi, after couple of minutes, on your chosen IP address.<\/div>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr011.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 14 Sep 2013 18:23:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:10:18 +0000","updated_by":1,"published_at":"Sat, 14 Sep 2013 18:23:00 +0000","published_by":1},{"id":13,"title":"Changing keyboard layout in Ubuntu Server (Linux, How to)","slug":"changing-keyboard-layout-in-ubuntu-server-linux-how-to","markdown":"\nSo I recently installed Ubuntu Server in a virtual machine and I had problems with my keyboard layout. I use UK layout while the default that comes with Ubuntu Server is US layout. As you can imagine, this caused problems and I had to go hunt for a solution. Finally, I found one and here it is:\n\nYou need to reconfigure keyboard configuration. Type the following:  \n`sudo dpkg-reconfigure keyboard-configuration`  \n  \n If that doesn\u2019t work, you might need to install console-data package. Install it by typing:  \n`sudo apt-get install console-data`\n\nand then try first step again.\n\nIf everything was fine, you should see this:\n\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr01-300x167.png?resize=320%2C177)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr01.png)<\/div><div style=\"clear: both; text-align: justify;\">Using this you can select brand of your keyboard. I am running this virtual machine on my Acer laptop and as you can see it is listed there. You can use up\/down\/page up\/page down keys to navigate through the list. When you have picked one, press enter to go to next screen.<\/div><div style=\"clear: both; text-align: justify;\"><\/div><div style=\"clear: both; text-align: justify;\">You should now see this:<\/div><div style=\"clear: both; text-align: center;\">[![](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr02-300x167.png?resize=320%2C178)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr02.png)<\/div><div style=\"clear: both; text-align: justify;\">This is the language selection screen. Navigate through the list, select your language and as you did in previous one, press enter to move on to next screen.<\/div><div style=\"clear: both; text-align: justify;\"><\/div><div style=\"clear: both; text-align: justify;\">This is the screen where you select your keyboard type:<\/div><div style=\"clear: both; text-align: center;\">[![](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr03-300x167.png?resize=320%2C177)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr03.png)<\/div><div style=\"clear: both; text-align: justify;\">Assuming you\u2019re using a qwerty keyboard for PC, you should just leave the first one selected and press enter. However, if you are running this on a mac or your keyboard is of different kind, you are free to choose whatever applies to you from the list and continue by pressing enter.<\/div>The next screen allows you to map a key on your keyboard to alternate grammar key (Alt Gr).\n\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr04-300x166.png?resize=320%2C176)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr04.png)<\/div>I have never used this key in my life for alternate grammar, however, hoping that I may use it at some point, I would map it to right-alt key (which actually is Alt Gr key). As usual, select one from the list and press enter.\n\nNext one is compose key. Same drill. Pick one and press enter:\n\n<div style=\"clear: both; text-align: center;\">[![](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr05-300x166.png?resize=320%2C177)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr05.png)<\/div>Compose key, if I remember correctly, allows you to type ASCII for certain characters when you press and hold it. This key on windows is left alt key however, here you can choose whatever you want for ubuntu server.\n\nWhen you press enter, the wizard will quit. So that was the keyboard configuration screen which allows you to configure your keyboard pretty neatly. Have fun!\n\n\n","html":"<p>So I recently installed Ubuntu Server in a virtual machine and I had problems with my keyboard layout. I use UK layout while the default that comes with Ubuntu Server is US layout. As you can imagine, this caused problems and I had to go hunt for a solution. Finally, I found one and here it is:<\/p>\n<p>You need to reconfigure keyboard configuration. Type the following:<br \/>\n<code>sudo dpkg-reconfigure keyboard-configuration<\/code><br \/>\n<!--more--><br \/>\nIf that doesn&#8217;t work, you might need to install console-data package. Install it by typing:<br \/>\n<code>sudo apt-get install console-data<\/code><\/p>\n<p>and then try first step again.<\/p>\n<p>If everything was fine, you should see this:<\/p>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr01.png\"><img src=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr01-300x167.png?resize=320%2C177\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: justify;\">Using this you can select brand of your keyboard. I am running this virtual machine on my Acer laptop and as you can see it is listed there. You can use up\/down\/page up\/page down keys to navigate through the list. When you have picked one, press enter to go to next screen.<\/div>\n<div style=\"clear: both; text-align: justify;\"><\/div>\n<div style=\"clear: both; text-align: justify;\">You should now see this:<\/div>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr02.png\"><img src=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr02-300x167.png?resize=320%2C178\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: justify;\">This is the language selection screen. Navigate through the list, select your language and as you did in previous one, press enter to move on to next screen.<\/div>\n<div style=\"clear: both; text-align: justify;\"><\/div>\n<div style=\"clear: both; text-align: justify;\">This is the screen where you select your keyboard type:<\/div>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr03.png\"><img src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr03-300x167.png?resize=320%2C177\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<div style=\"clear: both; text-align: justify;\">Assuming you&#8217;re using a qwerty keyboard for PC, you should just leave the first one selected and press enter. However, if you are running this on a mac or your keyboard is of different kind, you are free to choose whatever applies to you from the list and continue by pressing enter.<\/div>\n<p>The next screen allows you to map a key on your keyboard to alternate grammar key (Alt Gr).<\/p>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr04.png\"><img src=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr04-300x166.png?resize=320%2C176\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<p>I have never used this key in my life for alternate grammar, however, hoping that I may use it at some point, I would map it to right-alt key (which actually is Alt Gr key). As usual, select one from the list and press enter.<\/p>\n<p>Next one is compose key. Same drill. Pick one and press enter:<\/p>\n<div style=\"clear: both; text-align: center;\"><a style=\"margin-left: 1em; margin-right: 1em;\" href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr05.png\"><img src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr05-300x166.png?resize=320%2C177\" alt=\"\" border=\"0\" data-recalc-dims=\"1\" \/><\/a><\/div>\n<p>Compose key, if I remember correctly, allows you to type ASCII for certain characters when you press and hold it. This key on windows is left alt key however, here you can choose whatever you want for ubuntu server.<\/p>\n<p>When you press enter, the wizard will quit. So that was the keyboard configuration screen which allows you to configure your keyboard pretty neatly. Have fun!<\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2013\/09\/scr01.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 27 Sep 2013 07:22:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:11:19 +0000","updated_by":1,"published_at":"Fri, 27 Sep 2013 07:22:00 +0000","published_by":1},{"id":12,"title":"What to do when app downloads in Windows 8\/8.1 store get stuck at \"Pending\" or \"Downloading\" stage?","slug":"what-to-do-when-app-downloads-in-windows-88-1-store-get-stuck-at-pending-or-downloading-stage","markdown":"\nSo I had this problem today. Well to be honest, I had this problem before as well but I decided to fix it today. Earlier today, I upgraded to Windows 8.1 and I was hoping that the upgrade would resolve this problem. Well, it didn\u2019t. After surfing through the web, I found the solution. So, here it goes:\n\nFirst of all, you need to be in safe mode. Open the charms bar, click on the power button, hold the `shift` key and while you\u2019re holding it, click `\"Restart\"`. This will restart Windows in troubleshooting\/recovery mode.  \n  \n From the menu that has been presented to you, click `\"Troubleshoot\"`. Now select `\"Advanced options\"` and from the subsequent screen click `\"Windows Startup settings\"`. Finally, click `\"Restart\"` button.\n\nNow, if you have Windows 8 installed, you will have an old command prompt styled menu from which you can select `\"Safe Mode with Networking\"` by pressing navigational arrow keys. However, if you have Windows 8.1, then you will have to press number 5 key on your keyboard to go into `\"Safe Mode with Networking\"`. Once you\u2019re there, fire up command prompt. Type in:  \n`cd %systemroot%`\n\nThen type:  \n`ren SoftwareDistribution SoftwareDistribution.old`\n\nThat\u2019s it. Restart the computer by using the charms menu. This time, don\u2019t hold down the shift key.\n\nWhen Windows normally boots up, open the store, wait a few seconds and try updating\/downloading apps and it should work just fine.\n\n\n","html":"<p>So I had this problem today. Well to be honest, I had this problem before as well but I decided to fix it today. Earlier today, I upgraded to Windows 8.1 and I was hoping that the upgrade would resolve this problem. Well, it didn&#8217;t. After surfing through the web, I found the solution. So, here it goes:<\/p>\n<p>First of all, you need to be in safe mode. Open the charms bar, click on the power button, hold the <code>shift<\/code> key and while you&#8217;re holding it, click <code>\"Restart\"<\/code>. This will restart Windows in troubleshooting\/recovery mode.<br \/>\n<!--more--><br \/>\nFrom the menu that has been presented to you, click <code>\"Troubleshoot\"<\/code>. Now select <code>\"Advanced options\"<\/code> and from the subsequent screen click <code>\"Windows Startup settings\"<\/code>. Finally, click <code>\"Restart\"<\/code> button.<\/p>\n<p>Now, if you have Windows 8 installed, you will have an old command prompt styled menu from which you can select <code>\"Safe Mode with Networking\"<\/code> by pressing navigational arrow keys. However, if you have Windows 8.1, then you will have to press number 5 key on your keyboard to go into <code>\"Safe Mode with Networking\"<\/code>. Once you&#8217;re there, fire up command prompt. Type in:<br \/>\n<code>cd %systemroot%<\/code><\/p>\n<p>Then type:<br \/>\n<code>ren SoftwareDistribution SoftwareDistribution.old<\/code><\/p>\n<p>That&#8217;s it. Restart the computer by using the charms menu. This time, don&#8217;t hold down the shift key.<\/p>\n<p>When Windows normally boots up, open the store, wait a few seconds and try updating\/downloading apps and it should work just fine.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 17 Oct 2013 20:57:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:29:47 +0000","updated_by":1,"published_at":"Thu, 17 Oct 2013 20:57:00 +0000","published_by":1},{"id":11,"title":"An investigation regarding the effects of NoSQL on the four inherent problems of Software Engineering","slug":"an-investigation-regarding-the-effects-of-nosql-on-the-four-inherent-problems-of-software-engineering","markdown":"\n<div style=\"text-align: justify;\">So as you all know, I am currently in my final year of BEng (Hons) Software Engineering. In my final year, one of the courses that I am studying is Programming Frameworks. This course introduces to the academic side of Software Engineering and covers topics like quality plans, quality control, advanced methodologies, planning, frameworks etc.<\/div><div style=\"text-align: justify;\"><\/div><div style=\"text-align: justify;\">My coursework for this course was to prepare a quality plan for writing an academic paper and then write the academic paper. The main topic of the academic paper was to compare and contrast one of the three concepts with an academic paper \u201cNo Silver Bullet \u2013 Essence and Accidents of Software Engineering\u201d written by Fred Brooks. I prepared the quality plan, wrote the academic paper and submitted the coursework last month. I received a really good mark for it \u2013 88% and thus I thought that I should share the academic paper that I wrote in LaTeX. You can download the academic paper from here.<\/div><div style=\"text-align: justify;\">\n## [\u201cAn investigation regarding the effects of NoSQL on the four inherent problems of Software Engineering\u201d  \n by Manthan Dave](https:\/\/www.dropbox.com\/s\/0e52dtambpjzzsh\/nosql-paper.pdf)\n\n\u00a0\n\n<\/div><div style=\"text-align: justify;\">Let me know what you think about the paper in the comments below.<\/div>\n","html":"<div style=\"text-align: justify;\">So as you all know, I am currently in my final year of BEng (Hons) Software Engineering. In my final year, one of the courses that I am studying is Programming Frameworks. This course introduces to the academic side of Software Engineering and covers topics like quality plans, quality control, advanced methodologies, planning, frameworks etc.<\/div>\n<div style=\"text-align: justify;\"><\/div>\n<div style=\"text-align: justify;\">My coursework for this course was to prepare a quality plan for writing an academic paper and then write the academic paper. The main topic of the academic paper was to compare and contrast one of the three concepts with an academic paper &#8220;No Silver Bullet &#8211; Essence and Accidents of Software Engineering&#8221; written by Fred Brooks. I prepared the quality plan, wrote the academic paper and submitted the coursework last month. I received a really good mark for it &#8211; 88% and thus I thought that I should share the academic paper that I wrote in LaTeX. You can download the academic paper from here.<\/div>\n<p><!--more--><\/p>\n<div style=\"text-align: justify;\">\n<h2><a href=\"https:\/\/www.dropbox.com\/s\/0e52dtambpjzzsh\/nosql-paper.pdf\" target=\"_blank\">&#8220;An investigation regarding the effects of NoSQL on the four inherent problems of Software Engineering&#8221;<br \/>\nby Manthan Dave<\/a><\/h2>\n<p>&nbsp;<\/p>\n<\/div>\n<div style=\"text-align: justify;\">Let me know what you think about the paper in the comments below.<\/div>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 07 Dec 2013 20:17:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:02:49 +0000","updated_by":1,"published_at":"Sat, 07 Dec 2013 20:17:00 +0000","published_by":1},{"id":10,"title":"Rejecting a pairing request from a bluetooth device permanently on your Mac","slug":"rejecting-a-pairing-request-from-a-bluetooth-device-permanently-on-your-mac","markdown":"\nSo I had this problem the other day where I kept on getting pairing requests from my friend\u2019s bluetooth keyboard. This was rather annoying as the pairing dialog kept on popping up every few minutes. So I googled around a bit and found the following solution which worked for me.\n\nTurn your bluetooth off. This will disable your bluetooth mouse and keyboard and hence you will have to use the built-in ones.\n\nThe problem is that at some point the keyboard that is nagging you to connect would have connected to your laptop. Your laptop remembers this and hence accepts incoming pairing request prompting you to verify it.  \n  \n Our aim is to make the machine forget that it was ever connected to this device. In order to do this, you will need to edit a file called com.apple.Bluetooth.plist. This file is located in \/Library\/Preferences and ~\/Library\/Preferences folder. This file is in binary so in order to be able to edit it, you will have to convert it to xml first. So, open up terminal and type:  \n`sudo plutil -convert xml1 \/Library\/Preferences\/com.apple.Bluetooth.plist`\n\nNow you can go on to finder or whatever you have and edit this file. Remove \u2026 followed by \u2026 tags which relate to the keyboard\/device that you are trying to ban. Simple way is to find the name of the device in the file. This name will be in \u2026 tags. Once you find the name, remove the whole container \u2026 and preceding \u2026 tags. Once you do this, save the file and convert it back to binary using the following terminal command:  \n`sudo plutil -convert binary1 \/Library\/Preferences\/com.apple.Bluetooth.plist`\n\nDo the same for the com.apple.Bluetooth.plist file located in ~\/Library\/Preferences folder as well. If that folder does not have this file, copy it over from \/Library\/Preferences folder.\n\nOnce you are done, turn the bluetooth on and now it shouldn\u2019t prompt you for pairing requests.\n\n\n","html":"<p>So I had this problem the other day where I kept on getting pairing requests from my friend&#8217;s bluetooth keyboard. This was rather annoying as the pairing dialog kept on popping up every few minutes. So I googled around a bit and found the following solution which worked for me.<\/p>\n<p>Turn your bluetooth off. This will disable your bluetooth mouse and keyboard and hence you will have to use the built-in ones.<\/p>\n<p>The problem is that at some point the keyboard that is nagging you to connect would have connected to your laptop. Your laptop remembers this and hence accepts incoming pairing request prompting you to verify it.<br \/>\n<!--more--><br \/>\nOur aim is to make the machine forget that it was ever connected to this device. In order to do this, you will need to edit a file called com.apple.Bluetooth.plist. This file is located in \/Library\/Preferences and ~\/Library\/Preferences folder. This file is in binary so in order to be able to edit it, you will have to convert it to xml first. So, open up terminal and type:<br \/>\n<code>sudo plutil -convert xml1 \/Library\/Preferences\/com.apple.Bluetooth.plist<\/code><\/p>\n<p>Now you can go on to finder or whatever you have and edit this file. Remove &#8230; followed by &#8230; tags which relate to the keyboard\/device that you are trying to ban. Simple way is to find the name of the device in the file. This name will be in &#8230; tags. Once you find the name, remove the whole container &#8230; and preceding &#8230; tags. Once you do this, save the file and convert it back to binary using the following terminal command:<br \/>\n<code>sudo plutil -convert binary1 \/Library\/Preferences\/com.apple.Bluetooth.plist<\/code><\/p>\n<p>Do the same for the com.apple.Bluetooth.plist file located in ~\/Library\/Preferences folder as well. If that folder does not have this file, copy it over from \/Library\/Preferences folder.<\/p>\n<p>Once you are done, turn the bluetooth on and now it shouldn&#8217;t prompt you for pairing requests.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 05 Sep 2014 09:36:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:42:27 +0000","updated_by":1,"published_at":"Fri, 05 Sep 2014 09:36:00 +0000","published_by":1},{"id":9,"title":"Moving steam games to another drive","slug":"moving-steam-games-to-another-drive","markdown":"\nSo I came across this recently and thought to share it with you guys.\n\nFirst of all, exit steam completely. If you\u2019ve closed it, check if it\u2019s in the system tray and if it is, exit from there as well. Now, fire up Windows Explorer and go to where steam is installed on your computer. In my case, it was in the default installation directory (C:\\Program Files (x86)\\Steam).  \n  \n Once in the folder, delete everything EXCEPT the SteamApps folder and Steam.exe. Now, go UP a directory level and move the whole Steam folder to wherever you want it to be. In my case, I moved it to E:Games folder.\n\nWhen the move is complete, double-click on the steam.exe. It\u2019ll re-download steam client for you and will require you to login to your steam account again.\n\nSo that\u2019s it. You should have all your games available to you in your new drive location!\n\nOriginal source:\u00a0[https:\/\/support.steampowered.com\/kb_article.php?ref=7418-YUBN-8129](https:\/\/support.steampowered.com\/kb_article.php?ref=7418-YUBN-8129)\n\n\n","html":"<p>So I came across this recently and thought to share it with you guys.<\/p>\n<p>First of all, exit steam completely. If you&#8217;ve closed it, check if it&#8217;s in the system tray and if it is, exit from there as well. Now, fire up Windows Explorer and go to where steam is installed on your computer. In my case, it was in the default installation directory (C:\\Program Files (x86)\\Steam).<br \/>\n<!--more--><br \/>\nOnce in the folder, delete everything EXCEPT the SteamApps folder and Steam.exe. Now, go UP a directory level and move the whole Steam folder to wherever you want it to be. In my case, I moved it to E:Games folder.<\/p>\n<p>When the move is complete, double-click on the steam.exe. It&#8217;ll re-download steam client for you and will require you to login to your steam account again.<\/p>\n<p>So that&#8217;s it. You should have all your games available to you in your new drive location!<\/p>\n<p>Original source:\u00a0<a href=\"https:\/\/support.steampowered.com\/kb_article.php?ref=7418-YUBN-8129\">https:\/\/support.steampowered.com\/kb_article.php?ref=7418-YUBN-8129<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 06 Sep 2014 22:45:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:17:05 +0000","updated_by":1,"published_at":"Sat, 06 Sep 2014 22:45:00 +0000","published_by":1},{"id":8,"title":"Fixing proxy credentials dementia in Eclipse","slug":"eclipse-forgets-my-proxy-credentials","markdown":"\nThis usually happens when you are using a different eclipse installation to access your old eclipse workspace. Its something to do with the new installation not being able to access the secure storage created by your old installation. Here\u2019s what I did to solve it:\n\nClose eclipse completely. Now fire up terminal or command prompt or whatever and navigate to your home directory. In my case it is `'\/Users\/manthan\/'` directory.  \n  \n Now cd into `.eclipse\/org.eclipse.equinox.security directory`.\n\n> `cd .eclipse\/org.eclipse.equinox.security`\n\nIf you list files, you should see a file called secure_storage in it. Rename that to `secure_storage.old` or something like that. You can delete it if you want but I prefer to rename things, make sure it works and then delete the renamed file.\n\n<div>> `mv secure_storage secure_storage.old`\n\n<\/div><div>Now making sure that the `secure_storage` file has been renamed successfully, open eclipse and now try saving your credentials. It should ask for your key store master password and other stuff.<\/div><div><\/div><div>That\u2019s it! You\u2019re done.<\/div>\n","html":"<p>This usually happens when you are using a different eclipse installation to access your old eclipse workspace. Its something to do with the new installation not being able to access the secure storage created by your old installation. Here&#8217;s what I did to solve it:<\/p>\n<p>Close eclipse completely. Now fire up terminal or command prompt or whatever and navigate to your home directory. In my case it is <code>'\/Users\/manthan\/'<\/code> directory.<br \/>\n<!--more--><br \/>\nNow cd into <code>.eclipse\/org.eclipse.equinox.security directory<\/code>.<\/p>\n<blockquote><p><code>cd .eclipse\/org.eclipse.equinox.security<\/code><\/p><\/blockquote>\n<p>If you list files, you should see a file called secure_storage in it. Rename that to <code>secure_storage.old<\/code> or something like that. You can delete it if you want but I prefer to rename things, make sure it works and then delete the renamed file.<\/p>\n<div>\n<blockquote><p><code>mv secure_storage secure_storage.old<\/code><\/p><\/blockquote>\n<\/div>\n<div>Now making sure that the <code>secure_storage<\/code> file has been renamed successfully, open eclipse and now try saving your credentials. It should ask for your key store master password and other stuff.<\/div>\n<div><\/div>\n<div>That&#8217;s it! You&#8217;re done.<\/div>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 12 Sep 2014 10:49:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:01:53 +0000","updated_by":1,"published_at":"Fri, 12 Sep 2014 10:49:00 +0000","published_by":1},{"id":7,"title":"Unix\/Linux command to check if Apache Tomcat is running","slug":"unixlinux-command-to-check-if-apache-tomcat-is-running","markdown":"\nSo I\u2019ve come across this problem quite a few times. Normal way to do this is:\n\n`ps -ef | grep tomcat`\n\nThis works most of the times. If tomcat is running, it gives between 1 and 2 lines back but if not, it gives anywhere between 0 and 1 lines back. A much cleaner use of the above command would be with `wc -l`:\n\n`ps -ef | grep tomcat | wc -l`\n\nHowever, this doesn\u2019t solve the actual problem as along with the tomcat process, it also gives you the process of command `\"grep tomcat\"`.  \n  \n Here\u2019s the command to solve this problem. You can use either of the two below commands:\n\n`ps -ef | grep tomcat | grep -v \"grep tomcat\" | wc -l`\n\n`ps -ef | grep tomca[t] | wc -l`\n\nThe first command explicly says that once you get a list of all processes containing the word tomcat, ignore lines containing words `\"grep tomcat\"`. And then the usual, pipe it to word count and output the number of lines.\n\nThe second one, however, tricks the grep into using a regular expression and ignoring itself. This is because the actual output containing `\"grep tomca[t]\"` will have the square brackets which obviously won\u2019t match the actual regular expression.\n\n\n","html":"<p>So I&#8217;ve come across this problem quite a few times. Normal way to do this is:<\/p>\n<p><code>ps -ef | grep tomcat<\/code><\/p>\n<p>This works most of the times. If tomcat is running, it gives between 1 and 2 lines back but if not, it gives anywhere between 0 and 1 lines back. A much cleaner use of the above command would be with <code>wc -l<\/code>:<\/p>\n<p><code>ps -ef | grep tomcat | wc -l<\/code><\/p>\n<p>However, this doesn&#8217;t solve the actual problem as along with the tomcat process, it also gives you the process of command <code>\"grep tomcat\"<\/code>.<br \/>\n<!--more--><br \/>\nHere&#8217;s the command to solve this problem. You can use either of the two below commands:<\/p>\n<p><code>ps -ef | grep tomcat | grep -v \"grep tomcat\" | wc -l<\/code><\/p>\n<p><code>ps -ef | grep tomca[t] | wc -l<\/code><\/p>\n<p>The first command explicly says that once you get a list of all processes containing the word tomcat, ignore lines containing words <code>\"grep tomcat\"<\/code>. And then the usual, pipe it to word count and output the number of lines.<\/p>\n<p>The second one, however, tricks the grep into using a regular expression and ignoring itself. This is because the actual output containing <code>\"grep tomca[t]\"<\/code> will have the square brackets which obviously won&#8217;t match the actual regular expression.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 20 Jul 2015 12:39:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 22:41:19 +0000","updated_by":1,"published_at":"Mon, 20 Jul 2015 12:39:00 +0000","published_by":1},{"id":6,"title":"How to get file sizes recursively in a directory and list them in ascending order","slug":"how-to-get-file-sizes-recursively-in-a-directory-and-list-them-in-ascending-order","markdown":"\nWhen cleaning up unused files from my linux computer, I find it useful to know which files use most space so that I can easily determine which ones to remove.\n\nThe most basic command to find file size of a given file is:  \n`du -sm myfile.txt`  \n  \n If you are in a folder with multiple files and want to get file sizes of all the files, the command is:  \n`du -sm *`\n\nThe above command does not get file sizes recursively. However, if you are happy with it and want to sort the file sizes in ascending order:  \n`du -sm * | sort -n -k 1`\n\nNow, this is little useful as it does not do recursive file size listing. To do that:  \n`find . -type f -exec du -sm {} \\; | sort -n -k 1`\n\nIf you are looking for a particular type of file (eg. jar files):  \n`find . -name \"*.jar\" -type f -exec du -sm {} \\; | sort -n -k 1`\n\nEnjoy!\n\n\n","html":"<p>When cleaning up unused files from my linux computer, I find it useful to know which files use most space so that I can easily determine which ones to remove.<\/p>\n<p>The most basic command to find file size of a given file is:<br \/>\n<code>du -sm myfile.txt<\/code><br \/>\n<!--more--><br \/>\nIf you are in a folder with multiple files and want to get file sizes of all the files, the command is:<br \/>\n<code>du -sm *<\/code><\/p>\n<p>The above command does not get file sizes recursively. However, if you are happy with it and want to sort the file sizes in ascending order:<br \/>\n<code>du -sm * | sort -n -k 1<\/code><\/p>\n<p>Now, this is little useful as it does not do recursive file size listing. To do that:<br \/>\n<code>find . -type f -exec du -sm {} \\; | sort -n -k 1<\/code><\/p>\n<p>If you are looking for a particular type of file (eg. jar files):<br \/>\n<code>find . -name \"*.jar\" -type f -exec du -sm {} \\; | sort -n -k 1<\/code><\/p>\n<p>Enjoy!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 06 Aug 2015 10:24:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:14:35 +0000","updated_by":1,"published_at":"Thu, 06 Aug 2015 10:24:00 +0000","published_by":1},{"id":5,"title":"Configuring Java HttpClient to use proxy","slug":"configuring-java-httpclient-to-use-proxy","markdown":"\nSo, for various security reasons, at work, I have to go through proxy in order to access anything. I was doing some prototyping with sparkpost and none of my code worked as by default the code wasn\u2019t going through the business proxy.\n\nNaturally, I thought that I\u2019d set HTTP_PROXY and HTTPS_PROXY environment variables and then run my java code. I did and as it is with all things in Software Engineering industry, it didn\u2019t work.\n\nAfter several hours of googling (or what felt like several hours) I finally found what was wrong with it. In Java, due to security reasons, all proxy variables are ignored unless they have been explicitly set in code. Here\u2019s how you set them:\n\nString proxyHost = System.getenv(\"HTTP_PROXY_HOST\"); String proxyPort = System.getenv(\"HTTP_PROXY_PORT\"); String proxyUser = System.getenv(\"HTTP_PROXY_USER\"); String proxyPassword = System.getenv(\"HTTP_PROXY_PWRD\"); CredentialsProvider credsProvider = new BasicCredentialsProvider(); credsProvider.setCredentials(new AuthScope(proxyHost, Integer.parseInt(proxyPort)), \u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0new UsernamePasswordCredentials(proxyUser, proxyPassword)); HttpHost proxyHostObject = new HttpHost(proxyHost, Integer.parseInt(proxyPort)); HttpClient client = HttpClientBuilder.create().setProxy(proxyHostObject) .setProxyAuthenticationStrategy(new ProxyAuthenticationStrategy()) .setDefaultCredentialsProvider(credsProvider) .build(); HttpGet getRequest = new HttpGet(URL); getRequest.addHeader(\"Authorization\", API_KEY); getRequest.addHeader(\"Content-Type\", \"application\/json\"); try { \u00a0\u00a0 \u00a0System.out.println(\"Executing request...\"); \u00a0\u00a0 \u00a0HttpResponse response = client.execute(getRequest); \u00a0\u00a0 \u00a0System.out.println(\"Request successfully executed.\"); \u00a0\u00a0 \u00a0HttpEntity entity = response.getEntity(); \u00a0\u00a0 \u00a0String responseString = EntityUtils.toString(entity); \u00a0\u00a0 \u00a0System.out.println(responseString); } catch (IOException e) { \u00a0\u00a0 \u00a0e.printStackTrace(); }\n\nThe CredentialsProvider class allows one to save the proxy credentials while the HttpHost class allows storing the proxy host.\n\nI personally am not a fan of this arrangement as the proxyHost and proxyPort is being duplicated twice in both classes. If you find a better arrangement, feel free to drop me a line.\n\nThanks!\n\n\n","html":"<p>So, for various security reasons, at work, I have to go through proxy in order to access anything. I was doing some prototyping with sparkpost and none of my code worked as by default the code wasn&#8217;t going through the business proxy.<\/p>\n<p>Naturally, I thought that I&#8217;d set HTTP_PROXY and HTTPS_PROXY environment variables and then run my java code. I did and as it is with all things in Software Engineering industry, it didn&#8217;t work.<\/p>\n<p>After several hours of googling (or what felt like several hours) I finally found what was wrong with it. In Java, due to security reasons, all proxy variables are ignored unless they have been explicitly set in code. Here&#8217;s how you set them:<!--more--><\/p>\n<pre class=\"lang:java\">String proxyHost = System.getenv(\"HTTP_PROXY_HOST\");\r\nString proxyPort = System.getenv(\"HTTP_PROXY_PORT\");\r\nString proxyUser = System.getenv(\"HTTP_PROXY_USER\");\r\nString proxyPassword = System.getenv(\"HTTP_PROXY_PWRD\");\r\n\r\nCredentialsProvider credsProvider = new BasicCredentialsProvider();\r\ncredsProvider.setCredentials(new AuthScope(proxyHost, Integer.parseInt(proxyPort)),\r\n\u00a0\u00a0 \u00a0\u00a0\u00a0 \u00a0new UsernamePasswordCredentials(proxyUser, proxyPassword));\r\n\r\nHttpHost proxyHostObject = new HttpHost(proxyHost, Integer.parseInt(proxyPort));\r\n\r\nHttpClient client = HttpClientBuilder.create().setProxy(proxyHostObject)\r\n    .setProxyAuthenticationStrategy(new ProxyAuthenticationStrategy())\r\n    .setDefaultCredentialsProvider(credsProvider)\r\n    .build();\r\n\r\nHttpGet getRequest = new HttpGet(URL);\r\ngetRequest.addHeader(\"Authorization\", API_KEY);\r\ngetRequest.addHeader(\"Content-Type\", \"application\/json\");\r\ntry {\r\n\u00a0\u00a0 \u00a0System.out.println(\"Executing request...\");\r\n\u00a0\u00a0 \u00a0HttpResponse response = client.execute(getRequest);\r\n\u00a0\u00a0 \u00a0System.out.println(\"Request successfully executed.\");\r\n\r\n\u00a0\u00a0 \u00a0HttpEntity entity = response.getEntity();\r\n\u00a0\u00a0 \u00a0String responseString = EntityUtils.toString(entity);\r\n\u00a0\u00a0 \u00a0System.out.println(responseString);\r\n} catch (IOException e) {\r\n\u00a0\u00a0 \u00a0e.printStackTrace();\r\n}<\/pre>\n<p>The CredentialsProvider class allows one to save the proxy credentials while the HttpHost class allows storing the proxy host.<\/p>\n<p>I personally am not a fan of this arrangement as the proxyHost and proxyPort is being duplicated twice in both classes. If you find a better arrangement, feel free to drop me a line.<\/p>\n<p>Thanks!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 24 Aug 2015 10:36:00 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:30:19 +0000","updated_by":1,"published_at":"Mon, 24 Aug 2015 10:36:00 +0000","published_by":1},{"id":116,"title":"Finding text within files on Linux","slug":"finding-text-within-files-on-linux","markdown":"\nSo at some point in your professional life, you have probably faced the following problem:\n\n> I have a bunch of files and I want to quickly find a file that contains a particular text. I don\u2019t want to go through each file individually because that\u2019s lame. Also, I don\u2019t want to use GUI because I aspire to become pro at using Linux commands.\n\nYeah, yeah we\u2019ve all been there. As a beginner, I struggled a lot with this and even if I found a solution, I won\u2019t be able to recall it the next time when I\u2019d so desperately need it. Well, here it is!\n\nIf you are looking in all files whose name ends with `.txt`:\n\n`find \/home\/manthan\/folder_o_files -name *.txt -type f -exec grep awesome {} \\; -print;`\n\nIn the above command, we\u2019re looking through all the files (denoted by `-type f`) in `\/home\/manthan\/folder_o_files` folder. The `-exec` flag allows user to specify what command to execute for line of output from the find command. In this case, we\u2019re using our favourite command `grep` to find for text `awesome`. The opening and closing curly brackets (denoted by `{}`) serve as placeholder for each line of file outputted by the find command. We escape the semi-colon to end the grep command, ask find command to show its findings by supplying it with `-print` flag and then terminate our command by ending it with semi-colon.\n\nIf you are unsure of the case of the text (i.e. upper-case or lower-case), stick `-i` flag right after `grep`.\n\nIf for some reason you are looking through a bunch of tar files and are looking for a file that one of the tar files might contain, check out my other post on ***How to find a file contained with a bunch of tar files without opening**** (soon)*.\n\nEnjoy!\n\n\n","html":"<p>So at some point in your professional life, you have probably faced the following problem:<\/p>\n<blockquote><p>I have a bunch of files and I want to quickly find a file that contains a particular text. I don&#8217;t want to go through each file individually because that&#8217;s lame. Also, I don&#8217;t want to use GUI because I aspire to become pro at using Linux commands.<\/p><\/blockquote>\n<p>Yeah, yeah we&#8217;ve all been there. As a beginner, I struggled a lot with this and even if I found a solution, I won&#8217;t be able to recall it the next time when I&#8217;d so desperately need it. Well, here it is!<!--more--><\/p>\n<p>If you are looking in all files whose name ends with <code>.txt<\/code>:<\/p>\n<p><code>find \/home\/manthan\/folder_o_files -name *.txt -type f -exec grep awesome {} \\; -print;<\/code><\/p>\n<p>In the above command, we&#8217;re looking through all the files (denoted by <code>-type f<\/code>) in <code>\/home\/manthan\/folder_o_files<\/code> folder. The <code>-exec<\/code> flag allows user to specify what command to execute for line of output from the find command. In this case, we&#8217;re using our favourite command <code>grep<\/code> to find for text <code>awesome<\/code>. The opening and closing curly brackets (denoted by <code>{}<\/code>) serve as placeholder for each line of file outputted by the find command. We escape the semi-colon to end the grep command, ask find command to show its findings by supplying it with <code>-print<\/code> flag and then terminate our command by ending it with semi-colon.<\/p>\n<p>If you are unsure of the case of the text (i.e. upper-case or lower-case), stick <code>-i<\/code> flag right after <code>grep<\/code>.<\/p>\n<p>If for some reason you are looking through a bunch of tar files and are looking for a file that one of the tar files might contain, check out my other post on <strong><em>How to find a file contained with a bunch of tar files without opening<\/em><\/strong><em> (soon)<\/em>.<\/p>\n<p>Enjoy!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 24 Aug 2015 20:02:55 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:26:03 +0000","updated_by":1,"published_at":"Mon, 24 Aug 2015 20:02:55 +0000","published_by":1},{"id":122,"title":"Developer code of conduct for Scrum Refinement ceremonies (Scrum series 1)","slug":"developer-code-of-conduct-for-scrum-refinement-ceremonies","markdown":"\nRefinement sessions. Some people like them and few enjoy them. Over the years, I\u2019ve worked across multiple companies who embrace Agile, particularly Scrum. I\u2019ve started my career with Agile and so far, I\u2019ve worked in organisations who are either mature Agile organisations or those who are transforming themselves into being Agile so I like to think that I have a pretty good grip on what an organisation\u2019s Agile ideals should be. However, ideal things don\u2019t happen in practice \u2013 and that\u2019s a good thing because I don\u2019t think that all organisations can wear one single Agile hat.\n\nDifferent places have different ideas about what a refinement session should be about. Some think that its an opportunity to connect with the product owner at technical level while others think that it is an opportunity to itemize stories by capturing more detail in them. Then there are also some outliers who think that refinement should be about the dev team and how they feel about the stories that are in the backlog.\n\nI think that all of the above views are correct \u2013 up to an extent.\n\n**Refinement sessions provide an opportunity to connect with the product owner. **While, as an agile team, you should be regularly talking to your product owner, constantly getting feedback on your work anyway, refinement sessions allow the whole team to come together to review upcoming work in the backlog and at the same time, negotiate the required amount of work with the product owner. However, this shouldn\u2019t become a tug of war but rather an open dialogue where ideas are allowed to flow freely. This is an opportunity for the development team to connect with the product owner at \u2018why\u2019 and \u2018what\u2019 level so that they can find and apply meaning to their work.\n\n**Refinement sessions are about itemizing stories by packing as much detail in them as possible.** This is quite popular. I\u2019ve seen people getting into low level details during refinement, talking about \u2018how\u2019 instead of just \u2018what\u2019 and \u2018why\u2019. If you have highly technical people in your team, who know loads of stuff and are packed with experience, or just people who are highly motivated and just can\u2019t wait to start working, then either you\u2019re already having this mindset or will be facing it soon. Its not a sin to go into low level detail but it is when people go into it for too long. Remember, \u201cI want a API orchestration layer\u201d or \u201cI want my posts to be fetched from a JSON REST compatible API\u201d said no customer ever. Refinement sessions are about capturing enough detail from the user\u2019s point of view for the development team to get started.\n\n**Refinement sessions are about dev team and how they feel about the current state of the backlog.** Very few people see it from this point of view as most forget that it is a two way conversation between product owner and the development team. Product owners don\u2019t own the team. They own the backlog and the work that is in it. Development team has every right to negotiate items that are on there if they feel that it is the right thing to do for the team. Another development team might not feel the same way but that\u2019s not the point as they are not the ones working on it. In some cases, the backlog is not ready to be refined as items that are on it severely lack purpose, let alone the details. In this case, the product owner should go back to the drawing board and get the information that is required to give a story its meaning. The story can be skipped or the session can be ended if no story is fit enough for refinement.\n\nIn addition to complying with all the \u201crules\u201d, I think that the teams should also meta-refine refinement sessions. If they are becoming a chore find out why. Liven it up a bit. Change something, gamify, let someone else drive the sessions or try out new things in general. You\u2019ll eventually find a sweet spot.\n\n\n","html":"<p>Refinement sessions. Some people like them and few enjoy them. Over the years, I&#8217;ve worked across multiple companies who embrace Agile, particularly Scrum. I&#8217;ve started my career with Agile and so far, I&#8217;ve worked in organisations who are either mature Agile organisations or those who are transforming themselves into being Agile so I like to think that I have a pretty good grip on what an organisation&#8217;s Agile ideals should be. However, ideal things don&#8217;t happen in practice &#8211; and that&#8217;s a good thing because I don&#8217;t think that all organisations can wear one single Agile hat.<!--more--><\/p>\n<p>Different places have different ideas about what a refinement session should be about. Some think that its an opportunity to connect with the product owner at technical level while others think that it is an opportunity to itemize stories by capturing more detail in them. Then there are also some outliers who think that refinement should be about the dev team and how they feel about the stories that are in the backlog.<\/p>\n<p>I think that all of the above views are correct &#8211; up to an extent.<\/p>\n<p><strong>Refinement sessions provide an opportunity to connect with the product owner. <\/strong>While, as an agile team, you should be regularly talking to your product owner, constantly getting feedback on your work anyway, refinement sessions allow the whole team to come together to review upcoming work in the backlog and at the same time, negotiate the required amount of work with the product owner. However, this shouldn&#8217;t become a tug of war but rather an open dialogue where ideas are allowed to flow freely. This is an opportunity for the development team to connect with the product owner at &#8216;why&#8217; and &#8216;what&#8217; level so that they can find and apply meaning to their work.<\/p>\n<p><strong>Refinement sessions are about itemizing stories by packing as much detail in them as possible.<\/strong> This is quite popular. I&#8217;ve seen people getting into low level details during refinement, talking about &#8216;how&#8217; instead of just &#8216;what&#8217; and &#8216;why&#8217;. If you have highly technical people in your team, who know loads of stuff and are packed with experience, or just people who are highly motivated and just can&#8217;t wait to start working, then either you&#8217;re already having this mindset or will be facing it soon. Its not a sin to go into low level detail but it is when people go into it for too long. Remember, &#8220;I want a API orchestration layer&#8221; or &#8220;I want my posts to be fetched from a JSON REST compatible API&#8221; said no customer ever. Refinement sessions are about capturing enough detail from the user&#8217;s point of view for the development team to get started.<\/p>\n<p><strong>Refinement sessions are about dev team and how they feel about the current state of the backlog.<\/strong> Very few people see it from this point of view as most forget that it is a two way conversation between product owner and the development team. Product owners don&#8217;t own the team. They own the backlog and the work that is in it. Development team has every right to negotiate items that are on there if they feel that it is the right thing to do for the team. Another development team might not feel the same way but that&#8217;s not the point as they are not the ones working on it. In some cases, the backlog is not ready to be refined as items that are on it severely lack purpose, let alone the details. In this case, the product owner should go back to the drawing board and get the information that is required to give a story its meaning. The story can be skipped or the session can be ended if no story is fit enough for refinement.<\/p>\n<p>In addition to complying with all the &#8220;rules&#8221;, I think that the teams should also meta-refine refinement sessions. If they are becoming a chore find out why. Liven it up a bit. Change something, gamify, let someone else drive the sessions or try out new things in general. You&#8217;ll eventually find a sweet spot.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 24 Aug 2015 21:20:02 +0000","created_by":1,"updated_at":"Thu, 27 Aug 2015 22:06:27 +0000","updated_by":1,"published_at":"Mon, 24 Aug 2015 21:20:02 +0000","published_by":1},{"id":118,"title":"Finding a file contained within a bunch of tar files without opening any","slug":"finding-a-file-contained-within-a-bunch-of-tar-files-without-opening-any","markdown":"\nSo I was debugging some code the other day and I quickly found myself in a situation where I had to look for a class file that was located in one of the many millions of jar files that are there in my maven cache. My immediate reaction was\n\n> \u201cUgh, this is going to be loads of effort and is going to take ages!\u201d.\n\nLuckily, quick one minute google found the following command:\n\n`find <strong>\/home\/manthan\/.m2<\/strong> -name '<strong>*.jar<\/strong>' -type f|while read f; do p=\"$(tar tf $f|egrep <strong>awesome<\/strong>)\"; [ -n \"$p\" ] && echo -e \"$f\\n$p\" ; p=\"\"; done`\n\nWhere `awesome.class` is the file I\u2019m looking for in all files (indicated with `-type f` option) with extension `jar `found in `\/home\/manthan\/.m2` directory.\n\nSource: [Superuser \u2013 Search through tar files for a pattern and print the full path of what\u2019s found](http:\/\/superuser.com\/questions\/838143\/search-through-tar-files-for-a-pattern-and-print-the-full-path-of-whats-found)\n\n\n","html":"<p>So I was debugging some code the other day and I quickly found myself in a situation where I had to look for a class file that was located in one of the many millions of jar files that are there in my maven cache. My immediate reaction was<\/p>\n<blockquote><p>&#8220;Ugh, this is going to be loads of effort and is going to take ages!&#8221;.<\/p><\/blockquote>\n<p>Luckily, quick one minute google found the following command:<!--more--><\/p>\n<p><code>find <strong>\/home\/manthan\/.m2<\/strong> -name '<strong>*.jar<\/strong>' -type f|while read f; do p=\"$(tar tf $f|egrep <strong>awesome<\/strong>)\"; [ -n \"$p\" ] &amp;&amp; echo -e \"$f\\n$p\" ; p=\"\"; done<\/code><\/p>\n<p>Where <code>awesome.class<\/code> is the file I&#8217;m looking for in all files (indicated with <code>-type f<\/code> option) with extension <code>jar <\/code>found in <code>\/home\/manthan\/.m2<\/code> directory.<\/p>\n<p>Source: <a href=\"http:\/\/superuser.com\/questions\/838143\/search-through-tar-files-for-a-pattern-and-print-the-full-path-of-whats-found\" target=\"_blank\">Superuser &#8211; Search through tar files for a pattern and print the full path of what&#8217;s found<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 25 Aug 2015 18:00:12 +0000","created_by":1,"updated_at":"Tue, 25 Aug 2015 23:29:11 +0000","updated_by":1,"published_at":"Tue, 25 Aug 2015 18:00:12 +0000","published_by":1},{"id":198,"title":"Working in IT industry, this made me smile","slug":"working-in-it-industry-this-made-me-smile","markdown":"\n[![Project planning estimates according to dilbert](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2015\/08\/dilbert_project_planning.gif?fit=700%2C299&ssl=1)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2015\/08\/dilbert_project_planning.gif?ssl=1)\n\n\n","html":"<p><a href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2015\/08\/dilbert_project_planning.gif?ssl=1\"><img class=\"aligncenter size-full wp-image-200\" src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2015\/08\/dilbert_project_planning.gif?fit=700%2C299&#038;ssl=1\" alt=\"Project planning estimates according to dilbert\" data-recalc-dims=\"1\" \/><\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 26 Aug 2015 10:30:30 +0000","created_by":1,"updated_at":"Wed, 26 Aug 2015 10:50:11 +0000","updated_by":1,"published_at":"Wed, 26 Aug 2015 10:30:30 +0000","published_by":1},{"id":134,"title":"Finding text in text files within multiple tar archives without extracting any file onto the file system","slug":"finding-text-in-text-files-within-multiple-tar-archives-without-extracting-any-file-onto-the-file-system","markdown":"\nSo I was working the other day and then suddenly I had a need to find a piece of text within loads of text files contained across multitude of tar files. In a normal case, you\u2019d extract them all somewhere in temp directory and then use grep to find the text you want. However, this doesn\u2019t work very well when the tar files you have are in thousands. Also, there is something uncool about doing stuff manually when you\u2019re in a world where there\u2019s always a better way.\n\nAfter an hour of experimentation, I found the following magic command:\n\n`find \/home\/manthan\/loads_of_tarfiles -name \"*.tar\" -type f -exec sh -c \"tar -tf {} | grep awesome | xargs -I found -n 1 tar -Oxf {} found | grep im\" \\; -print`\n\nThe above command finds all files (indicated by `-type f`) in `\/home\/manthan\/loads_of_tarfiles` folder with extension `.tar` (indicatetd by `-name \"*.tar\"`). For each file it finds, it runs a shell command (indicated by `-exec sh -c`) which lists all files contained within that tar file (indicated by `tar -tf {}`, where `{}` is each instance in which the find command found a tar file). For each file contained within the resident tar file, it then finds a text file whose name contains the word awesome (`grep awesome`). This can be anything as long as you know what kind of file you are looking for.\n\nFor each text file (whose name contains the word awesome) that is found within the tar file, it then extracts the contents of that file to `STDOUT` (indicated by `tar -Oxf {}` found where `{}` is the tar file that is found in the first part of the command and found is the text file that is found in the latter part of the command by the `grep awesome` command). Now that the contents of the file are available in `STDOUT`, a simple search finds the text we need in that file (using `grep im` which searches for text `im` within the extracted file). If it is found, the matching contents are printed and the name of the tar file it matched in is also printed (indicated by `-print`).\n\nPhew! That was lengthy. As usual, if you have any questions or find a better way, let me know in the comments below and I\u2019ll make sure to update the post with credits. Enjoy!\n\n\n","html":"<p>So I was working the other day and then suddenly I had a need to find a piece of text within loads of text files contained across multitude of tar files. In a normal case, you&#8217;d extract them all somewhere in temp directory and then use grep to find the text you want. However, this doesn&#8217;t work very well when the tar files you have are in thousands. Also, there is something uncool about doing stuff manually when you&#8217;re in a world where there&#8217;s always a better way.<\/p>\n<p>After an hour of experimentation, I found the following magic command:<!--more--><\/p>\n<p><code>find \/home\/manthan\/loads_of_tarfiles -name \"*.tar\" -type f -exec sh -c \"tar -tf {} | grep awesome | xargs -I found -n 1 tar -Oxf {} found | grep im\" \\; -print<\/code><\/p>\n<p>The above command finds all files (indicated by <code>-type f<\/code>) in <code>\/home\/manthan\/loads_of_tarfiles<\/code> folder with extension <code>.tar<\/code> (indicatetd by <code>-name \"*.tar\"<\/code>). For each file it finds, it runs a shell command (indicated by <code>-exec sh -c<\/code>) which lists all files contained within that tar file (indicated by <code>tar -tf {}<\/code>, where <code>{}<\/code> is each instance in which the find command found a tar file). For each file contained within the resident tar file, it then finds a text file whose name contains the word awesome (<code>grep awesome<\/code>). This can be anything as long as you know what kind of file you are looking for.<\/p>\n<p>For each text file (whose name contains the word awesome) that is found within the tar file, it then extracts the contents of that file to <code>STDOUT<\/code> (indicated by <code>tar -Oxf {}<\/code> found where <code>{}<\/code> is the tar file that is found in the first part of the command and found is the text file that is found in the latter part of the command by the <code>grep awesome<\/code> command). Now that the contents of the file are available in <code>STDOUT<\/code>, a simple search finds the text we need in that file (using <code>grep im<\/code> which searches for text <code>im<\/code> within the extracted file). If it is found, the matching contents are printed and the name of the tar file it matched in is also printed (indicated by <code>-print<\/code>).<\/p>\n<p>Phew! That was lengthy. As usual, if you have any questions or find a better way, let me know in the comments below and I&#8217;ll make sure to update the post with credits. Enjoy!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 26 Aug 2015 18:49:52 +0000","created_by":1,"updated_at":"Wed, 26 Aug 2015 18:25:12 +0000","updated_by":1,"published_at":"Wed, 26 Aug 2015 18:49:52 +0000","published_by":1},{"id":222,"title":"About","slug":"temp-slug-50","markdown":"","html":"","image":null,"featured":0,"page":1,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 26 Aug 2015 19:33:22 +0000","created_by":1,"updated_at":"Wed, 26 Aug 2015 19:33:22 +0000","updated_by":1,"published_at":"","published_by":1},{"id":204,"title":"Streaming an archive from a server over SSH and extracting it on the fly","slug":"streaming-an-archive-from-a-server-over-ssh-and-extracting-it-on-the-fly","markdown":"\nI\u2019d found this command a while ago now but had completely forgotten about it until I stumbled upon it yesterday. So, if you had one or more tar files on a server and wanted to get them all down and then extract them somewhere to do your work, the normal way to achieve that would be:\n\n`scp awesomeuser@awesomeserver:\/tmp\/mytarfile.tar \/storage\/`\n\nThe problem with this is that its a two step action. 1) You download the tar file(s). 2) You extract the downloaded tar file(s). While this is \u201cfunctional\u201d, here\u2019s an awesome way to do it:\n\n`curl -u awesomeuser: --key \/home\/awesomeuser\/.ssh\/id_rsa --pubkey \/home\/awesomeuser\/.ssh\/id_rsa.pub scp:\/\/awesomeserver\/tmp\/mytarfile.tar | tar xf -`\n\nLets break that down. We\u2019re using the curl command to download the file using `awesomeuser` as the username (indicated by `-u awesomeuser`). For authentication, we are using our private and public keys as indicated by `--key \/home\/awesomeuser\/.ssh\/id_rsa` and `--pubkey \/home\/awesomeuser\/.ssh\/id_rsa.pub` flags. We\u2019re then asking the curl command to use the `scp` protocol for accessing our server (as indicated by `scp:\/\/awesomeserver`) and are then giving it the location of the tar file that we want to download (as indicated by `\/tmp\/mytarfile.tar`). Because curl command will output whatever it gets to standard out stream (`STDOUT`), we\u2019re leveraging this capability by redirecting `STDOUT` to a pipe which is then fed into the tar command (as indicated by `| tar xf -`).\n\nIn case you have a `tar.gz` file or a `tgz` file, use:\n\n`curl -u awesomeuser: --key \/home\/awesomeuser\/.ssh\/id_rsa --pubkey \/home\/awesomeuser\/.ssh\/id_rsa.pub scp:\/\/awesomeserver\/tmp\/mytarfile.tar.gz | gunzip -c - | tar xf -`\n\nSame principle except before the final pipe of `tar xf -`, we\u2019re squeezing `gunzip -c` in between to g-unzip the file first before untarring it.\n\n\n","html":"<p>I&#8217;d found this command a while ago now but had completely forgotten about it until I stumbled upon it yesterday. So, if you had one or more tar files on a server and wanted to get them all down and then extract them somewhere to do your work, the normal way to achieve that would be:<\/p>\n<p><code>scp awesomeuser@awesomeserver:\/tmp\/mytarfile.tar \/storage\/<\/code><\/p>\n<p>The problem with this is that its a two step action. 1) You download the tar file(s). 2) You extract the downloaded tar file(s). While this is &#8220;functional&#8221;, here&#8217;s an awesome way to do it:<!--more--><\/p>\n<p><code>curl -u awesomeuser: --key \/home\/awesomeuser\/.ssh\/id_rsa --pubkey \/home\/awesomeuser\/.ssh\/id_rsa.pub scp:\/\/awesomeserver\/tmp\/mytarfile.tar | tar xf -<\/code><\/p>\n<p>Lets break that down. We&#8217;re using the curl command to download the file using <code>awesomeuser<\/code> as the username (indicated by <code>-u awesomeuser<\/code>). For authentication, we are using our private and public keys as indicated by <code>--key \/home\/awesomeuser\/.ssh\/id_rsa<\/code> and <code>--pubkey \/home\/awesomeuser\/.ssh\/id_rsa.pub<\/code> flags. We&#8217;re then asking the curl command to use the <code>scp<\/code> protocol for accessing our server (as indicated by <code>scp:\/\/awesomeserver<\/code>) and are then giving it the location of the tar file that we want to download (as indicated by <code>\/tmp\/mytarfile.tar<\/code>). Because curl command will output whatever it gets to standard out stream (<code>STDOUT<\/code>), we&#8217;re leveraging this capability by redirecting <code>STDOUT<\/code> to a pipe which is then fed into the tar command (as indicated by <code>| tar xf -<\/code>).<\/p>\n<p>In case you have a <code>tar.gz<\/code> file or a <code>tgz<\/code> file, use:<\/p>\n<p><code>curl -u awesomeuser: --key \/home\/awesomeuser\/.ssh\/id_rsa --pubkey \/home\/awesomeuser\/.ssh\/id_rsa.pub scp:\/\/awesomeserver\/tmp\/mytarfile.tar.gz | gunzip -c - | tar xf -<\/code><\/p>\n<p>Same principle except before the final pipe of <code>tar xf -<\/code>, we&#8217;re squeezing <code>gunzip -c<\/code> in between to g-unzip the file first before untarring it.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 27 Aug 2015 19:23:04 +0000","created_by":1,"updated_at":"Wed, 26 Aug 2015 18:21:34 +0000","updated_by":1,"published_at":"Thu, 27 Aug 2015 19:23:04 +0000","published_by":1},{"id":226,"title":"Managing your executable Java project with Gradle","slug":"managing-your-executable-java-project-with-gradle","markdown":"\nI\u2019ve been using Gradle for a while now and have quite used to it. Most of my Java projects are web applications so all Gradle has to do is to build them. Once built, I can just plop the war file onto tomcat and it just works. However, when I\u2019m trying something out, I just need a simple .java file which I manually compile with `javac` and then run it with `java` command. This works most of the time, except when I wanted to try out something complex, something that uses loads of external libaries. Something like this normally needs a dependency management system like Gradle. Using my normal Gradle configuration, I got it to build the project really easily, however running it was quite difficult.\n\nThe main difficulty came from classpath problems. When you are running a web application via tomcat, it usually takes care of all that stuff. However, when you\u2019re doing it manually, YOU have to take care of it. Again, because this required manual effort, I started finding ways to achieve that via Gradle. After a couple of hours, I finally got it working with the following build.gradle file:\n\napply plugin: 'java' apply plugin: 'idea' apply plugin: 'application' mainClassName = \"com.mypackage.awesomeapp.Main\" dependencies { compile 'org.apache.httpcomponents:httpclient:4.5' } run { systemProperties System.getProperties() println(systemProperties) }\n\nThe two key things here are the `apply plugin: 'application'` and `mainClassName = \"com.mypackage.awesomeapp.Main\"` lines. As the statement suggests, the first one applies the application plugin, declaring your project as a standalone application. The second one tells Gradle what class is your main class. Gradle then sets this in the manifest file of the resulting jar file so that when executed, it knows what class to invoke first. Once you have the setup ready, execute:\n\n`gradle run`\n\nFeel free to give me a shout if you find better solution than this and I\u2019ll make sure the article gets updated with proper credit. Enjoy!\n\n\n","html":"<p>I&#8217;ve been using Gradle for a while now and have quite used to it. Most of my Java projects are web applications so all Gradle has to do is to build them. Once built, I can just plop the war file onto tomcat and it just works. However, when I&#8217;m trying something out, I just need a simple .java file which I manually compile with <code>javac<\/code> and then run it with <code>java<\/code> command. This works most of the time, except when I wanted to try out something complex, something that uses loads of external libaries. Something like this normally needs a dependency management system like Gradle. Using my normal Gradle configuration, I got it to build the project really easily, however running it was quite difficult.<!--more--><\/p>\n<p>The main difficulty came from classpath problems. When you are running a web application via tomcat, it usually takes care of all that stuff. However, when you&#8217;re doing it manually, YOU have to take care of it. Again, because this required manual effort, I started finding ways to achieve that via Gradle. After a couple of hours, I finally got it working with the following build.gradle file:<\/p>\n<pre class=\"lang:groovy \">apply plugin: 'java'\r\napply plugin: 'idea'\r\napply plugin: 'application'\r\n\r\nmainClassName = \"com.mypackage.awesomeapp.Main\"\r\n\r\ndependencies {\r\ncompile 'org.apache.httpcomponents:httpclient:4.5'\r\n}\r\n\r\nrun {\r\nsystemProperties System.getProperties()\r\nprintln(systemProperties)\r\n}\r\n<\/pre>\n<p>The two key things here are the <code>apply plugin: 'application'<\/code> and <code>mainClassName = \"com.mypackage.awesomeapp.Main\"<\/code> lines. As the statement suggests, the first one applies the application plugin, declaring your project as a standalone application. The second one tells Gradle what class is your main class. Gradle then sets this in the manifest file of the resulting jar file so that when executed, it knows what class to invoke first. Once you have the setup ready, execute:<\/p>\n<p><code>gradle run<\/code><\/p>\n<p>Feel free to give me a shout if you find better solution than this and I&#8217;ll make sure the article gets updated with proper credit. Enjoy!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 28 Aug 2015 20:51:16 +0000","created_by":1,"updated_at":"Mon, 31 Aug 2015 13:23:49 +0000","updated_by":1,"published_at":"Fri, 28 Aug 2015 20:51:16 +0000","published_by":1},{"id":233,"title":"Up and running with Express (Express series 1)","slug":"up-and-running-with-express","markdown":"\nFor those who don\u2019t know, Express is a web server framework that runs on Node.js. Basically, if you want to write a web application in node.js, express is one of your options. I prefer it because of its simplicity.\n\nThe quickest way to write an express application is to use the express npm module. If you haven\u2019t got it, install it by executing:\n\n`npm install -g express`\n\nOnce installed, go to your projects folder, where you normally keep your projects and then execute:\n\n`express myawesomeproject`\n\nYou should see some output and hopefully no errors. This command creates a basic project structure under `myawesomeproject` directory with some files for you to quickly get started. Some notable files are `.\/bin\/www`, `.\/app.js` and `.\/routes\/index.js`.\n\nNow, `cd` into the project folder. You\u2019ll see several files and folders in that directory. Don\u2019t worry about that now. Let\u2019s get the application server up and running. However, before we can do that, we\u2019ll have to install the project dependencies. See the file called package.json? It defines the project and all its dependencies. NPM, being a dependency management system will go through the list of dependencies defined here and will then fetch them for you. If those dependencies have any tertiary dependencies then it\u2019ll fetch them as well. To install the dependencies, execute:\n\n`npm install`\n\nIf you have a working internet connection, then this should go smoothly. Now let\u2019s run the application. To do this, execute the following:\n\n`npm start`\n\nOpen your favourite web browser and navigate to the following url:\n\n`http:\/\/localhost:3000\/`\n\nYou should see a page with Welcome to myawesomeproject as its heading.\n\nAwesome work! You\u2019ve just created your first express web application.\n\nIn the next tutorial we\u2019ll go through the project structure to see how it all fits together.\n\n\n","html":"<p>For those who don&#8217;t know, Express is a web server framework that runs on Node.js. Basically, if you want to write a web application in node.js, express is one of your options. I prefer it because of its simplicity.<\/p>\n<p>The quickest way to write an express application is to use the express npm module. If you haven&#8217;t got it, install it by executing:<br \/>\n<!--more--><\/p>\n<p><code>npm install -g express<\/code><\/p>\n<p>Once installed, go to your projects folder, where you normally keep your projects and then execute:<\/p>\n<p><code>express myawesomeproject<\/code><\/p>\n<p>You should see some output and hopefully no errors. This command creates a basic project structure under <code>myawesomeproject<\/code> directory with some files for you to quickly get started. Some notable files are <code>.\/bin\/www<\/code>, <code>.\/app.js<\/code> and <code>.\/routes\/index.js<\/code>.<\/p>\n<p>Now, <code>cd<\/code> into the project folder. You&#8217;ll see several files and folders in that directory. Don&#8217;t worry about that now. Let&#8217;s get the application server up and running. However, before we can do that, we&#8217;ll have to install the project dependencies. See the file called package.json? It defines the project and all its dependencies. NPM, being a dependency management system will go through the list of dependencies defined here and will then fetch them for you. If those dependencies have any tertiary dependencies then it&#8217;ll fetch them as well. To install the dependencies, execute:<\/p>\n<p><code>npm install<\/code><\/p>\n<p>If you have a working internet connection, then this should go smoothly. Now let&#8217;s run the application. To do this, execute the following:<\/p>\n<p><code>npm start<\/code><\/p>\n<p>Open your favourite web browser and navigate to the following url:<\/p>\n<p><code>http:\/\/localhost:3000\/<\/code><\/p>\n<p>You should see a page with Welcome to myawesomeproject as its heading.<\/p>\n<p>Awesome work! You&#8217;ve just created your first express web application.<\/p>\n<p>In the next tutorial we&#8217;ll go through the project structure to see how it all fits together.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 31 Aug 2015 21:00:16 +0000","created_by":1,"updated_at":"Tue, 01 Sep 2015 21:50:44 +0000","updated_by":1,"published_at":"Mon, 31 Aug 2015 21:00:16 +0000","published_by":1},{"id":206,"title":"Thinking iteratively (Scrum series 2)","slug":"thinking-iteratively","markdown":"\nMost of us think linearly, that is step by step, from start to finish. Think about making something physical, like a wooden table. Your mind quickly starts creating steps leading to that goal. Something like this feels familiar?\n\n1. Buy lots of raw wood.\n2. Buy sandpaper.\n3. Buy varnish.\n4. Make four legs.\n5. Make table top.\n6. Apply sandpaper.\n7. Join it all up.\n8. Apply varnish.\n\nSorry, I\u2019m no expert in the art of table making but most of you would have a similar list of actions. This is linear, meaning that the table cannot be used at all unless all the steps are complete. If you don\u2019t have a dining table at all in your house, then you\u2019ll probably have to sit somewhere else to eat for a couple of days (depending on how good you are at making a wooden table).\n\nNow think about writing software, say a website to help out your friend sell hats for cats online. Automatically you\u2019ll start thinking:\n\n1. Design frontend.\n2. Develop backend.\n3. Implement frontend.\n4. Integrate payment systems.\n5. Integrate email notification service.\n6. Test.\n7. Deploy.\n8. Handover.\n\nThere\u2019s probably a lot of intermediary steps but this is the gist of it all. Again, if you look through carefully, your friend does not get her website unless all of the steps are complete.\n\nIterative development only means that instead of doing everything in linear sequential steps, you are doing it in cycles. Delivery is key here meaning that you should aim to deliver at least once by the end of each cycle but mature teams are able to deliver the product more than twice each cycle. Thinking in line with the Scrum methodology, each cycle (or sprint) can be one week or up to one month. In our case, if we choose that one cycle is one week, here\u2019s what it might look like:\n\n1. Cycle 1: 1. Design basic frontend.\n2. Implement mock backend.\n3. Implement basic frontend.\n4. Integrate mock payment system.\n5. Integrate mock email notification service.\n6. Deploy and deliver.\n2. Cycle 2: 1. Improve frontend design.\n2. Implement basic backend functionality still mocking rest.\n3. Implement improved frontend design.\n4. Integrate mock payment system.\n5. Integrate mock email notification service.\n6. Deploy and deliver.\n3. Cycle 3: 1. Implement more backend functionality.\n2. Revisit frontend designs.\n3. Implement frontend design changes.\n4. Integrate basic payment system.\n5. Integrate mock email notification service.\n6. Deploy and deliver.\n4. Cycle 4: 1. Implement more backend functionality.\n2. Revisit frontend designs.\n3. Implement frontend design changes.\n4. Integrate basic email notification service.\n5. Deploy and deliver.\n5. Cycle 5: 1. Implement more backend functionality.\n2. Integrate advanced payment system.\n3. Deploy and deliver.\n6. Cycle 6: 1. Implement more backend functionality.\n2. Revisit frontend designs.\n3. Implement frontend design changes.\n4. Integrate advanced email notification service.\n5. Deploy and deliver.\n\nHere, you are deploying and delivering the product to your friend every week so that she can inspect the whole flow. This also allows you to receive feedback regarding the entire product every week which is not possible in the linear way of doing things (as your product is not ready for your friend to look at until the end). Also you don\u2019t have to come up with all the cycles at once. You basically have to have the current cycle plus at least next two cycles within your sights.\n\nMocking things is very important in this flow as it helps your friend to \u201cfeel\u201d how the website will look at the end. The mocks allow her to go through the checkout flow as if it was actually there. It might seem like a waste of time as you are developing something that you know will get replaced at the end anyway, it still has tremendous value as there is nothing worse than the client changing her mind and wanting to go in a completely different direction towards the end of the project. The mocks allow the client to focus as they get to see the product as it is being developed while also giving you continuous feedback.\n\nWhile this was a simple example, it gets quite a bit complicated when new projects are undertaken in large scale organisations as there can be lack of clarity before a project is started. I\u2019ll do another post on the Scrum Framework for this sometime in near future but this post provides a base that can help you understand Scrum as it is also about iterative software development.\n\nAgain, if you observe the technological advancements that humans has achieved so far, you\u2019ll notice the iterative approach at large scale. We didn\u2019t aspire to build shiny cars from day one. We first started with the wheel, advanced to wheel barrow, horse cart, trains and then finally a built car.\n\nHope you enjoyed this article. I\u2019ll appreciate it very much if you could leave your thoughts in the comments below.\n\n\n","html":"<p>Most of us think linearly, that is step by step, from start to finish. Think about making something physical, like a wooden table. Your mind quickly starts creating steps leading to that goal. Something like this feels familiar?<\/p>\n<ol>\n<li>Buy lots of raw wood.<\/li>\n<li>Buy sandpaper.<\/li>\n<li>Buy varnish.<\/li>\n<li>Make four legs.<\/li>\n<li>Make table top.<\/li>\n<li>Apply sandpaper.<\/li>\n<li>Join it all up.<\/li>\n<li>Apply varnish.<\/li>\n<\/ol>\n<p><!--more--><\/p>\n<p>Sorry, I&#8217;m no expert in the art of table making but most of you would have a similar list of actions. This is linear, meaning that the table cannot be used at all unless all the steps are complete. If you don&#8217;t have a dining table at all in your house, then you&#8217;ll probably have to sit somewhere else to eat for a couple of days (depending on how good you are at making a wooden table).<\/p>\n<p>Now think about writing software, say a website to help out your friend sell hats for cats online. Automatically you&#8217;ll start thinking:<\/p>\n<ol>\n<li>Design frontend.<\/li>\n<li>Develop backend.<\/li>\n<li>Implement frontend.<\/li>\n<li>Integrate payment systems.<\/li>\n<li>Integrate email notification service.<\/li>\n<li>Test.<\/li>\n<li>Deploy.<\/li>\n<li>Handover.<\/li>\n<\/ol>\n<p>There&#8217;s probably a lot of intermediary steps but this is the gist of it all. Again, if you look through carefully, your friend does not get her website unless all of the steps are complete.<\/p>\n<p>Iterative development only means that instead of doing everything in linear sequential steps, you are doing it in cycles. Delivery is key here meaning that you should aim to deliver at least once by the end of each cycle but mature teams are able to deliver the product more than twice each cycle. Thinking in line with the Scrum methodology, each cycle (or sprint) can be one week or up to one month. In our case, if we choose that one cycle is one week, here&#8217;s what it might look like:<\/p>\n<ol>\n<li>Cycle 1:\n<ol>\n<li>Design basic frontend.<\/li>\n<li>Implement mock backend.<\/li>\n<li>Implement basic frontend.<\/li>\n<li>Integrate mock payment system.<\/li>\n<li>Integrate mock email notification service.<\/li>\n<li>Deploy and deliver.<\/li>\n<\/ol>\n<\/li>\n<li>Cycle 2:\n<ol>\n<li>Improve frontend design.<\/li>\n<li>Implement basic backend functionality still mocking rest.<\/li>\n<li>Implement improved frontend design.<\/li>\n<li>Integrate mock payment system.<\/li>\n<li>Integrate mock email notification service.<\/li>\n<li>Deploy and deliver.<\/li>\n<\/ol>\n<\/li>\n<li>Cycle 3:\n<ol>\n<li>Implement more backend functionality.<\/li>\n<li>Revisit frontend designs.<\/li>\n<li>Implement frontend design changes.<\/li>\n<li>Integrate basic payment system.<\/li>\n<li>Integrate mock email notification service.<\/li>\n<li>Deploy and deliver.<\/li>\n<\/ol>\n<\/li>\n<li>Cycle 4:\n<ol>\n<li>Implement more backend functionality.<\/li>\n<li>Revisit frontend designs.<\/li>\n<li>Implement frontend design changes.<\/li>\n<li>Integrate basic email notification service.<\/li>\n<li>Deploy and deliver.<\/li>\n<\/ol>\n<\/li>\n<li>Cycle 5:\n<ol>\n<li>Implement more backend functionality.<\/li>\n<li>Integrate advanced payment system.<\/li>\n<li>Deploy and deliver.<\/li>\n<\/ol>\n<\/li>\n<li>Cycle 6:\n<ol>\n<li>Implement more backend functionality.<\/li>\n<li>Revisit frontend designs.<\/li>\n<li>Implement frontend design changes.<\/li>\n<li>Integrate advanced email notification service.<\/li>\n<li>Deploy and deliver.<\/li>\n<\/ol>\n<\/li>\n<\/ol>\n<p>Here, you are deploying and delivering the product to your friend every week so that she can inspect the whole flow. This also allows you to receive feedback regarding the entire product every week which is not possible in the linear way of doing things (as your product is not ready for your friend to look at until the end). Also you don&#8217;t have to come up with all the cycles at once. You basically have to have the current cycle plus at least next two cycles within your sights.<\/p>\n<p>Mocking things is very important in this flow as it helps your friend to &#8220;feel&#8221; how the website will look at the end. The mocks allow her to go through the checkout flow as if it was actually there. It might seem like a waste of time as you are developing something that you know will get replaced at the end anyway, it still has tremendous value as there is nothing worse than the client changing her mind and wanting to go in a completely different direction towards the end of the project. The mocks allow the client to focus as they get to see the product as it is being developed while also giving you continuous feedback.<\/p>\n<p>While this was a simple example, it gets quite a bit complicated when new projects are undertaken in large scale organisations as there can be lack of clarity before a project is started. I&#8217;ll do another post on the Scrum Framework for this sometime in near future but this post provides a base that can help you understand Scrum as it is also about iterative software development.<\/p>\n<p>Again, if you observe the technological advancements that humans has achieved so far, you&#8217;ll notice the iterative approach at large scale. We didn&#8217;t aspire to build shiny cars from day one. We first started with the wheel, advanced to wheel barrow, horse cart, trains and then finally a built car.<\/p>\n<p>Hope you enjoyed this article. I&#8217;ll appreciate it very much if you could leave your thoughts in the comments below.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 01 Sep 2015 12:30:26 +0000","created_by":1,"updated_at":"Thu, 27 Aug 2015 22:06:59 +0000","updated_by":1,"published_at":"Tue, 01 Sep 2015 12:30:26 +0000","published_by":1},{"id":238,"title":"Understanding the default project structure (Express series 2)","slug":"understanding-the-default-project-structure-express-series-2","markdown":"\nNow that we have a basic express web app created, lets go through the project structure.\n\n**package.json**  \n In the last post we briefly went through the purpose of package.json. Now its time to go through it in bit more detail. As the extension `.json` indicates, data in this file is in `json` format. Yours should look something like this:\n\n```\n{<br><\/br>\n\"name\": \"myawesomeproject\",<br><\/br>\n\"version\": \"0.0.0\",<br><\/br>\n\"private\": true,<br><\/br>\n\"scripts\": {<br><\/br>\n\"start\": \"node .\/bin\/www\"<br><\/br>\n},<br><\/br>\n\"dependencies\": {<br><\/br>\n\"body-parser\": \"~1.12.0\",<br><\/br>\n\"cookie-parser\": \"~1.3.4\",<br><\/br>\n\"debug\": \"~2.1.1\",<br><\/br>\n\"express\": \"~4.12.2\",<br><\/br>\n\"jade\": \"~1.9.2\",<br><\/br>\n\"morgan\": \"~1.5.1\",<br><\/br>\n\"serve-favicon\": \"~2.2.0\"<br><\/br>\n}```\n\nThe name and version fields indicate the project name and version respectively. The field private indicates that this is a private npm project. So, in case you were developing a npm module, this field will determine whether or not to allow you to publicly publish your project. The scripts field contains commands and the respective files that should be executed when the command is called. The two most popular ones here are start and test. As you can see, the express tool automatically generates the start script command and maps it to .\/bin\/www.\n\nThe last thing here is the dependencies field. This contains all the dependencies of your project. The tilda sign (`~`) prefixed in the version means that if a newer bug fix version is found then it\u2019ll update it to that automatically. For instance, in our case, the current version of express module is 4.12.2. In case a newer version 4.12.3 is released, the tilda sign means that npm will fetch that version. However, if express 4.13 is released, npm won\u2019t update to that. On the contrary, if the hat sign (`^`) was used, npm will update to 4.13 but will hold off for version 5.0.\n\n**app.js**  \n This file is the central point for configuring the express web server. It defines several things like the view engine, logging levels etc and loads all middleware components (more on that in later posts). You\u2019ll be visiting this file more often than you think once you start developing express web applications.\n\nIf you scroll down, you\u2019ll see that this file also loads routes. This is where the server routes are defined. In this case, the route file `index.js` is loaded in and all its routes are mapped from the root route (indicated by `\/`). In the next line it also loads another route file called `users.js`, routes of which are mapped from `\/users`.\n\n**routes**  \n This directory contains files that define different routes of the web application. You can define routes in multiple files here and then reference them in the `app.js` in parent folder. This helps to keep things separate from one another to avoid clutter. Although JavaScript has no concept of a \u201cclass\u201d, you can define various objects here and then re-use them across your various routes. We\u2019ll go over that in later posts.\n\n**views**  \n This directory contains all your templates. Since the default templating engine is Jade, you\u2019ll find files with `.jade` extension here. You can use one of the other options that express tool provides like ejs or handlebars or you can choose a completely different templating engine as well. Whatever you choose will have to be configured in the `app.js` file. The templates that have been defined here can be changed on the fly, meaning that changing the template while the application is running will have immediate effect. In contrast to this, if you change one of the javascript files, you\u2019ll have to restart the whole application. Check out Jade templating engine here [http:\/\/jade-lang.com\/](http:\/\/jade-lang.com\/)\n\n**public**  \n As the name suggests, this directory contains all the public assets. Whatever you place in here can be accessed without authentication as these are the assets that pages normally require to render. Any files here can be referenced from root level. For instance, a stylesheet named `style.css` located in stylesheets folder in public directory can be accessed by `\/stylesheets\/style.css` from the web server. You can create as many sub directories as you want without making any configuration changes in `app.js`.\n\nExcellent! Now you know what different parts of an express web app does. In the next post we\u2019ll create our first RESTful web service using Express. Excited?\n\n\n","html":"<p>Now that we have a basic express web app created, lets go through the project structure.<\/p>\n<p><strong>package.json<\/strong><br \/>\nIn the last post we briefly went through the purpose of package.json. Now its time to go through it in bit more detail. As the extension <code>.json<\/code> indicates, data in this file is in <code>json<\/code> format. Yours should look something like this:<!--more--><\/p>\n<p><code>{<br \/>\n\"name\": \"myawesomeproject\",<br \/>\n\"version\": \"0.0.0\",<br \/>\n\"private\": true,<br \/>\n\"scripts\": {<br \/>\n\"start\": \"node .\/bin\/www\"<br \/>\n},<br \/>\n\"dependencies\": {<br \/>\n\"body-parser\": \"~1.12.0\",<br \/>\n\"cookie-parser\": \"~1.3.4\",<br \/>\n\"debug\": \"~2.1.1\",<br \/>\n\"express\": \"~4.12.2\",<br \/>\n\"jade\": \"~1.9.2\",<br \/>\n\"morgan\": \"~1.5.1\",<br \/>\n\"serve-favicon\": \"~2.2.0\"<br \/>\n}<\/code><\/p>\n<p>The name and version fields indicate the project name and version respectively. The field private indicates that this is a private npm project. So, in case you were developing a npm module, this field will determine whether or not to allow you to publicly publish your project. The scripts field contains commands and the respective files that should be executed when the command is called. The two most popular ones here are start and test. As you can see, the express tool automatically generates the start script command and maps it to .\/bin\/www.<\/p>\n<p>The last thing here is the dependencies field. This contains all the dependencies of your project. The tilda sign (<code>~<\/code>) prefixed in the version means that if a newer bug fix version is found then it&#8217;ll update it to that automatically. For instance, in our case, the current version of express module is 4.12.2. In case a newer version 4.12.3 is released, the tilda sign means that npm will fetch that version. However, if express 4.13 is released, npm won&#8217;t update to that. On the contrary, if the hat sign (<code>^<\/code>) was used, npm will update to 4.13 but will hold off for version 5.0.<\/p>\n<p><strong>app.js<\/strong><br \/>\nThis file is the central point for configuring the express web server. It defines several things like the view engine, logging levels etc and loads all middleware components (more on that in later posts). You&#8217;ll be visiting this file more often than you think once you start developing express web applications.<\/p>\n<p>If you scroll down, you&#8217;ll see that this file also loads routes. This is where the server routes are defined. In this case, the route file <code>index.js<\/code> is loaded in and all its routes are mapped from the root route (indicated by <code>\/<\/code>). In the next line it also loads another route file called <code>users.js<\/code>, routes of which are mapped from <code>\/users<\/code>.<\/p>\n<p><strong>routes<\/strong><br \/>\nThis directory contains files that define different routes of the web application. You can define routes in multiple files here and then reference them in the <code>app.js<\/code> in parent folder. This helps to keep things separate from one another to avoid clutter. Although JavaScript has no concept of a &#8220;class&#8221;, you can define various objects here and then re-use them across your various routes. We&#8217;ll go over that in later posts.<\/p>\n<p><strong>views<\/strong><br \/>\nThis directory contains all your templates. Since the default templating engine is Jade, you&#8217;ll find files with <code>.jade<\/code> extension here. You can use one of the other options that express tool provides like ejs or handlebars or you can choose a completely different templating engine as well. Whatever you choose will have to be configured in the <code>app.js<\/code> file. The templates that have been defined here can be changed on the fly, meaning that changing the template while the application is running will have immediate effect. In contrast to this, if you change one of the javascript files, you&#8217;ll have to restart the whole application. Check out Jade templating engine here <a href=\"http:\/\/jade-lang.com\/\">http:\/\/jade-lang.com\/<\/a><\/p>\n<p><strong>public<\/strong><br \/>\nAs the name suggests, this directory contains all the public assets. Whatever you place in here can be accessed without authentication as these are the assets that pages normally require to render. Any files here can be referenced from root level. For instance, a stylesheet named <code>style.css<\/code> located in stylesheets folder in public directory can be accessed by <code>\/stylesheets\/style.css<\/code> from the web server. You can create as many sub directories as you want without making any configuration changes in <code>app.js<\/code>.<\/p>\n<p>Excellent! Now you know what different parts of an express web app does. In the next post we&#8217;ll create our first RESTful web service using Express. Excited?<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 02 Sep 2015 18:32:07 +0000","created_by":1,"updated_at":"Tue, 01 Sep 2015 21:50:14 +0000","updated_by":1,"published_at":"Wed, 02 Sep 2015 18:32:07 +0000","published_by":1},{"id":244,"title":"Creating a RESTful API (Express series 3)","slug":"creating-a-restful-api-express-series-3","markdown":"\nSo in the last post we went over the project structure. This time, we\u2019ll actually modify some of the code to create our own RESTful web service.\n\nIf you don\u2019t know what a RESTful web service is, you should check out the [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer) entry for it.\n\nAs discussed in the previous post, all of our routes are defined in files located in the routes folder which are then referenced and collated in the app.js file. So, to create our own RESTful API, we need to do something in this area.\n\nLets checkout the `index.js` file and see whats in it.\n\nvar express = require('express'); var router = express.Router(); \/* GET home page. *\/ router.get('\/', function(req, res, next) { res.render('index', { title: 'Express' }); }); module.exports = router;\n\nThis is the typical structure of any nodejs module. Top section is to declare dependencies and instantiate objects. Middle part is to actually do things. And the final bottom part is to clean up and export the module. This file says that when the root (`\/`) is accessed by a HTTP GET method (indicated by `router.get`), call a function. This function is defined as `function(req, res, next) {...}`. The three variables, `req`, `res` and `next` are function parameters. I won\u2019t go into the details regarding basic JavaScript in any of these tutorials. If you have any questions, check out some of many JavaScript resources available on the web.\n\nThese parameters `req`, `res` and `next` are the request object, the response object and the function call to the next module in the middle ware stack respectively. The line `res.render('index', { title: 'Express' });` is using the render function of the response object, asking express to render the template `'index'` with parameters specified by the JSON object `{ title: 'Express' }`. Since Express knows that we are using Jade templating engine (defined in `app.js`), it will go to the views folder and find the file named `index.jade` and will render it.\n\nRight, so now that\u2019s out of the way, lets go and create our own service. For the purpose of this tutorial, we\u2019ll create a health check web service. When it gets called, it will return `OK` status indicating that the server health is ok. Since this will require a GET request to be invoked, we\u2019ll be using router.get and hence the first line of the code will be very similar to the already existing one.\n\nrouter.get('\/', function(req, res, next) { \/\/ My health check API });\n\n``Next, we need to change two things. One is the URL reference. Currently, its set to `'\/'`. Because this has already been defined, lets modify it to our own path. This can be anything you want but for the sake of simplicity, lets make it `\/healthcheck`. Your code should now look like this:\n\nrouter.get('\/healthcheck', function(req, res, next) { \/\/ My health check API });\n\n``The code doesn\u2019t do anything yet. Lets change that. Remove the comment `<strong>\/\/ My health check API<\/strong>` and put the following logic in it.\n\nvar statusString = 'OK'; return res.send(statusString);\n\n``After your change, the file `index.js` should look like this:\n\nvar express = require('express'); var router = express.Router(); \/* GET home page. *\/ router.get('\/', function(req, res, next) { res.render('index', { title: 'Express' }); }); router.get('\/healthcheck', function(req, res, next) { var statusString = 'OK'; return res.send(statusString); }); module.exports = router;\n\nAwesome. Now go back to your command line and run `npm start` command to start your server. Once started, go to `http:\/\/localhost:3000\/healthcheck` on your web browser. You should see the following output:\n\n`OK`\n\nNow this isn\u2019t very clean as its returning plain text instead of proper JSON response. Shut down the server and go back to your IDE. Modify the code in the `\/healthcheck` router in `index.js` to be the following:\n\nvar statusObject = {status: 'OK'}; return res.send(statusObject);\n\n``Start the server and navigate back to `http:\/\/localhost:3000\/healthcheck` in your browser. You should see a JSON response this time:\n\n{ \"status\": \"OK\" }\n\nGreat work! You\u2019ve just written your first RESTful web service in Node.js using Express! In the next post we\u2019ll take this to a next level and migrate what we\u2019ve written to a separate file to reduce clutter and make the code a bit cleaner. Enjoy!\n\n\n","html":"<p>So in the last post we went over the project structure. This time, we&#8217;ll actually modify some of the code to create our own RESTful web service.<\/p>\n<p>If you don&#8217;t know what a RESTful web service is, you should check out the <a href=\"https:\/\/en.wikipedia.org\/wiki\/Representational_state_transfer\" target=\"_blank\">wikipedia<\/a> entry for it.<\/p>\n<p>As discussed in the previous post, all of our routes are defined in files located in the routes folder which are then referenced and collated in the app.js file. So, to create our own RESTful API, we need to do something in this area.<\/p>\n<p>Lets checkout the <code>index.js<\/code> file and see whats in it.<!--more--><\/p>\n<pre class=\"lang:js decode:true \">var express = require('express');\r\nvar router = express.Router();\r\n\r\n\/* GET home page. *\/\r\nrouter.get('\/', function(req, res, next) {\r\nres.render('index', { title: 'Express' });\r\n});\r\n\r\nmodule.exports = router;<\/pre>\n<p>This is the typical structure of any nodejs module. Top section is to declare dependencies and instantiate objects. Middle part is to actually do things. And the final bottom part is to clean up and export the module. This file says that when the root (<code>\/<\/code>) is accessed by a HTTP GET method (indicated by <code>router.get<\/code>), call a function. This function is defined as <code>function(req, res, next) {...}<\/code>. The three variables, <code>req<\/code>, <code>res<\/code> and <code>next<\/code> are function parameters. I won&#8217;t go into the details regarding basic JavaScript in any of these tutorials. If you have any questions, check out some of many JavaScript resources available on the web.<\/p>\n<p>These parameters <code>req<\/code>, <code>res<\/code> and <code>next<\/code> are the request object, the response object and the function call to the next module in the middle ware stack respectively. The line <code>res.render('index', { title: 'Express' });<\/code> is using the render function of the response object, asking express to render the template <code>'index'<\/code> with parameters specified by the JSON object <code>{ title: 'Express' }<\/code>. Since Express knows that we are using Jade templating engine (defined in <code>app.js<\/code>), it will go to the views folder and find the file named <code>index.jade<\/code> and will render it.<\/p>\n<p>Right, so now that&#8217;s out of the way, lets go and create our own service. For the purpose of this tutorial, we&#8217;ll create a health check web service. When it gets called, it will return <code>OK<\/code> status indicating that the server health is ok. Since this will require a GET request to be invoked, we&#8217;ll be using router.get and hence the first line of the code will be very similar to the already existing one.<\/p>\n<pre class=\"lang:js decode:true \">router.get('\/', function(req, res, next) {\r\n\/\/ My health check API\r\n});<\/pre>\n<p><code><\/code>Next, we need to change two things. One is the URL reference. Currently, its set to <code>'\/'<\/code>. Because this has already been defined, lets modify it to our own path. This can be anything you want but for the sake of simplicity, lets make it <code>\/healthcheck<\/code>. Your code should now look like this:<\/p>\n<pre class=\"lang:js decode:true \">router.get('\/healthcheck', function(req, res, next) {\r\n\/\/ My health check API\r\n});<\/pre>\n<p><code><\/code>The code doesn&#8217;t do anything yet. Lets change that. Remove the comment <code><strong>\/\/ My health check API<\/strong><\/code> and put the following logic in it.<\/p>\n<pre class=\"lang:js decode:true \">var statusString = 'OK';\r\nreturn res.send(statusString);<\/pre>\n<p><code><\/code>After your change, the file <code>index.js<\/code> should look like this:<\/p>\n<pre class=\"lang:js decode:true \">var express = require('express');\r\nvar router = express.Router();\r\n\r\n\/* GET home page. *\/\r\nrouter.get('\/', function(req, res, next) {\r\nres.render('index', { title: 'Express' });\r\n});\r\n\r\nrouter.get('\/healthcheck', function(req, res, next) {\r\nvar statusString = 'OK';\r\nreturn res.send(statusString);\r\n});\r\n\r\nmodule.exports = router;<\/pre>\n<p>Awesome. Now go back to your command line and run <code>npm start<\/code> command to start your server. Once started, go to <code>http:\/\/localhost:3000\/healthcheck<\/code> on your web browser. You should see the following output:<\/p>\n<p><code>OK<\/code><\/p>\n<p>Now this isn&#8217;t very clean as its returning plain text instead of proper JSON response. Shut down the server and go back to your IDE. Modify the code in the <code>\/healthcheck<\/code> router in <code>index.js<\/code> to be the following:<\/p>\n<pre class=\"lang:js decode:true \">var statusObject = {status: 'OK'};\r\nreturn res.send(statusObject);<\/pre>\n<p><code><\/code>Start the server and navigate back to <code>http:\/\/localhost:3000\/healthcheck<\/code> in your browser. You should see a JSON response this time:<\/p>\n<pre class=\"lang:default decode:true \">{\r\n\r\n\"status\": \"OK\"\r\n\r\n}<\/pre>\n<p>Great work! You&#8217;ve just written your first RESTful web service in Node.js using Express! In the next post we&#8217;ll take this to a next level and migrate what we&#8217;ve written to a separate file to reduce clutter and make the code a bit cleaner. Enjoy!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 03 Sep 2015 19:08:53 +0000","created_by":1,"updated_at":"Sun, 10 Jan 2016 14:04:28 +0000","updated_by":1,"published_at":"Thu, 03 Sep 2015 19:08:53 +0000","published_by":1},{"id":252,"title":"The Future of Webapps 2015","slug":"temp-slug-57","markdown":"\nSo in the last two days, I attended the future of webapps and I have to say, it was probably one of the most engaging conferences I have attended. It was so good that I think every frontend developer should attend it at least once.\n\nThe conference touched on various web related topics, the past, present and of course, the future. It highlighted various trends that we\u2019re all seeing but haven\u2019t been quite able to make anything of it. It answered most of the questions that I had but also left me with a couple of questions for myself to find and answer.\n\n\n","html":"<p>So in the last two days, I attended the future of webapps and I have to say, it was probably one of the most engaging conferences I have attended. It was so good that I think every frontend developer should attend it at least once.<\/p>\n<p>The conference touched on various web related topics, the past, present and of course, the future. It highlighted various trends that we&#8217;re all seeing but haven&#8217;t been quite able to make anything of it. It answered most of the questions that I had but also left me with a couple of questions for myself to find and answer.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 08 Oct 2015 13:33:37 +0000","created_by":1,"updated_at":"Thu, 08 Oct 2015 13:33:37 +0000","updated_by":1,"published_at":"","published_by":1},{"id":255,"title":"Notes from Angular Connect 2015 Keynote","slug":"notes-from-angular-connect-2015-keynote","markdown":"\n0940\n\nDay One.\n\nKeynote.\n\nMakes it simpler to render stuff by pre rendering everything by adding a build step.\n\nAngular 2 alpha build blows every other mvvm framework out of the water.\n\nAngular 2 drops a lot of current directives by introducing property binding which allows binding models directly to properties like value. Drops about 43 directives.\n\nTemplate directive plus controller in angular 1 is now a component in angular 2. Use @Component from typescript.\n\nEs5\/6, typescript and Dart are supported for using Angular 2.\n\nES6, typescript and Dart, all need compilation. Typescript and Dart introduces types.\n\nDart does not have a Javascript syntax.\n\nAngular 2 is written in typescript.\n\nIncremental releases. About 57 in last two weeks.\n\nIgor minar now presents tooling in AngularJS\n\nConfiguration is a major barrier to tool adoption.\n\nGithub.com\/angular\/angular-cli\n\nStart a project by  \n ng new greetings-ac\n\nCreates initial project scaffolding. Creates a build pipeline based on Typescript.\n\nStart a project and see live reload. Helpful error messages with stack trace right in the browser.\n\nUse  \n ng generate component componentname\n\nTo generate new component files including css, template and unit test spec files.\n\nDeploy to github by  \n ng github-pages:deploy\n\nTo deploy directly to github pages.\n\nNow talking about angular batarang. Allows inspecting bugs in chrome.\n\nNew called Batarangle.  \n rangle.io\/batarangle\n\nNow talking about Ionic Framework, Max Lynch and Adam Bradley.\n\nAbout 1.2 million Ionic and angular apps since 2\/2014. Used by startups to the fortune 50.\n\nWorking with angular 2 to optimise Ionic architecture to be ready for adoption.\n\nReduce barrier to entry by allowing mostly ES2015\/Typescript\n\nOverhaul in the navigation system allowing apps to push boundaries.\n\nMaterial design support. New animation and scrolling. Powerful theming.\n\nIonic 2 is ready to use today.  \n ionic.io\/2\n\nReady enough to use but be warned of bugs.\n\nGiving out Angular themed Google Cardboard.\n\nNow talking about NativeScript, Dimo Iliev.\n\nDifferent way of thinking about web. Expectations of native applications few different from applications that run in the browser. How do you go native without actually going native.\n\nUse what we know, I. E. Express, npm, javascript, Angular, Typescript to write native applications that run inside a Javascript virtual machine. Note, not in a browser but in a Javascript virtual machine. Use angular 2 components.\n\nDemo looks awesome where a native list view is instantiated via javascript\/css. Extensively uses Typescript and angular 2 components.\n\n\u201cI wonder how this fares compared to Ionic.\u201c\n\nCombining xml template with data to create native view. Same technology used in angular universe to render dom more efficiently.\n\nNow talking about Path to Angular 2.\n\nAngular Upgrade project in works, hoping to make it simpler to convert the existing application piece by piece whilst it\u2019s live.\n\nNow talking about successful angular ecosystem, Jules Kremer.\n\nAngular being open source has helped its adoption. Collaboration with Typescript and visual studio code team to provide better tooling.\n\nyo office  \n Command to build scaffolding for apps providing office integration.\n\nOh my god. Angular 2 will support IE 9!\n\nContributing to Angular is important as it helps shape the future.\n\nNow talking about Angular 2 release status.\n\nGreentea team building Angular 2. 70k devs, 300k LOC.\n\nLots of Google projects porting to Angular 2. The dev team is using the feedback from people porting to Angular 2 to speedup development.\u00a0\n\nNow talking about deleting cancer.\n\nLaunched a general public appeal asking for everyone globally to register as a stem cell donor. Quick and easy one time action. Could save someone\u2019s life if a match is found.\n\nMargot foundation is entirely volunteer based foundation.\n\n\n","html":"<p>0940<\/p>\n<p>Day One.<\/p>\n<p>Keynote. <\/p>\n<p>Makes it simpler to render stuff by pre rendering everything by adding a build step. <\/p>\n<p>Angular 2 alpha build blows every other mvvm framework out of the water. <\/p>\n<p>Angular 2 drops a lot of current directives by introducing property binding which allows binding models directly to properties like value. Drops about 43 directives. <\/p>\n<p>Template directive plus controller in angular 1 is now a component in angular 2. Use @Component from typescript. <\/p>\n<p>Es5\/6, typescript and Dart are supported for using Angular 2.<\/p>\n<p><!--more--><\/p>\n<p>ES6, typescript and Dart, all need compilation. Typescript and Dart introduces types. <\/p>\n<p>Dart does not have a Javascript syntax. <\/p>\n<p>Angular 2 is written in typescript. <\/p>\n<p>Incremental releases. About 57 in last two weeks. <\/p>\n<p>Igor minar now presents tooling in AngularJS <\/p>\n<p>Configuration is a major barrier to tool adoption. <\/p>\n<p>Github.com\/angular\/angular-cli <\/p>\n<p>Start a project by<br \/>\nng new greetings-ac <\/p>\n<p>Creates initial project scaffolding. Creates a build pipeline based on Typescript. <\/p>\n<p>Start a project and see live reload. Helpful error messages with stack trace right in the browser. <\/p>\n<p>Use<br \/>\nng generate component componentname <\/p>\n<p>To generate new component files including css, template and unit test spec files. <\/p>\n<p>Deploy to github by<br \/>\nng github-pages:deploy <\/p>\n<p>To deploy directly to github pages.<\/p>\n<p>Now talking about angular batarang. Allows inspecting bugs in chrome. <\/p>\n<p>New called Batarangle.<br \/>\nrangle.io\/batarangle <\/p>\n<p>Now talking about Ionic Framework, Max Lynch and Adam Bradley. <\/p>\n<p>About 1.2 million Ionic and angular apps since 2\/2014. Used by startups to the fortune 50.<\/p>\n<p>Working with angular 2 to optimise Ionic architecture to be ready for adoption. <\/p>\n<p>Reduce barrier to entry by allowing mostly ES2015\/Typescript <\/p>\n<p>Overhaul in the navigation system allowing apps to push boundaries. <\/p>\n<p>Material design support. New animation and scrolling. Powerful theming. <\/p>\n<p>Ionic 2 is ready to use today.<br \/>\nionic.io\/2<\/p>\n<p>Ready enough to use but be warned of bugs. <\/p>\n<p>Giving out Angular themed Google Cardboard. <\/p>\n<p>Now talking about NativeScript, Dimo Iliev. <\/p>\n<p>Different way of thinking about web. Expectations of native applications few different from applications that run in the browser. How do you go native without actually going native. <\/p>\n<p>Use what we know, I. E. Express, npm, javascript, Angular, Typescript to write native applications that run inside a Javascript virtual machine. Note, not in a browser but in a Javascript virtual machine. Use angular 2 components. <\/p>\n<p>Demo looks awesome where a native list view is instantiated via javascript\/css. Extensively uses Typescript and angular 2 components. <\/p>\n<p>\u201cI wonder how this fares compared to Ionic.\u201c<\/p>\n<p>Combining xml template with data to create native view. Same technology used in angular universe to render dom more efficiently. <\/p>\n<p>Now talking about Path to Angular 2.<\/p>\n<p>Angular Upgrade project in works, hoping to make it simpler to convert the existing application piece by piece whilst it&#8217;s live. <\/p>\n<p>Now talking about successful angular ecosystem, Jules Kremer. <\/p>\n<p>Angular being open source has helped its adoption. Collaboration with Typescript and visual studio code team to provide better tooling. <\/p>\n<p>yo office<br \/>\nCommand to build scaffolding for apps providing office integration. <\/p>\n<p>Oh my god. Angular 2 will support IE 9!<\/p>\n<p>Contributing to Angular is important as it helps shape the future. <\/p>\n<p>Now talking about Angular 2 release status. <\/p>\n<p>Greentea team building Angular 2. 70k devs, 300k LOC. <\/p>\n<p>Lots of Google projects porting to Angular 2. The dev team is using the feedback from people porting to Angular 2 to speedup development.&nbsp; <\/p>\n<p>Now talking about deleting cancer. <\/p>\n<p>Launched a general public appeal asking for everyone globally to register as a stem cell donor. Quick and easy one time action. Could save someone&#8217;s life if a match is found. <\/p>\n<p>Margot foundation is entirely volunteer based foundation.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 20 Oct 2015 10:50:16 +0000","created_by":1,"updated_at":"Wed, 21 Oct 2015 22:38:42 +0000","updated_by":1,"published_at":"Tue, 20 Oct 2015 10:50:16 +0000","published_by":1},{"id":257,"title":"Notes from What's new in Typescript? At Angular Connect 2015","slug":"notes-from-whats-new-in-typescript-at-angular-connect-2015","markdown":"\n**1100**\n\nDay One\n\nWhat\u2019s new in Typescript? By Bill Ticehurst\n\nGoals is to make javascript development more productive and enjoyable.\n\nHard to work in code bases with lack of types. Hard to debug.\n\nSuperset of javascript that compiles into plain javascript. Simple mapping. Makes it easy to debug, back and forth.\n\nES 6 sounds futuristic. Once you get used to it, it\u2019s hard to go back.\n\nStatic types allow better documentation which in turn gives better tooling. Allows jumping around points in code easier.\n\nTypes are optional. Don\u2019t need to have types everywhere. Kinda like code coverage. Do what you need.\n\nNow showing demos. There\u2019s a plugin available for sublime which helps writing Typescript easier. Now urging audience to find bugs in presented demo code. Interfaces help debug code as it sets expectations for assignable types.\n\nLive stack trace. Bugs and trace disappears as you fix them. How cool is that?\n\nDebugging in this thing is amazing. Errors are helpful and intellisense is better as the code knows things!\n\nA variable can have more than one type. Like string or a string array. In this case, the Typescript expects common methods in both types to be valid. If specific methods needs to be used then need to cast or parse it.\n\nGenerics!\n\n\u201cthis demo has blown my mind.\u201d\n\nNow showing a demo with angular.\n\nImports works dynamically with node modules. It just works!\n\nUrging developers to start distributing Typescript files along with their modules.\n\nDecorators are able to interfere with the declaration of an object. Logger decorator injects function before execution of the target function.\n\nReflection API. Decorators don\u2019t modify the object but adds metadata using the reflection API.\n\nReflect.getmetadata(\u201cannotations\u201d, varname)\n\nEmbed html code directly in javascript in jsx files. Typescript allows autocompletion in jsx files for both, html and Typescript \/ javascript. Coolest thing I\u2019ve ever seen.\n\nPowerful project wide find references tooling + variable name refactoring. \u201cis this available in all Typescript plugins?\u201d\n\nES 6 async and await looks incredibly powerful way to control async functions execution flow.\n\nawait Promise.all(arrayOfPromises)\n\nWaits for all promises to be fulfilled before executing any of the remaining code.\n\n\n","html":"<p><strong>1100<\/strong><\/p>\n<p>Day One<\/p>\n<p>What&#8217;s new in Typescript? By Bill Ticehurst <\/p>\n<p>Goals is to make javascript development more productive and enjoyable. <\/p>\n<p>Hard to work in code bases with lack of types. Hard to debug. <\/p>\n<p><!--more--><\/p>\n<p>Superset of javascript that compiles into plain javascript. Simple mapping. Makes it easy to debug, back and forth. <\/p>\n<p>ES 6 sounds futuristic. Once you get used to it, it&#8217;s hard to go back. <\/p>\n<p>Static types allow better documentation which in turn gives better tooling. Allows jumping around points in code easier. <\/p>\n<p>Types are optional. Don&#8217;t need to have types everywhere. Kinda like code coverage. Do what you need. <\/p>\n<p>Now showing demos. There&#8217;s a plugin available for sublime which helps writing Typescript easier. Now urging audience to find bugs in presented demo code. Interfaces help debug code as it sets expectations for assignable types. <\/p>\n<p>Live stack trace. Bugs and trace disappears as you fix them. How cool is that? <\/p>\n<p>Debugging in this thing is amazing. Errors are helpful and intellisense is better as the code knows things! <\/p>\n<p>A variable can have more than one type. Like string or a string array. In this case, the Typescript expects common methods in both types to be valid. If specific methods needs to be used then need to cast or parse it. <\/p>\n<p>Generics! <\/p>\n<p>\u201cthis demo has blown my mind.\u201d<\/p>\n<p>Now showing a demo with angular.<\/p>\n<p>Imports works dynamically with node modules. It just works! <\/p>\n<p>Urging developers to start distributing Typescript files along with their modules. <\/p>\n<p>Decorators are able to interfere with the declaration of an object. Logger decorator injects function before execution of the target function. <\/p>\n<p>Reflection API. Decorators don&#8217;t modify the object but adds metadata using the reflection API. <\/p>\n<p>Reflect.getmetadata(\u201cannotations\u201d, varname) <\/p>\n<p>Embed html code directly in javascript in jsx files. Typescript allows autocompletion in jsx files for both, html and Typescript \/ javascript. Coolest thing I&#8217;ve ever seen. <\/p>\n<p>Powerful project wide find references tooling + variable name refactoring. \u201cis this available in all Typescript plugins?\u201d<\/p>\n<p>ES 6 async and await looks incredibly powerful way to control async functions execution flow. <\/p>\n<p>await Promise.all(arrayOfPromises) <\/p>\n<p>Waits for all promises to be fulfilled before executing any of the remaining code.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 20 Oct 2015 12:03:00 +0000","created_by":1,"updated_at":"Wed, 21 Oct 2015 22:38:37 +0000","updated_by":1,"published_at":"Tue, 20 Oct 2015 12:03:00 +0000","published_by":1},{"id":259,"title":"Notes from Full stack Angular 2 at Angular Connect 2015","slug":"notes-from-full-stack-angular-2-at-angular-connect-2015","markdown":"\n11:30\n\nDay one\n\nFull stack angular 2\n\nWhy full stack?\n\nWhy angular?\n\nBest tool for the job doesn\u2019t always win. Sometimes it\u2019s political driven.\n\nCommunication problems. Context switching adds unnecessary overhead.\n\nSame things needs to be implemented many times over and over again.\n\nCommittees, contention, context switching, code duplication. Problems in today\u2019s world. Time wasted is significant.\n\nChallenges to use same technology across whole stack. Javascript gets bad rep sometimes as it used to be used only in the web browser.\n\nES6 and ES7 plus angular 2 will make it more and more prominent for use in full stack.\n\nLots of features makes it powerful. Dependency injection is one of the examples.\n\nComplex flows like communicating from browser to server to database still results in code duplication, even with javascript.\n\nUse dependency injection, one way to eliminate dom references, making it faster.\n\nStill DI is not ideal as DI adds complexity overhead.\n\nDemo is showing DI in action, switching implementation quickly by only modifying one line of code.\n\n@injectable annotation makes it easier to use DI.\n\nNow talking about Angular 2 Universal.\n\nWrite client side app and just install the server side plugin to allow easy access via model. Basically converting a client side application to do server rendering.\n\nExample showing conversion of client side application into server side.\n\nImporting client side main angular app file on server and then setting it as model object to pass on to render method for rendering index view.\n\nUsing ng2engine module from angular2-universal-preview module. Setting instance of that as app engine for all .ng2.html files and then binding ng2.engine as view engine in express app.\n\nNow reloading the server. Now the view is rendered on the server side!\n\nWorking towards supporting more server based environments.\n\nFull stack angular 2 transforms full stack Web development.\n\nbit.ly\/ng2stack\n\nFullstackangular2.com\n\n@jeffwhelpley @gdi2290\n\n\n","html":"<p>11:30<\/p>\n<p>Day one <\/p>\n<p>Full stack angular 2<\/p>\n<p>Why full stack? <\/p>\n<p>Why angular? <\/p>\n<p>Best tool for the job doesn&#8217;t always win. Sometimes it&#8217;s political driven. <\/p>\n<p>Communication problems. Context switching adds unnecessary overhead. <\/p>\n<p>Same things needs to be implemented many times over and over again. <\/p>\n<p>Committees, contention, context switching, code duplication. Problems in today&#8217;s world. Time wasted is significant. <\/p>\n<p><!--more--><\/p>\n<p>Challenges to use same technology across whole stack. Javascript gets bad rep sometimes as it used to be used only in the web browser. <\/p>\n<p>ES6 and ES7 plus angular 2 will make it more and more prominent for use in full stack. <\/p>\n<p>Lots of features makes it powerful. Dependency injection is one of the examples. <\/p>\n<p>Complex flows like communicating from browser to server to database still results in code duplication, even with javascript. <\/p>\n<p>Use dependency injection, one way to eliminate dom references, making it faster. <\/p>\n<p>Still DI is not ideal as DI adds complexity overhead. <\/p>\n<p>Demo is showing DI in action, switching implementation quickly by only modifying one line of code. <\/p>\n<p>@injectable annotation makes it easier to use DI. <\/p>\n<p>Now talking about Angular 2 Universal. <\/p>\n<p>Write client side app and just install the server side plugin to allow easy access via model. Basically converting a client side application to do server rendering. <\/p>\n<p>Example showing conversion of client side application into server side. <\/p>\n<p>Importing client side main angular app file on server and then setting it as model object to pass on to render method for rendering index view. <\/p>\n<p>Using ng2engine module from angular2-universal-preview module. Setting instance of that as app engine for all .ng2.html files and then binding ng2.engine as view engine in express app. <\/p>\n<p>Now reloading the server. Now the view is rendered on the server side! <\/p>\n<p>Working towards supporting more server based environments. <\/p>\n<p>Full stack angular 2 transforms full stack Web development. <\/p>\n<p>bit.ly\/ng2stack<\/p>\n<p>Fullstackangular2.com<\/p>\n<p>@jeffwhelpley @gdi2290<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 20 Oct 2015 13:06:20 +0000","created_by":1,"updated_at":"Wed, 21 Oct 2015 22:38:30 +0000","updated_by":1,"published_at":"Tue, 20 Oct 2015 13:06:20 +0000","published_by":1},{"id":261,"title":"Notes from Routing in eleven dimensions with Component Router at Angular Connect 2015","slug":"notes-from-routing-in-eleven-dimensions-with-component-router-at-angular-connect-2015","markdown":"\n12:00\n\nDay One.\n\nRouting in eleven dimensions with component router by Brian Ford\n\nRoute configuration. Mapping url to component.\n\nRoute configuration meta data decorator helps to do this in angular 2. Use path parameters style (same as express) to define variables in routes when mapping.\n\nInject route paramo service then in the target component to give access to those route params.\n\nUse square bracket router-link (property binding) directive to provide links between routers.\n\nFrom the RouteConfig, one can also link the routes by their id field (as attribute). If the route url has parameters then a map with parameters must be passed as second attribute to the property binding.\n\nThis new Link DSL is robust and survives refractors better than URLs. Also provides a safety net in case one of the parameters aren\u2019t met.\n\nChild routes allows embedding routes within routes. Basically nested routes.\n\nTo make this work, add route config with router outlet directive in the template. Denote if that route has child route by suffixing url with \/\u2026\n\nURLs to child routes are relative to the url of the parent route. They have their own parameters so don\u2019t need to worry about collisions. Could use id within parent route it the prefix of .\/ notation.\n\nNow talking about auxiliary routes. Each component can have zero or more auxiliary routes.\n\nURLs are  \n \/parent(parent-aux)\/child(child-aux)\n\nA modal is just an auxiliary routes. It can be thought of like tree of trees.\n\n\n","html":"<p>12:00<\/p>\n<p>Day One. <\/p>\n<p>Routing in eleven dimensions with component router by Brian Ford <\/p>\n<p>Route configuration. Mapping url to component. <\/p>\n<p>Route configuration meta data decorator helps to do this in angular 2. Use path parameters style (same as express) to define variables in routes when mapping. <\/p>\n<p>Inject route paramo service then in the target component to give access to those route params.<\/p>\n<p>Use square bracket router-link (property binding) directive to provide links between routers. <\/p>\n<p><!--more--><\/p>\n<p>From the RouteConfig, one can also link the routes by their id field (as attribute). If the route url has parameters then a map with parameters must be passed as second attribute to the property binding. <\/p>\n<p>This new Link DSL is robust and survives refractors better than URLs. Also provides a safety net in case one of the parameters aren&#8217;t met. <\/p>\n<p>Child routes allows embedding routes within routes. Basically nested routes. <\/p>\n<p>To make this work, add route config with router outlet directive in the template. Denote if that route has child route by suffixing url with \/\u2026 <\/p>\n<p>URLs to child routes are relative to the url of the parent route. They have their own parameters so don&#8217;t need to worry about collisions. Could use id within parent route it the prefix of .\/ notation. <\/p>\n<p>Now talking about auxiliary routes. Each component can have zero or more auxiliary routes. <\/p>\n<p>URLs are<br \/>\n\/parent(parent-aux)\/child(child-aux) <\/p>\n<p>A modal is just an auxiliary routes. It can be thought of like tree of trees.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 20 Oct 2015 13:12:48 +0000","created_by":1,"updated_at":"Wed, 21 Oct 2015 22:38:24 +0000","updated_by":1,"published_at":"Tue, 20 Oct 2015 13:12:48 +0000","published_by":1},{"id":263,"title":"Notes from Extreme Programming in a nutshell at Angular Connect 2015","slug":"notes-from-extreme-programming-in-a-nutshell-at-angular-connect-2015","markdown":"\n13:30\n\nDay one\n\nExtreme programming in a nutshell.\n\nExtreme programming has practices that keep you safe. A bunch of practices that act as a safe net against failure.\n\nOf all the practices, going to talk about pairing, test first and continuous delivery.\n\nStarting with pair programming. Similar to Pacific Rim.\n\nAll production code is developed by more than one person. When pairing, design decisions are discussed between pairs.\n\nTwo roles, navigator and worker. Switch pairs everyday to make sure that everybody is exposed to the code. Everyone is responsible for how the code works.\n\nMake best use of the team expertise. Use everybody\u2019s skills to the fullest and enable cross functional skills share. Allows sanity checking of ideas, avoiding rabbit holes.\n\nTaking pair programming to more extreme level. Mob programming. Taking the driver and navigator idea to a team. Whole team sits around a screen with one person with the keyboard.\n\nDriver types while team navigates. Run a timer to switch roles in and out. Kinda like coding dojo but do it in an open workspace. 3 or 4 people works the best. Fits nicely into size of ideal agile teams.\n\nThis means that if one of the person has to go to the meeting the work continues.\n\nUse spikes. Spikes are done solo. Mob might split up and then do spike of their own. Each spike is an investigation. Prove out an architecture or a design idea and then bring it in and explain it to the team. Code produced is thrown away. This is done to maintain code quality and not use something that was developed in a rush.\n\nDon\u2019t use code reviews. The process causes a longer delay between creation and release. Also causes bottleneck. Pair \/ mob programming removes the need of this.\n\nNow talking about Test First.\n\nGives confidence to deploy and refactor. Avoid manual mistakes. Keeps focus on what we are building now instead of other distractions on what it might be in future.\n\nCreate acceptance test first. Helps focus implementation. Make sure we only build what we need. Helps us with continuous delivery.\n\nNow talking about continuous delivery.\n\nDeploy several times a day. From implementation to live in a couple of hours. Keep development pipeline short.\n\nUse extensive monitoring to help track impact of releases. Measure various metrics, from hard metrics like CPU monitoring to actual speed of responses.\n\nAlways committing to master. Avoid problems by decouple releases from development. Code is always released but use feature toggles to turn new features on or off. Settings based on environments.\n\nToggles don\u2019t stay for forever. Once it\u2019s production ready and proven, it gets taken off and it\u2019s permanently on.\n\nUse mutex for deployments instead of ci pipeline. Use deployment tokens like a teddy bear to allow teams to deploy to production.\n\nNow top 3 tips even if you\u2019re not in extreme programming\n\n1. Add monitoring.  \n Use it even if you\u2019re not using continuous delivery. Even if code is not changed, the environment might change. Start with generic health check and then proceed to what you need.\n\n2. Test first bug fix.  \n Diagnose the issue and the \u00a0reproduce the steps. Once done, write a test that reproduces the issue.\n\n3. Pair code reviews  \n Review code while pairing. Eliminate bottlenecks.\n\nAdd practices from XP incrementally and see what works for you.\n\nTech.unruly.co\n\n\n","html":"<p>13:30<\/p>\n<p>Day one<\/p>\n<p>Extreme programming in a nutshell. <\/p>\n<p>Extreme programming has practices that keep you safe. A bunch of practices that act as a safe net against failure. <\/p>\n<p>Of all the practices, going to talk about pairing, test first and continuous delivery. <\/p>\n<p>Starting with pair programming. Similar to Pacific Rim. <\/p>\n<p><!--more--><\/p>\n<p>All production code is developed by more than one person. When pairing, design decisions are discussed between pairs. <\/p>\n<p>Two roles, navigator and worker. Switch pairs everyday to make sure that everybody is exposed to the code. Everyone is responsible for how the code works. <\/p>\n<p>Make best use of the team expertise. Use everybody&#8217;s skills to the fullest and enable cross functional skills share. Allows sanity checking of ideas, avoiding rabbit holes. <\/p>\n<p>Taking pair programming to more extreme level. Mob programming. Taking the driver and navigator idea to a team. Whole team sits around a screen with one person with the keyboard. <\/p>\n<p>Driver types while team navigates. Run a timer to switch roles in and out. Kinda like coding dojo but do it in an open workspace. 3 or 4 people works the best. Fits nicely into size of ideal agile teams. <\/p>\n<p>This means that if one of the person has to go to the meeting the work continues. <\/p>\n<p>Use spikes. Spikes are done solo. Mob might split up and then do spike of their own. Each spike is an investigation. Prove out an architecture or a design idea and then bring it in and explain it to the team. Code produced is thrown away. This is done to maintain code quality and not use something that was developed in a rush. <\/p>\n<p>Don&#8217;t use code reviews. The process causes a longer delay between creation and release. Also causes bottleneck. Pair \/ mob programming removes the need of this. <\/p>\n<p>Now talking about Test First. <\/p>\n<p>Gives confidence to deploy and refactor. Avoid manual mistakes. Keeps focus on what we are building now instead of other distractions on what it might be in future. <\/p>\n<p>Create acceptance test first. Helps focus implementation. Make sure we only build what we need. Helps us with continuous delivery. <\/p>\n<p>Now talking about continuous delivery. <\/p>\n<p>Deploy several times a day. From implementation to live in a couple of hours. Keep development pipeline short.<\/p>\n<p>Use extensive monitoring to help track impact of releases. Measure various metrics, from hard metrics like CPU monitoring to actual speed of responses. <\/p>\n<p>Always committing to master. Avoid problems by decouple releases from development. Code is always released but use feature toggles to turn new features on or off. Settings based on environments. <\/p>\n<p>Toggles don&#8217;t stay for forever. Once it&#8217;s production ready and proven, it gets taken off and it&#8217;s permanently on. <\/p>\n<p>Use mutex for deployments instead of ci pipeline. Use deployment tokens like a teddy bear to allow teams to deploy to production. <\/p>\n<p>Now top 3 tips even if you&#8217;re not in extreme programming <\/p>\n<p>1. Add monitoring.<br \/>\nUse it even if you&#8217;re not using continuous delivery. Even if code is not changed, the environment might change. Start with generic health check and then proceed to what you need. <\/p>\n<p>2. Test first bug fix.<br \/>\nDiagnose the issue and the \u00a0reproduce the steps. Once done, write a test that reproduces the issue. <\/p>\n<p>3. Pair code reviews<br \/>\nReview code while pairing. Eliminate bottlenecks. <\/p>\n<p>Add practices from XP incrementally and see what works for you. <\/p>\n<p>Tech.unruly.co<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 20 Oct 2015 14:09:43 +0000","created_by":1,"updated_at":"Wed, 21 Oct 2015 22:38:17 +0000","updated_by":1,"published_at":"Tue, 20 Oct 2015 14:09:43 +0000","published_by":1},{"id":266,"title":"Notes from Modern authentication solutions using OAuth 2.0, OpenID Connect and AngularJS at Angular Connect 2015","slug":"notes-from-modern-authentication-solutions-using-oauth-2-0-openid-connect-and-angularjs-at-angular-connect-2015","markdown":"\nDay two\n\n0945\n\nModern authentication solutions with oauth 2.0, openid connect and angularjs by Manfred Steyer\n\nOne client has too many accounts. One application is bound to many other applications. Lack of trust between applications.\n\nOauth was developed at twitter and ma.golia\n\nStandard for delegation of restricted rights.\n\nClient wants to access a resource server on the behalf of the user who owns the resource. The authorization server has access to the resource.\n\nClient sends the scope to the authorization server. The auth server then asks the user if the client should be granted permissions. The user then agrees if he wants to and then the auth server then sends the access token. Which the client then uses to get access to the restricted resources.\n\nAdvantage is the client doesn\u2019t have the password. The authorization server helps segregate duties and concerns between parties.\n\nFlows\n\nAuthorisation code grant is designed for server side applications.\n\nImplicit grant is designed for single page applications.\n\nNow talking about Implicit grant.\n\nOpenid connect defines how to use Oauth for authentication. Client gets id token which is a jwt token with information about the user. Can be signed by the issuer.\n\nId token is for the client while the access token is for the resource server.\n\nNow showing the demo.\n\nIt\u2019s a single page application that provides some vouchers. To buy the voucher the user has to be logged in.\n\nShowing the oauth network flow using fiddler.\n\n\n","html":"<p>Day two <\/p>\n<p>0945<\/p>\n<p>Modern authentication solutions with oauth 2.0, openid connect and angularjs by Manfred Steyer <\/p>\n<p>One client has too many accounts. One application is bound to many other applications. Lack of trust between applications. <\/p>\n<p>Oauth was developed at twitter and ma.golia<\/p>\n<p>Standard for delegation of restricted rights. <\/p>\n<p><!--more--><\/p>\n<p>Client wants to access a resource server on the behalf of the user who owns the resource. The authorization server has access to the resource. <\/p>\n<p>Client sends the scope to the authorization server. The auth server then asks the user if the client should be granted permissions. The user then agrees if he wants to and then the auth server then sends the access token. Which the client then uses to get access to the restricted resources. <\/p>\n<p>Advantage is the client doesn&#8217;t have the password. The authorization server helps segregate duties and concerns between parties. <\/p>\n<p>Flows<\/p>\n<p>Authorisation code grant is designed for server side applications. <\/p>\n<p>Implicit grant is designed for single page applications. <\/p>\n<p>Now talking about Implicit grant. <\/p>\n<p>Openid connect defines how to use Oauth for authentication. Client gets id token which is a jwt token with information about the user. Can be signed by the issuer. <\/p>\n<p>Id token is for the client while the access token is for the resource server. <\/p>\n<p>Now showing the demo. <\/p>\n<p>It&#8217;s a single page application that provides some vouchers. To buy the voucher the user has to be logged in. <\/p>\n<p>Showing the oauth network flow using fiddler.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 22 Oct 2015 12:02:27 +0000","created_by":1,"updated_at":"Thu, 22 Oct 2015 12:02:27 +0000","updated_by":1,"published_at":"Thu, 22 Oct 2015 12:02:27 +0000","published_by":1},{"id":268,"title":"Notes from ngAnimate 2.0 at Angular Connect 2015","slug":"notes-from-nganimate-2-0-at-angular-connect-2015","markdown":"\nDay two\n\n14:30\n\nngAnimate 2.0 by Robert Masserie\n\nNo longer css. Css doesn\u2019t allow complex logic. Using domain specific language (DSL).\n\nJson file dictates chaining and choreography. Json is limited as it doesn\u2019t alow dynamic logic. Represents static data.\n\nNow using full DSL with full programmatic API.\n\nngAnimate 2.0 is a proof of concept as of now.\n\nUse of animation factory to create animations. Still allows one to define static animations but one can also use the programmatic API to include complex logic in animation flows. Seamlessly switch between parallel and sequential animations.\n\nStagger animations by specifying key frames at various points in progress. For instance, apply colour change at 25%, and scaling at, 75%.\n\nIt comes with better event integration. For instance trigger a modal animation from the point of click on the button. This is possible using the programmatic API.\n\nNow talking about Material Design.\n\nPretty difficult to do using angular 1 as some of the animations originated from the point of click.\n\nNow, listen on click events and then run the ngAnimate functions to handle the animations programmatically.\n\nTab animations are tricky because it animates the tabs themselves, the ink bar that transitions left or right and the actual content below. This is implemented in angular 2 by setting the tab index and then programmatically loading the rest or the animations I. E. Left or right based on what the tab index is.\n\nNow talking about what ngAnimate will be able to.\n\nRich unit testing, performance tuning, migration and adaptive styling look like interesting goals.\n\nAnimation asserts help test if an animation sequence has happened or not in unit test.\n\nAddressing performance by no longer doing reflows. Also pre calculating delays to provide more efficient animations.\n\nConsidering an upgrade path from ngAnimate 1 to 2.\n\nAbility to control animations at any stage. Pause, fast forward or reverse. Showed a demo with a slider controlling full animation sequence of a mix of asynchronous and synchronous animation flows.\n\n\n","html":"<p>Day two <\/p>\n<p>14:30<\/p>\n<p>ngAnimate 2.0 by Robert Masserie <\/p>\n<p>No longer css. Css doesn&#8217;t allow complex logic. Using domain specific language (DSL). <\/p>\n<p>Json file dictates chaining and choreography. Json is limited as it doesn&#8217;t alow dynamic logic. Represents static data. <\/p>\n<p>Now using full DSL with full programmatic API. <\/p>\n<p><!--more--><\/p>\n<p>ngAnimate 2.0 is a proof of concept as of now. <\/p>\n<p>Use of animation factory to create animations. Still allows one to define static animations but one can also use the programmatic API to include complex logic in animation flows. Seamlessly switch between parallel and sequential animations. <\/p>\n<p>Stagger animations by specifying key frames at various points in progress. For instance, apply colour change at 25%, and scaling at, 75%.<\/p>\n<p>It comes with better event integration. For instance trigger a modal animation from the point of click on the button. This is possible using the programmatic API. <\/p>\n<p>Now talking about Material Design. <\/p>\n<p>Pretty difficult to do using angular 1 as some of the animations originated from the point of click. <\/p>\n<p>Now, listen on click events and then run the ngAnimate functions to handle the animations programmatically. <\/p>\n<p>Tab animations are tricky because it animates the tabs themselves, the ink bar that transitions left or right and the actual content below. This is implemented in angular 2 by setting the tab index and then programmatically loading the rest or the animations I. E. Left or right based on what the tab index is.<\/p>\n<p>Now talking about what ngAnimate will be able to. <\/p>\n<p>Rich unit testing, performance tuning, migration and adaptive styling look like interesting goals. <\/p>\n<p>Animation asserts help test if an animation sequence has happened or not in unit test.<\/p>\n<p>Addressing performance by no longer doing reflows. Also pre calculating delays to provide more efficient animations. <\/p>\n<p>Considering an upgrade path from ngAnimate 1 to 2.<\/p>\n<p>Ability to control animations at any stage. Pause, fast forward or reverse. Showed a demo with a slider controlling full animation sequence of a mix of asynchronous and synchronous animation flows.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 22 Oct 2015 12:05:16 +0000","created_by":1,"updated_at":"Thu, 22 Oct 2015 12:05:16 +0000","updated_by":1,"published_at":"Thu, 22 Oct 2015 12:05:16 +0000","published_by":1},{"id":272,"title":"Notes from TechCrunch Disrupt London 2015","slug":"notes-from-techcrunch-disrupt-london-2015","markdown":"\n\n# [![TC2015 1](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1-1024x768.jpg?fit=696%2C522&ssl=1)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?ssl=1)\n\n\n# Day One\n\n\n## Sevenhugs remote\n\nEU startup. Easy control of Phillips hue lights. Maps the person with the remote into a 3D space. Could potentially be used for indoor navigation?  \n Point the device at any smart bulb or device and instantly control it.  \n Quite expensive. \u20ac200 to produce the remote.\n\n\n## Mainframe\n\nContext sensitive instant messaging with backwards compatibility to email. AI that goes through your email\/messages looking for actions. Based on content and context, it converts them to tasks\/calendar events\/reminders. Pretty cool. Also does same thing for incoming email messages, which means that incoming email acts as an instant message. When sending instant message, if recipient is an email, it creates action links which allows the recipient to integrate with the system.\n\n\n## Max\n\nSA startup. Delivery provider in Lagos. Allows delivery in 3 hours. Drivers drive motorbikes to escape traffic. Customer website allows booking of deliveries with ease. Have launched APIs and are helping retailers integrate their platform with them. The delivery guys are also being trained to be sales agents. They pitch their product, increase awareness.\n\n\n## YayPay\n\nAccounts receivable office for every small business. Setup invoice on YayPay. Client gets the invoice and can pay using various pay,net methods. Also allows clients to pay a deposit. Allows sending up a collection process, like sending an email two weeks before due date, SMS reminders two days before, call after a week after due date and then emails every two weeks. Uses machine learning to determine best payment method.\n\n\n## FairFleet\n\nMunich, Germany. Inspect your construction site using drones. Crowd sourcing model. Choose the construction site you want to get inspected. Mark area of inspection. Set accuracy. Get accurate photos daily. Go back in timeline to inspect progress. Uses MapBox API for geo mapping.\n\n\n## JukeDeck (winner of the startup battlefield)\n\nArtificially intelligent music player. Have build software that composes original music note by note. Google and the royal family have used their music. Choose the length, mood, style and it creates music from scratch. Most awesome demo with live rap.\n\n\n## Move bubble\n\nConsumer property app. Property feed. Learns with use. Allows setting up appointments with property owners.\n\n\n## Yoobic\n\nApps for helping sales employees work better with sales management team. App allows publishing of goals and missions that can be delivered wirelessly to various stores. Sales employees of their stores will get notified of available mission. App can also be used for inventory management (sort of). Analytics for viewing various metrics like sales percentages for various products etc. Claims can increase sales by 25%. Collaboration platform for brands and retailers.\n\n\n## Caruma\n\nA device with two cameras, front and back, 4G, and wifi hotspot. Provides security to prevent theft. Uses motion and face detection to check if something is wrong. Sounds alarm even if the driver is dozing off. Live video recording and upload capabilities. Comes with the app that notifies when something\u2019s wrong with the car.\n\n\n# Day two\n\n\n## Fireside chat with CEO of Twilio\n\nJeff Lawson. Used to work at Amazon web services. Observed that communications is key when building a customer centric business. In those times, comms costed a lot and required a lot of hardware infrastructure. Set out to create Twilio to make software driven communications that can be setup in matter of minutes. Strongly believes that the ability to experiment stuff very quickly is key to innovation and hence has built a business that allows developers to do exactly that.\n\n\n## Chat with Hassle.com\n\nAlex Depledge. Website to get house cleaners on demand. It is important to be naive when moving into a new industry. She was able to make such an impact in cleaning industry because she was naive. Lost a lot of money, down to \u00a3100 but kept pushing. She and her team coded all Christmas and launched the product in January and made more money in that month than all in last six months combined. Strongly believes in free market which allows movement of labour. Talks about gender equality. Started \u201cGirls can code\u201d movement and found that there are women in tech but they are just aren\u2019t recognised. Also, at the younger age, there is a serious lack of awareness about technology. For instance, girls didn\u2019t know what a UX developer was or what a full stack developer did in their day job.  \n Thinks that government legislation needs to be refactored. The laws that govern today\u2019s business have been created in old times, situation that is probably not relevant anymore. Worse thing than refactoring code is writing more and more code on top of old code.\n\n\n","html":"<h1><a href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?ssl=1\"><img class=\"aligncenter size-large wp-image-275\" src=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1-1024x768.jpg?fit=696%2C522&#038;ssl=1\" alt=\"TC2015 1\" srcset=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?resize=1024%2C768&amp;ssl=1 1024w, https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?resize=300%2C225&amp;ssl=1 300w, https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?resize=1272%2C954&amp;ssl=1 1272w, https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?w=1400&amp;ssl=1 1400w, https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/01\/TC2015-1.jpg?w=2100&amp;ssl=1 2100w\" sizes=\"(max-width: 696px) 100vw, 696px\" data-recalc-dims=\"1\" \/><\/a><\/h1>\n<h1 style=\"text-align: justify;\">Day One<\/h1>\n<h2 style=\"text-align: justify;\">Sevenhugs remote<\/h2>\n<p style=\"text-align: justify;\">EU startup. Easy control of Phillips hue lights. Maps the person with the remote into a 3D space. Could potentially be used for indoor navigation?<br \/>\nPoint the device at any smart bulb or device and instantly control it.<br \/>\nQuite expensive. \u20ac200 to produce the remote.<!--more--><\/p>\n<h2 style=\"text-align: justify;\">Mainframe<\/h2>\n<p style=\"text-align: justify;\">Context sensitive instant messaging with backwards compatibility to email. AI that goes through your email\/messages looking for actions. Based on content and context, it converts them to tasks\/calendar events\/reminders. Pretty cool. Also does same thing for incoming email messages, which means that incoming email acts as an instant message. When sending instant message, if recipient is an email, it creates action links which allows the recipient to integrate with the system.<\/p>\n<h2 style=\"text-align: justify;\">Max<\/h2>\n<p style=\"text-align: justify;\">SA startup. Delivery provider in Lagos. Allows delivery in 3 hours. Drivers drive motorbikes to escape traffic. Customer website allows booking of deliveries with ease. Have launched APIs and are helping retailers integrate their platform with them. The delivery guys are also being trained to be sales agents. They pitch their product, increase awareness.<\/p>\n<h2 style=\"text-align: justify;\">YayPay<\/h2>\n<p style=\"text-align: justify;\">Accounts receivable office for every small business. Setup invoice on YayPay. Client gets the invoice and can pay using various pay,net methods. Also allows clients to pay a deposit. Allows sending up a collection process, like sending an email two weeks before due date, SMS reminders two days before, call after a week after due date and then emails every two weeks. Uses machine learning to determine best payment method.<\/p>\n<h2 style=\"text-align: justify;\">FairFleet<\/h2>\n<p style=\"text-align: justify;\">Munich, Germany. Inspect your construction site using drones. Crowd sourcing model. Choose the construction site you want to get inspected. Mark area of inspection. Set accuracy. Get accurate photos daily. Go back in timeline to inspect progress. Uses MapBox API for geo mapping.<\/p>\n<h2 style=\"text-align: justify;\">JukeDeck (winner of the startup battlefield)<\/h2>\n<p style=\"text-align: justify;\">Artificially intelligent music player. Have build software that composes original music note by note. Google and the royal family have used their music. Choose the length, mood, style and it creates music from scratch. Most awesome demo with live rap.<\/p>\n<h2 style=\"text-align: justify;\">Move bubble<\/h2>\n<p style=\"text-align: justify;\">Consumer property app. Property feed. Learns with use. Allows setting up appointments with property owners.<\/p>\n<h2 style=\"text-align: justify;\">Yoobic<\/h2>\n<p style=\"text-align: justify;\">Apps for helping sales employees work better with sales management team. App allows publishing of goals and missions that can be delivered wirelessly to various stores. Sales employees of their stores will get notified of available mission. App can also be used for inventory management (sort of). Analytics for viewing various metrics like sales percentages for various products etc. Claims can increase sales by 25%. Collaboration platform for brands and retailers.<\/p>\n<h2 style=\"text-align: justify;\">Caruma<\/h2>\n<p style=\"text-align: justify;\">A device with two cameras, front and back, 4G, and wifi hotspot. Provides security to prevent theft. Uses motion and face detection to check if something is wrong. Sounds alarm even if the driver is dozing off. Live video recording and upload capabilities. Comes with the app that notifies when something&#8217;s wrong with the car.<\/p>\n<h1 style=\"text-align: justify;\">Day two<\/h1>\n<h2 style=\"text-align: justify;\">Fireside chat with CEO of Twilio<\/h2>\n<p style=\"text-align: justify;\">Jeff Lawson. Used to work at Amazon web services. Observed that communications is key when building a customer centric business. In those times, comms costed a lot and required a lot of hardware infrastructure. Set out to create Twilio to make software driven communications that can be setup in matter of minutes. Strongly believes that the ability to experiment stuff very quickly is key to innovation and hence has built a business that allows developers to do exactly that.<\/p>\n<h2 style=\"text-align: justify;\">Chat with Hassle.com<\/h2>\n<p style=\"text-align: justify;\">Alex Depledge. Website to get house cleaners on demand. It is important to be naive when moving into a new industry. She was able to make such an impact in cleaning industry because she was naive. Lost a lot of money, down to \u00a3100 but kept pushing. She and her team coded all Christmas and launched the product in January and made more money in that month than all in last six months combined. Strongly believes in free market which allows movement of labour. Talks about gender equality. Started \u201cGirls can code\u201d movement and found that there are women in tech but they are just aren&#8217;t recognised. Also, at the younger age, there is a serious lack of awareness about technology. For instance, girls didn&#8217;t know what a UX developer was or what a full stack developer did in their day job.<br \/>\nThinks that government legislation needs to be refactored. The laws that govern today&#8217;s business have been created in old times, situation that is probably not relevant anymore. Worse thing than refactoring code is writing more and more code on top of old code.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 10 Jan 2016 11:58:34 +0000","created_by":1,"updated_at":"Sun, 10 Jan 2016 11:59:13 +0000","updated_by":1,"published_at":"Sun, 10 Jan 2016 11:58:34 +0000","published_by":1},{"id":280,"title":"Some thoughts on the RESTful way of the APIs","slug":"the-restful-way-of-the-apis","markdown":"\nSo, it\u2019s been a while now since I have written a post reflecting my thoughts on a subject. It has been primarily like this since I\u2019ve been busy doing what I always do \u2013 trying out new cool things and ideas.\n\nIn almost every app, every SaaS platform or every web based backend system I write, the first thing I start with is an API. This API first attitude has helped me in many ways. First and foremost, it helps me think how the architecture must work to suffice the needs of the client. It also leads me to think about containers and how best to distribute load etc. Now, this might seem like an overkill when just trying out a new idea but it at least gives me a rough estimate of the scope of the project that I\u2019m looking at. Besides, now a days, working with thick client frameworks like Angular JS almost demands that you have a strong backend with a well written collection of APIs.\n\nWriting restful APIs time and time again have helped me improve my understanding regarding the idea of \u201crestful APIs\u201d with every iteration and hence, I\u2019ve come up with few thoughts regarding ways in which a RESTful API should be written. So, here we go.\n\n**A URL path should contain singular nouns and nothing else. **This comes straight out of the REST specification. However, there have been cases where I have been forced to use verbs but that has only been the case when I haven\u2019t been very clear about what that endpoint is doing. Before defining your endpoints, be very clear on what you are trying to achieve. For instance, once I was writing an endpoint for an application to process an incoming web hook. Hence, I named the endpoint \u201cPOST: \/process\u201d. While I actually did implement this, it kept bugging me throughout the implementation of the application. Finally, when that piece of functionality was coming to an end did I get crystal clear understanding regarding what that endpoint was doing. When a web hook gets triggered, things happen within the application. THAT thing is an Event! Without doubt, I quickly renamed the endpoint to \u201cPOST: \/event\u201d and my mind was at peace. In retrospect, I think I should\u2019ve created a separate endpoint for each application consuming that endpoint.\n\nWhile developing one of the APIs, I had a devious idea. Normally when defining an endpoint that allows one to retrieve a resource, say for instance a post object, one\u2019s target endpoint might look like \u201cGET: \/post\/1\u201d where \u201cpost\u201d is the resource and \u201c1\u201d is the ID of the resource. But what if I wanted to select posts by their category ID? Would my target endpoint look like \u201cGET: \/post\/1\u201d? If yes then whats the difference between the first and second call? How about \u201cGET: \/post\/byCategory\/1\u201d? Or even \u201cGET: \/postsByCategory\/1\u201d?\n\nFor some time I had a crazy obsession with such endpoints to an extent that quite a few endpoints during that time were defined like this. However, after a while these endpoints just started to look weird. Also, when I started developing the frontend AngularJS application, making queries started to feel more difficult than it should\u2019ve been. For instance, I had a dropdown that allowed a user to select the search type (category|tags|name) and then whatever the user typed in the search box got searched. Initially I had a horrible if|elseif|else blocks which chose which endpoint to call based on selected search type. However, this quickly became really boring, tedious and frankly, dirty. So, I revisited my endpoints again. If I, as someone who had made these endpoints struggled to develop an application using them, then what should I expect from the developers who will build their applications on my platform?\n\nFinally, I ended up having a couple of thoughts regarding how this should be done. \u201cGET: \/post?categoryId=5\u201d was my final answer. This not only solved the problem of querying by multiple search parameters but also made it easier for the consuming application to call the endpoint. Now, this new endpoint meant that I only had to assign field names to each dropdown and the query would build itself!\n\n**Query strings represent things that you want to do to your resource. **This is normally fairly obvious and I will cover more of this in depth in later paragraphs but for starters, query strings can be nouns, adjectives or even verbs (rare) with values or switches on them. For instance, \u201cGET: \/person\/1?gender=male\u201d clearly means that give me a person with ID 1 whose gender is male.\n\nYou can also do other neat things with query strings like provide metadata regarding your call. For instance, \u201cGET: \/person\/1?_select=firstname,lastname\u201d means give me person whose ID is 1 and only select firstname and lastname fields of each object. This kind of functionality is especially useful as it helps clients save their precious processing power, memory and network bandwidth. Think of small micro controllers displaying temperature information or mobile apps running on 2G or even Edge network. Any query parameter indicating metadata should begin with an underscore so that they can easily be differentiated in a pool of query parameters.\n\nNever ever build a query within a query. I saw some examples on the web where someone had built an endpoint like \u201cGET: \/post\/?query=startsWith:Senate,hasMinWords:100\u201d. I mean, why? How does one even go thinking like this? This is a horrible idea because you\u2019re forcing your users to learn your query language even when the principles you\u2019re building this on (REST) provides a framework like query string for you to use. The above query within query can be broken down as \u201cGET: \/post?startsWith=Senate&hasMinWords=100\u201d. This is perfectly acceptable.\n\nYou could ask, what if I wanted to search posts by their post header, body and tag components? Well, you could just do \u201cGET: \/post?headerStartsWith=Senate&bodyHasMinWords=100\u201d or even \u201cGET: \/post?header.startsWith=Senate&body.hasMinWords=100\u201d although the first one reads better but might be slightly tricky to implement.\n\n**Respond properly, don\u2019t be lazy. **When it comes to responses, it\u2019s easy to forget how important it is to respond properly. In most of the APIs that I write, for every POST request, I like to respond with HTTP 200 returning the object instance that was created as a result of that request. The same goes for PUT, but not for DELETE. The name of that HTTP method, DELETE clearly means that the resource was deleted, i.e. it doesn\u2019t exist. It makes me sad when I see implementations online where people return the resource that was deleted. If it was deleted what can the client possibly use it for? Hence, for DELETE, the response should be HTTP 204 no content.\n\nNow, depending on how secure you want to make your API, the behaviour of your endpoints on error will differ. It is incredibly useful to respond with a stack trace in a development environment but probably not when it comes to production. It pays in the long run to respond in a meaningful way when something goes wrong. At the very least, you must respond with HTTP 400 when it is the client\u2019s fault, for example validation of the input data gone wrong. However, there are tons of other error codes you could use for other situations.\n\nSometimes you might have a protected resource, something that requires a login. In situations such as these, there are two ways you can respond, 401 (Unauthorized) and 403 (Forbidden). Typically, I like to respond with 403 when user is not logged in, i.e. not authenticated and 401 when the user is authenticated but doesn\u2019t have enough privileges i.e. is not authorized to do that operation. Never ever redirect to login page or API endpoint when the client is trying to do something that requires login.\n\nThere is no shame in responding with HTTP 500. Yes you read that right. HTTP 500 does not mean that your app has crashed. It only means that something has gone wrong at the server side and the server doesn\u2019t want to tell you about it. Think about it. If the app had crashed, you wouldn\u2019t receive any response. It\u2019d just die. When writing any API during my first iteration, I just respond with 500s in all places where I think things might go wrong. I normally then go back and improve things one by one, handling errors carefully and responding in a helpful way.\n\nNow, a word on caching. As far as I know, most containers and frameworks handle caching internally. However, sometimes you might need to configure stuff at endpoint level. For instance, if your endpoint is outputting information that gets updated as part of a batch process that occurs infrequently, it will be incredibly useful to use the expires headers. Also, when responding to a request, if the if-modified-since header is indicating that the data that you have on the server hasn\u2019t changed since last time, respond with 304. This is incredibly useful especially when your client is a webapp running in a web browser as it will help improve client experience dramatically.\n\n**Login requests must always be a POST request. **I get this a lot. If you ask this to me a couple of years ago, I\u2019d probably say that the objective of Login is to \u201cget\u201d a session and hence it should be a GET request. I don\u2019t even know how I got that logic but one of my colleagues opened my eyes when they mentioned that the objective of Login is not to GET a login session but to CREATE one and hence it is a POST request. Same goes for registration. Your response at this stage depends. You could either respond with the user object which contains information about the logged in user or you can just return with 204. However, bear in mind that you\u2019ll have to strip out sensitive information like passwords and security question answers when responding with the user object to avoid security nightmare.\n\nHowever, for consumer facing applications, game has slightly changed with the introduction of social logins. In this case, you\u2019ll be making everyone\u2019s life easier if the initial request to login via social network was a GET request. As a general rule of thumb, I normally go with \u201cGET: \/auth\/<provider name>\u201d type of format when introducing social logins where in case of Facebook, this becomes \u201cGET: \/auth\/facebook\u201d.\n\nSo I hope this has given you something to think about. Let me know if you have any questions, comments or feedback. I\u2019d love to hear your thoughts on this!\n\n\n","html":"<p style=\"text-align: justify;\">So, it&#8217;s been a while now since I have written a post reflecting my thoughts on a subject. It has been primarily like this since I&#8217;ve been busy doing what I always do &#8211; trying out new cool things and ideas.<\/p>\n<p style=\"text-align: justify;\">In almost every app, every SaaS platform or every web based backend system I write, the first thing I start with is an API. This API first attitude has helped me in many ways. First and foremost, it helps me think how the architecture must work to suffice the needs of the client. It also leads me to think about containers and how best to distribute load etc. Now, this might seem like an overkill when just trying out a new idea but it at least gives me a rough estimate of the scope of the project that I&#8217;m looking at. Besides, now a days, working with thick client frameworks like Angular JS almost demands that you have a strong backend with a well written collection of APIs.<!--more--><\/p>\n<p style=\"text-align: justify;\">Writing restful APIs time and time again have helped me improve my understanding regarding the idea of &#8220;restful APIs&#8221; with every iteration and hence, I&#8217;ve come up with few thoughts regarding ways in which a RESTful API should be written. So, here we go.<\/p>\n<p style=\"text-align: justify;\"><strong>A URL path should contain singular nouns and nothing else. <\/strong>This comes straight out of the REST specification. However, there have been cases where I have been forced to use verbs but that has only been the case when I haven&#8217;t been very clear about what that endpoint is doing. Before defining your endpoints, be very clear on what you are trying to achieve. For instance, once I was writing an endpoint for an application to process an incoming web hook. Hence, I named the endpoint &#8220;POST: \/process&#8221;. While I actually did implement this, it kept bugging me throughout the implementation of the application. Finally, when that piece of functionality was coming to an end did I get crystal clear understanding regarding what that endpoint was doing. When a web hook gets triggered, things happen within the application. THAT thing is an Event! Without doubt, I quickly renamed the endpoint to &#8220;POST: \/event&#8221; and my mind was at peace. In retrospect, I think I should&#8217;ve created a separate endpoint for each application consuming that endpoint.<\/p>\n<p style=\"text-align: justify;\">While developing one of the APIs, I had a devious idea. Normally when defining an endpoint that allows one to retrieve a resource, say for instance a post object, one&#8217;s target endpoint might look like &#8220;GET: \/post\/1&#8221; where &#8220;post&#8221; is the resource and &#8220;1&#8221; is the ID of the resource. But what if I wanted to select posts by their category ID? Would my target endpoint look like &#8220;GET: \/post\/1&#8221;? If yes then whats the difference between the first and second call? How about &#8220;GET: \/post\/byCategory\/1&#8221;? Or even &#8220;GET: \/postsByCategory\/1&#8221;?<\/p>\n<p style=\"text-align: justify;\">For some time I had a crazy obsession with such endpoints to an extent that quite a few endpoints during that time were defined like this. However, after a while these endpoints just started to look weird. Also, when I started developing the frontend AngularJS application, making queries started to feel more difficult than it should&#8217;ve been. For instance, I had a dropdown that allowed a user to select the search type (category|tags|name) and then whatever the user typed in the search box got searched. Initially I had a horrible if|elseif|else blocks which chose which endpoint to call based on selected search type. However, this quickly became really boring, tedious and frankly, dirty. So, I revisited my endpoints again. If I, as someone who had made these endpoints struggled to develop an application using them, then what should I expect from the developers who will build their applications on my platform?<\/p>\n<p style=\"text-align: justify;\">Finally, I ended up having a couple of thoughts regarding how this should be done. &#8220;GET: \/post?categoryId=5&#8221; was my final answer. This not only solved the problem of querying by multiple search parameters but also made it easier for the consuming application to call the endpoint. Now, this new endpoint meant that I only had to assign field names to each dropdown and the query would build itself!<\/p>\n<p style=\"text-align: justify;\"><b>Query strings represent things that you want to do to your resource. <\/b>This is normally fairly obvious and I will cover more of this in depth in later paragraphs but for starters, query strings can be nouns, adjectives or even verbs (rare) with values or switches on them. For instance, &#8220;GET: \/person\/1?gender=male&#8221; clearly means that give me a person with ID 1 whose gender is male.<\/p>\n<p style=\"text-align: justify;\">You can also do other neat things with query strings like provide metadata regarding your call. For instance, &#8220;GET: \/person\/1?_select=firstname,lastname&#8221; means give me person whose ID is 1 and only select firstname and lastname fields of each object. This kind of functionality is especially useful as it helps clients save their precious processing power, memory and network bandwidth. Think of small micro controllers displaying temperature information or mobile apps running on 2G or even Edge network. Any query parameter indicating metadata should begin with an underscore so that they can easily be differentiated in a pool of query parameters.<\/p>\n<p style=\"text-align: justify;\">Never ever build a query within a query. I saw some examples on the web where someone had built an endpoint like &#8220;GET: \/post\/?query=startsWith:Senate,hasMinWords:100&#8221;. I mean, why? How does one even go thinking like this? This is a horrible idea because you&#8217;re forcing your users to learn your query language even when the principles you&#8217;re building this on (REST) provides a framework like query string for you to use. The above query within query can be broken down as &#8220;GET: \/post?startsWith=Senate&amp;hasMinWords=100&#8221;. This is perfectly acceptable.<\/p>\n<p style=\"text-align: justify;\">You could ask, what if I wanted to search posts by their post header, body and tag components? Well, you could just do &#8220;GET: \/post?headerStartsWith=Senate&amp;bodyHasMinWords=100&#8221; or even &#8220;GET: \/post?header.startsWith=Senate&amp;body.hasMinWords=100&#8221; although the first one reads better but might be slightly tricky to implement.<\/p>\n<p style=\"text-align: justify;\"><strong>Respond properly, don&#8217;t be lazy. <\/strong>When it comes to responses, it&#8217;s easy to forget how important it is to respond properly. In most of the APIs that I write, for every POST request, I like to respond with HTTP 200 returning the object instance that was created as a result of that request. The same goes for PUT, but not for DELETE. The name of that HTTP method, DELETE clearly means that the resource was deleted, i.e. it doesn&#8217;t exist. It makes me sad when I see implementations online where people return the resource that was deleted. If it was deleted what can the client possibly use it for? Hence, for DELETE, the response should be HTTP 204 no content.<\/p>\n<p style=\"text-align: justify;\">Now, depending on how secure you want to make your API, the behaviour of your endpoints on error will differ. It is incredibly useful to respond with a stack trace in a development environment but probably not when it comes to production. It pays in the long run to respond in a meaningful way when something goes wrong. At the very least, you must respond with HTTP 400 when it is the client&#8217;s fault, for example validation of the input data gone wrong. However, there are tons of other error codes you could use for other situations.<\/p>\n<p style=\"text-align: justify;\">Sometimes you might have a protected resource, something that requires a login. In situations such as these, there are two ways you can respond, 401 (Unauthorized) and 403 (Forbidden). Typically, I like to respond with 403 when user is not logged in, i.e. not authenticated and 401 when the user is authenticated but doesn&#8217;t have enough privileges i.e. is not authorized to do that operation. Never ever redirect to login page or API endpoint when the client is trying to do something that requires login.<\/p>\n<p style=\"text-align: justify;\">There is no shame in responding with HTTP 500. Yes you read that right. HTTP 500 does not mean that your app has crashed. It only means that something has gone wrong at the server side and the server doesn&#8217;t want to tell you about it. Think about it. If the app had crashed, you wouldn&#8217;t receive any response. It&#8217;d just die. When writing any API during my first iteration, I just respond with 500s in all places where I think things might go wrong. I normally then go back and improve things one by one, handling errors carefully and responding in a helpful way.<\/p>\n<p style=\"text-align: justify;\">Now, a word on caching. As far as I know, most containers and frameworks handle caching internally. However, sometimes you might need to configure stuff at endpoint level. For instance, if your endpoint is outputting information that gets updated as part of a batch process that occurs infrequently, it will be incredibly useful to use the expires headers. Also, when responding to a request, if the if-modified-since header is indicating that the data that you have on the server hasn&#8217;t changed since last time, respond with 304. This is incredibly useful especially when your client is a webapp running in a web browser as it will help improve client experience dramatically.<\/p>\n<p style=\"text-align: justify;\"><b>Login requests must always be a POST request. <\/b>I get this a lot. If you ask this to me a couple of years ago, I&#8217;d probably say that the objective of Login is to &#8220;get&#8221; a session and hence it should be a GET request. I don&#8217;t even know how I got that logic but one of my colleagues opened my eyes when they mentioned that the objective of Login is not to GET a login session but to CREATE one and hence it is a POST request. Same goes for registration. Your response at this stage depends. You could either respond with the user object which contains information about the logged in user or you can just return with 204. However, bear in mind that you&#8217;ll have to strip out sensitive information like passwords and security question answers when responding with the user object to avoid security nightmare.<\/p>\n<p style=\"text-align: justify;\">However, for consumer facing applications, game has slightly changed with the introduction of social logins. In this case, you&#8217;ll be making everyone&#8217;s life easier if the initial request to login via social network was a GET request. As a general rule of thumb, I normally go with &#8220;GET: \/auth\/&lt;provider name&gt;&#8221; type of format when introducing social logins where in case of Facebook, this becomes &#8220;GET: \/auth\/facebook&#8221;.<\/p>\n<p style=\"text-align: justify;\">So I hope this has given you something to think about. Let me know if you have any questions, comments or feedback. I&#8217;d love to hear your thoughts on this!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 11 Jan 2016 07:21:17 +0000","created_by":1,"updated_at":"Mon, 11 Jan 2016 07:31:59 +0000","updated_by":1,"published_at":"Mon, 11 Jan 2016 07:21:17 +0000","published_by":1},{"id":301,"title":"Waiting for tomcat to start up in a script","slug":"waiting-for-tomcat-to-start-up-in-a-script","markdown":"\nSo here\u2019s something that I have found that waits for tomcat to come up but rather than polling or a static time based wait, it uses FIFO pipeline to wait.\n\nfunction isTomcatUp { # Use FIFO pipeline to check catalina.out for server startup notification rather than # ping with an HTTP request. This was recommended by ForgeRock (Zoltan). FIFO=\/tmp\/notifytomcatfifo mkfifo \"${FIFO}\" || exit 1 { # run tail in the background so that the shell can # kill tail when notified that grep has exited tail -f $CATALINA_HOME\/logs\/catalina.out & # remember tail's PID TAILPID=$! # wait for notification that grep has exited read foo <${FIFO} # grep has exited, time to go kill \"${TAILPID}\" } | { grep -m 1 \"INFO: Server startup\" # notify the first pipeline stage that grep is done echo >${FIFO} } # clean up rm \"${FIFO}\" }\n\nDrop this into a tomcat-util.sh file, make it executable and then source the file:\n\nchmod u+x tomcat-util.sh source tomcat-util.sh\n\nYou\u2019ll now have isTomcatUp available as a bash command.\n\n\u00a0\n\n\n","html":"<p>So here&#8217;s something that I have found that waits for tomcat to come up but rather than polling or a static time based wait, it uses FIFO pipeline to wait.<\/p>\n<pre class=\"lang:sh decode:true\">function isTomcatUp {\r\n       \r\n    # Use FIFO pipeline to check catalina.out for server startup notification rather than\r\n    # ping with an HTTP request. This was recommended by ForgeRock (Zoltan).\r\n   \r\n    FIFO=\/tmp\/notifytomcatfifo\r\n    mkfifo \"${FIFO}\" || exit 1\r\n    {\r\n        # run tail in the background so that the shell can\r\n        # kill tail when notified that grep has exited\r\n        tail -f $CATALINA_HOME\/logs\/catalina.out &amp;\r\n        # remember tail's PID\r\n        TAILPID=$!\r\n        # wait for notification that grep has exited\r\n        read foo &lt;${FIFO}\r\n        # grep has exited, time to go\r\n        kill \"${TAILPID}\"\r\n    } | {\r\n        grep -m 1 \"INFO: Server startup\"\r\n        # notify the first pipeline stage that grep is done\r\n        echo &gt;${FIFO}\r\n    }\r\n    # clean up\r\n    rm \"${FIFO}\"\r\n}<\/pre>\n<p>Drop this into a tomcat-util.sh file, make it executable and then source the file:<\/p>\n<pre class=\"lang:sh decode:true \">chmod u+x tomcat-util.sh\r\nsource tomcat-util.sh<\/pre>\n<p>You&#8217;ll now have isTomcatUp available as a bash command.<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 15 Jan 2016 13:49:16 +0000","created_by":1,"updated_at":"Fri, 15 Jan 2016 13:49:38 +0000","updated_by":1,"published_at":"Fri, 15 Jan 2016 13:49:16 +0000","published_by":1},{"id":306,"title":"Gulpfile.js for vanilla AngularJS projects","slug":"gulpfile-js-for-vanilla-angularjs-projects","markdown":"\nSince I\u2019ve learnt Angular, most of my front-end web applications are written in it. Its brilliant. Everything works fine, up to a point where your application has grown 10 times the size it was since you started it. At some point you\u2019d want something to accelerate your delivery. That\u2019s when gulp comes in.\n\nGulp is kinda like a build tool but for javascript based applications. It runs on Node.js and is available on npm. All you need to do is:\n\nnpm install -g gulp\n\nor if you are using a unix based application:\n\nsudo npm install -g gulp\n\nThat gets you access to the gulp command line. Once you have gulp installed, you need to something that tells it what to do with your project. Unlike many other build tools, gulp favours code over configuration. This means that rather than writing XML or YAML files that define the tasks, you write code. This is what makes gulp unique.\n\nSo, here\u2019s what I have. Below is my `gulpfile.js` that I use in most of my AngularJS projects:\n\n\/** * Created by manthanhd on 04\/01\/2016. *\/ var gulp = require('gulp'), gutil = require('gulp-util'), sourcemaps = require('gulp-sourcemaps'), concat = require('gulp-concat'), ngAnnotate = require('gulp-ng-annotate'), watch = require('gulp-watch'), uglify = require('gulp-uglify'); gulp.task('default', ['build-js']); gulp.task('build-js', function() { return gulp.src(['public\/app\/**\/*.js']) .pipe(sourcemaps.init()) .pipe(concat('bundle.min.js')) .pipe(ngAnnotate()) \/\/only uglify if gulp is ran with '--type production' .pipe(gutil.env.type === 'production' ? uglify() : gutil.noop()) .pipe(gutil.env.type === 'production' ? gutil.noop() : sourcemaps.write()) .pipe(gulp.dest('public\/dist')); }); gulp.task('watch', function() { gulp.watch('public\/app\/**\/*.js', ['build-js']); });\n\nMost of my AngularJS code resides in `public\/app` directory and hence you\u2019ll see that in multiple places in that file.\n\nA quick glance should tell you that this minifies all of my AngularJS code and combines it all in one big file called `bundle.min.js` which is then placed in `public\/dist` folder. My HTML page is configured to only reference `bundle.min.js`.\n\nWhen executed normally, it doesn\u2019t minify the code because it assumes that the javascript code is being used for development. However, when I run `gulp --type production` it minifies the code and doesn\u2019t write sourcemaps.\n\nYou can install all of this by running:\n\nnpm install --save-dev gulp gulp-concat gulp-ng-annotate gulp-sourcemaps gulp-uglify gulp-util gulp-watch\n\nOh and here\u2019s my `package.json`:\n\n { \"name\": \"my-sample-angular-project\", \"version\": \"0.0.0\", \"private\": true, \"scripts\": { \"start\": \"node .\/bin\/www\" }, \"devDependencies\": { \"gulp\": \"^3.9.0\", \"gulp-concat\": \"^2.6.0\", \"gulp-ng-annotate\": \"^1.1.0\", \"gulp-sourcemaps\": \"^1.6.0\", \"gulp-uglify\": \"^1.5.1\", \"gulp-util\": \"^3.0.7\", \"gulp-watch\": \"^4.3.5\" } }\n\n\u00a0\n\n\n","html":"<p>Since I&#8217;ve learnt Angular, most of my front-end web applications are written in it. Its brilliant. Everything works fine, up to a point where your application has grown 10 times the size it was since you started it. At some point you&#8217;d want something to accelerate your delivery. That&#8217;s when gulp comes in.<\/p>\n<p>Gulp is kinda like a build tool but for javascript based applications. It runs on Node.js and is available on npm. All you need to do is:<\/p>\n<pre class=\"lang:sh decode:true \">npm install -g gulp<\/pre>\n<p>or if you are using a unix based application:<\/p>\n<pre class=\"lang:sh decode:true \">sudo npm install -g gulp<\/pre>\n<p>That gets you access to the gulp command line. Once you have gulp installed, you need to something that tells it what to do with your project. Unlike many other build tools, gulp favours code over configuration. This means that rather than writing XML or YAML files that define the tasks, you write code. This is what makes gulp unique.<\/p>\n<p>So, here&#8217;s what I have. Below is my <code>gulpfile.js<\/code> that I use in most of my AngularJS projects:<!--more--><\/p>\n<pre class=\"lang:js decode:true \">\/**\n* Created by manthanhd on 04\/01\/2016.\n*\/\nvar gulp = require('gulp'),\ngutil = require('gulp-util'),\nsourcemaps = require('gulp-sourcemaps'),\nconcat = require('gulp-concat'),\nngAnnotate = require('gulp-ng-annotate'),\nwatch = require('gulp-watch'),\nuglify = require('gulp-uglify');\n\ngulp.task('default', ['build-js']);\n\ngulp.task('build-js', function() {\nreturn gulp.src(['public\/app\/**\/*.js'])\n.pipe(sourcemaps.init())\n.pipe(concat('bundle.min.js'))\n.pipe(ngAnnotate())\n\/\/only uglify if gulp is ran with '--type production'\n.pipe(gutil.env.type === 'production' ? uglify() : gutil.noop())\n.pipe(gutil.env.type === 'production' ? gutil.noop() : sourcemaps.write())\n.pipe(gulp.dest('public\/dist'));\n});\n\ngulp.task('watch', function() {\ngulp.watch('public\/app\/**\/*.js', ['build-js']);\n});<\/pre>\n<p>Most of my AngularJS code resides in <code>public\/app<\/code> directory and hence you&#8217;ll see that in multiple places in that file.<\/p>\n<p>A quick glance should tell you that this minifies all of my AngularJS code and combines it all in one big file called <code>bundle.min.js<\/code> which is then placed in <code>public\/dist<\/code> folder. My HTML page is configured to only reference <code>bundle.min.js<\/code>.<\/p>\n<p>When executed normally, it doesn&#8217;t minify the code because it assumes that the javascript code is being used for development. However, when I run <code>gulp --type production<\/code> it minifies the code and doesn&#8217;t write sourcemaps.<\/p>\n<p>You can install all of this by running:<\/p>\n<pre class=\"lang:sh decode:true \">npm install --save-dev gulp gulp-concat gulp-ng-annotate gulp-sourcemaps gulp-uglify gulp-util gulp-watch<\/pre>\n<p>Oh and here&#8217;s my <code>package.json<\/code>:<\/p>\n<pre class=\"lang:default decode:true \"> {\n\"name\": \"my-sample-angular-project\",\n\"version\": \"0.0.0\",\n\"private\": true,\n\"scripts\": {\n\"start\": \"node .\/bin\/www\"\n},\n\"devDependencies\": {\n\"gulp\": \"^3.9.0\",\n\"gulp-concat\": \"^2.6.0\",\n\"gulp-ng-annotate\": \"^1.1.0\",\n\"gulp-sourcemaps\": \"^1.6.0\",\n\"gulp-uglify\": \"^1.5.1\",\n\"gulp-util\": \"^3.0.7\",\n\"gulp-watch\": \"^4.3.5\"\n}\n}<\/pre>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 22 Jan 2016 18:48:30 +0000","created_by":1,"updated_at":"Sun, 18 Sep 2016 20:37:27 +0000","updated_by":1,"published_at":"Fri, 22 Jan 2016 18:48:30 +0000","published_by":1},{"id":310,"title":"What I've learnt about writing Expressjs applications","slug":"what-ive-learnt-about-writing-expressjs-applications","markdown":"\nThere\u2019s a lot of ways you could write an express application. If you searched for Express app generator, you\u2019ll probably find at least five in the first page of search results.\n\nIn my opinion, most of these are designed for hackathons \u2013 to get you up and running as fast as possible but also as crudely as possible. While they are good when you want to try something out quickly, most of that code will turn into a big pile of mess as your application grows.\n\nI\u2019ve been in this situation before. I get my application up and running as fast as possible and in order to get my first MVP done but then quickly realise that the auto generated code that was meant to accelerate my development is now slowing me down. To remedy this, this is now how I develop my node.js applications.\n\nJust a quick note. I still use the <span class=\"lang:default decode:true crayon-inline \">\u2018express-generator<\/span>\u00a0\u2018 module for all my express applications.\n\nEvery <span class=\"lang:default decode:true crayon-inline \">Route<\/span>\u00a0 file should have its own function. Consider this:\n\n\/\/ file: routes\/IndexRoute.js function IndexRoute(express) { var router = express.Router(); router.get('\/', function(req, res) { return res.render('index'); }); return router; } module.exports = IndexRoute;\n\nNow, implement this in <span class=\"lang:default decode:true crayon-inline \">app.js<\/span>\u00a0 like so:\n\n\/\/ file: app.js var express = require('express'); ... var indexRoute = require('.\/routes\/IndexRoute')(express); ... app.use('\/', indexRoute);\n\nI quite like this approach because it keeps code clean and avoids having to use `require` everywhere. Also, by just looking at the module you can figure out what it depends on. For instance, here in <span class=\"lang:default decode:true crayon-inline \">IndexRoute.js<\/span>\u00a0 file (above) you can tell that it depends on express. For more complicated modules that require additional dependencies you can keep adding them in the module function declaration. For example:\n\nfunction UserRoute(express, userModel, moment) { ... } module.exports = UserRoute;\n\nHere the <span class=\"lang:default decode:true crayon-inline\">UserRoute<\/span>\u00a0clearly depends on express, <span class=\"lang:default decode:true crayon-inline\">userModel<\/span>\u00a0(mongoose model) and moment (npm moment.js).\n\nSometimes when developing cool and awesome stuff, it is easy to get carried away. For instance, in the case above, the <span class=\"lang:default decode:true crayon-inline \">UserRoute<\/span>\u00a0 is doing too much. It\u2019s doing something with time using moment and also manipulating user in the database using <span class=\"lang:default decode:true crayon-inline \">userModel<\/span>\u00a0. While it\u2019s borderline fine (with massive cringe), you might want to think about what the <span class=\"lang:default decode:true crayon-inline\">UserRoute <\/span>\u00a0should actually be doing.\n\nIn most cases, the way I group responsibilities is following. Everything suffixed with <span class=\"lang:default decode:true crayon-inline \">Route<\/span>\u00a0 manages Expressjs route mappings. One level down is <span class=\"lang:default decode:true crayon-inline \">Controller<\/span>\u00a0 which actually manages what\u2019s being done to that endpoint that has been mapped. Another level down is <span class=\"lang:default decode:true crayon-inline \">Service<\/span>\u00a0. This layer is more intelligent than controller and most of your logic should go in here. Lastly, you have <span class=\"lang:default decode:true crayon-inline\">Model<\/span>\u00a0. In case of mongoose this is just a model definition but there\u2019s nothing to stop you from doing more complex data interactions here.\n\nThis kind of code separation really helps as it is highly cohesive which in turn makes it testable. Now you can test the service and inject all its dependencies directly using the function parameters. Also, when you change definition of a service, it will break all your tests and yes that is a good thing because those broken tests will tell you the size of your impact crater that the change has created. If you didn\u2019t have those tests in place, you might have never known and maybe spent hours debugging trying to find out what went wrong. I know this because I\u2019ve been there!\n\nOne more advantage of this that I can see is that it will stop you from accidently redefining something in your code that you would\u2019ve if you used require all the time. This is especially true with Mongoose model definitions. It doesn\u2019t like it when you define a model twice and that is really easy to do when you\u2019re using require. Using this method, you require your model once in app.js and then pass it down to every service that requires it. In some cases it could also make your code more efficient as you aren\u2019t redefining objects evey time something like a service call is made.\n\nI\u2019m still working on finding the best way to write applications. Every time I write a new application, I learn about a new problem which I then set out to solve. I hope you do too!\n\n\n","html":"<p>There&#8217;s a lot of ways you could write an express application. If you searched for Express app generator, you&#8217;ll probably find at least five in the first page of search results.<\/p>\n<p>In my opinion, most of these are designed for hackathons &#8211; to get you up and running as fast as possible but also as crudely as possible. While they are good when you want to try something out quickly, most of that code will turn into a big pile of mess as your application grows.<\/p>\n<p>I&#8217;ve been in this situation before. I get my application up and running as fast as possible and in order to get my first MVP done but then quickly realise that the auto generated code that was meant to accelerate my development is now slowing me down. To remedy this, this is now how I develop my node.js applications.<!--more--><\/p>\n<p>Just a quick note. I still use the <span class=\"lang:default decode:true crayon-inline \">&#8216;express-generator<\/span>\u00a0&#8216; module for all my express applications.<\/p>\n<p>Every <span class=\"lang:default decode:true crayon-inline \">Route<\/span>\u00a0 file should have its own function. Consider this:<\/p>\n<pre class=\"lang:js decode:true\">\/\/ file: routes\/IndexRoute.js \r\nfunction IndexRoute(express) {\r\n     var router = express.Router();\r\n     router.get('\/', function(req, res) {\r\n         return res.render('index');\r\n     });\r\n     return router;\r\n}\r\n\r\nmodule.exports = IndexRoute;<\/pre>\n<p>Now, implement this in <span class=\"lang:default decode:true crayon-inline \">app.js<\/span>\u00a0 like so:<\/p>\n<pre class=\"lang:js decode:true \">\/\/ file: app.js \r\nvar express = require('express'); \r\n... \r\nvar indexRoute = require('.\/routes\/IndexRoute')(express); \r\n... \r\napp.use('\/', indexRoute);<\/pre>\n<p>I quite like this approach because it keeps code clean and avoids having to use <code>require<\/code> everywhere. Also, by just looking at the module you can figure out what it depends on. For instance, here in <span class=\"lang:default decode:true crayon-inline \">IndexRoute.js<\/span>\u00a0 file (above) you can tell that it depends on express. For more complicated modules that require additional dependencies you can keep adding them in the module function declaration. For example:<\/p>\n<pre class=\"lang:js decode:true\">function UserRoute(express, userModel, moment) { \r\n... \r\n}\r\nmodule.exports = UserRoute;<\/pre>\n<p>Here the <span class=\"lang:default decode:true crayon-inline\">UserRoute<\/span>\u00a0clearly depends on express, <span class=\"lang:default decode:true crayon-inline\">userModel<\/span>\u00a0(mongoose model) and moment (npm moment.js).<\/p>\n<p>Sometimes when developing cool and awesome stuff, it is easy to get carried away. For instance, in the case above, the <span class=\"lang:default decode:true crayon-inline \">UserRoute<\/span>\u00a0 is doing too much. It&#8217;s doing something with time using moment and also manipulating user in the database using <span class=\"lang:default decode:true crayon-inline \">userModel<\/span>\u00a0. While it&#8217;s borderline fine (with massive cringe), you might want to think about what the <span class=\"lang:default decode:true crayon-inline\">UserRoute <\/span>\u00a0should actually be doing.<\/p>\n<p>In most cases, the way I group responsibilities is following. Everything suffixed with <span class=\"lang:default decode:true crayon-inline \">Route<\/span>\u00a0 manages Expressjs route mappings. One level down is <span class=\"lang:default decode:true crayon-inline \">Controller<\/span>\u00a0 which actually manages what&#8217;s being done to that endpoint that has been mapped. Another level down is <span class=\"lang:default decode:true crayon-inline \">Service<\/span>\u00a0. This layer is more intelligent than controller and most of your logic should go in here. Lastly, you have <span class=\"lang:default decode:true crayon-inline\">Model<\/span>\u00a0. In case of mongoose this is just a model definition but there&#8217;s nothing to stop you from doing more complex data interactions here.<\/p>\n<p>This kind of code separation really helps as it is highly cohesive which in turn makes it testable. Now you can test the service and inject all its dependencies directly using the function parameters. Also, when you change definition of a service, it will break all your tests and yes that is a good thing because those broken tests will tell you the size of your impact crater that the change has created. If you didn&#8217;t have those tests in place, you might have never known and maybe spent hours debugging trying to find out what went wrong. I know this because I&#8217;ve been there!<\/p>\n<p>One more advantage of this that I can see is that it will stop you from accidently redefining something in your code that you would&#8217;ve if you used require all the time. This is especially true with Mongoose model definitions. It doesn&#8217;t like it when you define a model twice and that is really easy to do when you&#8217;re using require. Using this method, you require your model once in app.js and then pass it down to every service that requires it. In some cases it could also make your code more efficient as you aren&#8217;t redefining objects evey time something like a service call is made.<\/p>\n<p>I&#8217;m still working on finding the best way to write applications. Every time I write a new application, I learn about a new problem which I then set out to solve. I hope you do too!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 06 Feb 2016 22:42:55 +0000","created_by":1,"updated_at":"Sun, 07 Feb 2016 09:31:38 +0000","updated_by":1,"published_at":"Sat, 06 Feb 2016 22:42:55 +0000","published_by":1},{"id":313,"title":"Building a machine learning recommendation system in under a minute","slug":"building-a-machine-learning-recommendation-system-in-under-a-minute","markdown":"\nTheoratically there are only two steps to this (well, three if you count the <span class=\"lang:default decode:true crayon-inline\">cd<\/span>).  \n Clone GitHub repository available [here](https:\/\/github.com\/manthanhd\/apache-mahout-recommendation-starter).\n\ngit clone https:\/\/github.com\/manthanhd\/apache-mahout-recommendation-starter.git\n\nChange to that directory using <span class=\"lang:default decode:true crayon-inline\">cd<\/span>. Assuming you have <span class=\"lang:default decode:true crayon-inline \">gradle<\/span> installed, run:\n\ngradle clean run\n\nYou should see output like:\n\n11:47:58: Executing external tasks 'clean run'... :clean :compileJava :processResources :classes log4j:WARN No appenders could be found for logger (org.apache.mahout.cf.taste.impl.model.file.FileDataModel). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http:\/\/logging.apache.org\/log4j\/1.2\/faq.html#noconfig for more info. :run RecommendedItem[item:12, value:4.8328104] RecommendedItem[item:13, value:4.6656213] RecommendedItem[item:14, value:4.331242] BUILD SUCCESSFUL Total time: 0.52 secs 11:47:58: External tasks execution finished 'clean run'.\n\nThe highlighted lines are the recommendations that have been generated. The training data set for this can be found in <span class=\"lang:default decode:true crayon-inline \">src\/main\/resources\/dataset.csv<\/span> file. You can also find it [here](https:\/\/raw.githubusercontent.com\/manthanhd\/apache-mahout-recommendation-starter\/master\/src\/main\/resources\/dataset.csv).\n\nFrom those highlighted lines, the <span class=\"lang:default decode:true crayon-inline\">item:12<\/span>, <span class=\"lang:default decode:true crayon-inline \">item:13<\/span> and <span class=\"lang:default decode:true crayon-inline\">item:14<\/span> refer to the <span class=\"lang:default decode:true crayon-inline \">ItemId<\/span> (second column in data set) and <span class=\"lang:default decode:true crayon-inline\">value:4.83<\/span>, <span class=\"lang:default decode:true crayon-inline\">value:4.66<\/span> and <span class=\"lang:default decode:true crayon-inline \">value:4.33<\/span> refer to the recommendation strength. This range depends on the implementation.\n\npublic static void main(String[] args) throws TasteException { Starter starter = new Starter(); starter.recommendFor(2, 3); }\n\nLooking at the code in <span class=\"lang:default decode:true crayon-inline\">Starter.java<\/span> file, the recommendation is made for <span class=\"lang:default decode:true crayon-inline\">UserId<\/span> (first column in data set) 2. By providing 3 as the second argument we\u2019re asking for 3 item recommendations for this user.\n\nWell, so now that you have the basic framework in place, feel free to play with the <span class=\"lang:default decode:true crayon-inline \">UserId<\/span> values for recommendation, recommendation sizes or even with the data set itself. The more training data you add to the data set, the more accurate its recommendations will be.\n\n\n","html":"<p>Theoratically there are only two steps to this (well, three if you count the <span class=\"lang:default decode:true crayon-inline\">cd<\/span>).<br \/>\nClone GitHub repository available <a href=\"https:\/\/github.com\/manthanhd\/apache-mahout-recommendation-starter\" target=\"_blank\">here<\/a>.<\/p>\n<pre class=\"lang:default decode:true \">git clone https:\/\/github.com\/manthanhd\/apache-mahout-recommendation-starter.git<\/pre>\n<p>Change to that directory using <span class=\"lang:default decode:true crayon-inline\">cd<\/span>. Assuming you have <span class=\"lang:default decode:true crayon-inline \">gradle<\/span> installed, run:<\/p>\n<pre class=\"lang:default decode:true\">gradle clean run<\/pre>\n<p>You should see output like:<\/p>\n<pre class=\"lang:default mark:10-12 decode:true\">11:47:58: Executing external tasks 'clean run'...\r\n:clean\r\n:compileJava\r\n:processResources\r\n:classes\r\nlog4j:WARN No appenders could be found for logger (org.apache.mahout.cf.taste.impl.model.file.FileDataModel).\r\nlog4j:WARN Please initialize the log4j system properly.\r\nlog4j:WARN See http:\/\/logging.apache.org\/log4j\/1.2\/faq.html#noconfig for more info.\r\n:run\r\nRecommendedItem[item:12, value:4.8328104]\r\nRecommendedItem[item:13, value:4.6656213]\r\nRecommendedItem[item:14, value:4.331242]\r\n\r\nBUILD SUCCESSFUL\r\n\r\nTotal time: 0.52 secs\r\n11:47:58: External tasks execution finished 'clean run'.<\/pre>\n<p>The highlighted lines are the recommendations that have been generated. The training data set for this can be found in <span class=\"lang:default decode:true crayon-inline \">src\/main\/resources\/dataset.csv<\/span> file. You can also find it <a href=\"https:\/\/raw.githubusercontent.com\/manthanhd\/apache-mahout-recommendation-starter\/master\/src\/main\/resources\/dataset.csv\" target=\"_blank\">here<\/a>.<\/p>\n<p>From those highlighted lines, the <span class=\"lang:default decode:true crayon-inline\">item:12<\/span>, <span class=\"lang:default decode:true crayon-inline \">item:13<\/span> and <span class=\"lang:default decode:true crayon-inline\">item:14<\/span> refer to the <span class=\"lang:default decode:true crayon-inline \">ItemId<\/span> (second column in data set) and <span class=\"lang:default decode:true crayon-inline\">value:4.83<\/span>, <span class=\"lang:default decode:true crayon-inline\">value:4.66<\/span> and <span class=\"lang:default decode:true crayon-inline \">value:4.33<\/span> refer to the recommendation strength. This range depends on the implementation.<\/p>\n<pre class=\"lang:default mark:3 decode:true\">public static void main(String[] args) throws TasteException {\r\n    Starter starter = new Starter();\r\n    starter.recommendFor(2, 3);\r\n}<\/pre>\n<p>Looking at the code in <span class=\"lang:default decode:true crayon-inline\">Starter.java<\/span> file, the recommendation is made for <span class=\"lang:default decode:true crayon-inline\">UserId<\/span> (first column in data set) 2. By providing 3 as the second argument we&#8217;re asking for 3 item recommendations for this user.<\/p>\n<p>Well, so now that you have the basic framework in place, feel free to play with the <span class=\"lang:default decode:true crayon-inline \">UserId<\/span> values for recommendation, recommendation sizes or even with the data set itself. The more training data you add to the data set, the more accurate its recommendations will be.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 10 Feb 2016 05:45:31 +0000","created_by":1,"updated_at":"Sun, 07 Feb 2016 12:57:32 +0000","updated_by":1,"published_at":"Wed, 10 Feb 2016 05:45:31 +0000","published_by":1},{"id":326,"title":"Decision making framework for Community of Practices (Draft 2)","slug":"decision-making-framework-for-community-of-practices-draft-2","markdown":"\n\n# \u201cEmpowering, open and well-managed\u201d\n\n\n## **Empowering**\n\nEveryone in the CoP and outside CoP must be able to demonstrate idea, opinions and thoughts. The CoP must respectfully accommodate individual thoughts and factor in thoughts of every participating individuals when making decision regarding a new idea or thought process.\n\nIf sufficient number of members are not present in the CoP to participate in the decision then the decision must be withheld until such people in required capacity are present. However, if required, due to a valid business reason, this can be overridden and an executive decision can be made with the number of members available at the time.\n\nIn such a case, the community (when members in sufficient capacity are present) has the right to question, criticise and challenge the decision that has been made. If the executive decision maker is unable to provide reasonable reason as to why the decision was made (why it was made in absence of sufficient community members as well), the community has the right to undo \/ repair the decision, even to full extent where possible.\n\nDepending on the size of the community, it may not be possible to accommodate everyone in every decision all the time as not all members might be able to make it to the community all the time. In such a case one of the two below techniques can be used to distribute power.\n\n### **Two Twins**\n\nEach community member elects his\/her counterpart. This is the person that they trust with their decision. Ideally, the counterpart must be unique i.e. if someone is already someone else\u2019s counterpart then they can not be elected a counterpart again. If this is happening frequently then you might want to see the \u201c[Executive Lead](https:\/\/docs.google.com\/document\/d\/1247caF8ttr7N9yZLlNbFPTPkSyMDus4_itxt63IrOPQ\/edit#heading=h.wptbowy78vtk)\u201d approach. The counterpart must be someone they trust with a decision and ideally from a different team.\n\nThe idea of choosing the counterpart is that if someone isn\u2019t available at a time when a decision is made then the counterpart can speak on their behalf. This is why this person has to be someone they trust will make a sound judgement. This helps reduce number of people who need to be present when making a decision and also helps avoid people making executive decisions when sufficient number of people aren\u2019t available (as theoretically one person is representing two). This works best when members in the community are equal to or below 20.\n\n### **Executive Lead**\n\nDivide the total members of community by 7, round it down and the result is the number of executive leads the community needs. An executive lead is someone that everyone in the community is comfortable with and is trusting to make a sound decision. In community\u2019s absence a vote is still made within the executive leads but this helps reduce the number of people that need to be present.\n\nBeing an executive lead also comes with responsibility. He\/she is responsible for communicating a decision clearly to the community, along with the reasons if the decision has already been made. When participating in a decision, the executive lead must also think about the wider impact and not just impact in his\/her immediate vicinity (per se). The community has the right to demote an executive lead if sufficient number (see [impact sizes](https:\/\/docs.google.com\/document\/d\/1247caF8ttr7N9yZLlNbFPTPkSyMDus4_itxt63IrOPQ\/edit#heading=h.kp8sz1p9yooi)) think that they\u2019ve had a negative or not much impact in the role.\n\nThe impact size agreement levels also apply to the executive leads. Unless the required number agree, the decision must not be made otherwise. Executive leads nullify the need of having to make an executive decision as they are the community representatives.\n\nDepending on size of the community and type of the organisation executive leads can have more responsibilities that help them justify the need for them to stay being an executive lead. This could range from empowering individuals beyond their potential to influencing the range of impact a particular community has.\n\nIdeally the community should cycle their executive leads once every couple of months, depending on cadence.\n\n\n## **Open**\n\nEvery decision that goes through the community must be communicated very clearly and in \u201cas is\u201d format. Any community member is encouraged to challenge the decision no matter how senior a person who has proposed\/made the decision is. For this, open forums, wiki pages and online chat (eg. slack) can be used.\n\nWhen an idea is presented, it must be sized according to its impact. This could work similarly to story points (range 1, 3, 5, 8, 13, 20). Breakdowns according to impact are as follows:\n\n### **Impact Size: 1, 3**\n\nLow impact. Eg. Someone proposing a minor non breaking change in a library that impacts a small project and future projects.\n\nNeeds agreement from 60% of the community.\n\n### **Impact Size: 8, 13**\n\nMedium impact. Eg. Someone proposing a major non breaking change\/feature in a library that may impact multiple existing projects and all future projects.\n\nNeeds agreement from 70% of the community.\n\n### **Impact Size: 20, 40**\n\nHigh Impact. Eg. Someone proposing a change in framework that is being used. This change impacts current projects and all future projects.\n\nNeeds agreement from at least 80% of the community.\n\n\n## **Well-managed**\n\nCommunity refines its own processes to make sure that they stay current with changing times. This can be achieved with a retrospective every once or twice a month (depending on cadence). Defect within a process should be raised like a normal issue or idea and discussed as such. The community as a whole has the power to improve, challenge, criticise and disagree with its own processes. These processes shouldn\u2019t be treated as a barrier but more like a malleable guideline that is in place to grow the community. If it is not being used in line with the ideology then the community is empowered to replace its processes with ones that it sees fit.\n\nThe goal of the community is to create a utopia where people can openly voice their ideas, share opinions and collectively improve the organisation as a whole.\n\n\n","html":"<h1 style=\"text-align: center;\">\u201cEmpowering, open and well-managed\u201d<\/h1>\n<h2><strong>Empowering<\/strong><\/h2>\n<p>Everyone in the CoP and outside CoP must be able to demonstrate idea, opinions and thoughts. The CoP must respectfully accommodate individual thoughts and factor in thoughts of every participating individuals when making decision regarding a new idea or thought process.<\/p>\n<p>If sufficient number of members are not present in the CoP to participate in the decision then the decision must be withheld until such people in required capacity are present. However, if required, due to a valid business reason, this can be overridden and an executive decision can be made with the number of members available at the time.<!--more--><\/p>\n<p>In such a case, the community (when members in sufficient capacity are present) has the right to question, criticise and challenge the decision that has been made. If the executive decision maker is unable to provide reasonable reason as to why the decision was made (why it was made in absence of sufficient community members as well), the community has the right to undo \/ repair the decision, even to full extent where possible.<\/p>\n<p>Depending on the size of the community, it may not be possible to accommodate everyone in every decision all the time as not all members might be able to make it to the community all the time. In such a case one of the two below techniques can be used to distribute power.<\/p>\n<h3><strong>Two Twins<\/strong><\/h3>\n<p>Each community member elects his\/her counterpart. This is the person that they trust with their decision. Ideally, the counterpart must be unique i.e. if someone is already someone else\u2019s counterpart then they can not be elected a counterpart again. If this is happening frequently then you might want to see the \u201c<a href=\"https:\/\/docs.google.com\/document\/d\/1247caF8ttr7N9yZLlNbFPTPkSyMDus4_itxt63IrOPQ\/edit#heading=h.wptbowy78vtk\">Executive Lead<\/a>\u201d approach. The counterpart must be someone they trust with a decision and ideally from a different team.<\/p>\n<p>The idea of choosing the counterpart is that if someone isn\u2019t available at a time when a decision is made then the counterpart can speak on their behalf. This is why this person has to be someone they trust will make a sound judgement. This helps reduce number of people who need to be present when making a decision and also helps avoid people making executive decisions when sufficient number of people aren\u2019t available (as theoretically one person is representing two). This works best when members in the community are equal to or below 20.<\/p>\n<h3><strong>Executive Lead<\/strong><\/h3>\n<p>Divide the total members of community by 7, round it down and the result is the number of executive leads the community needs. An executive lead is someone that everyone in the community is comfortable with and is trusting to make a sound decision. In community\u2019s absence a vote is still made within the executive leads but this helps reduce the number of people that need to be present.<\/p>\n<p>Being an executive lead also comes with responsibility. He\/she is responsible for communicating a decision clearly to the community, along with the reasons if the decision has already been made. When participating in a decision, the executive lead must also think about the wider impact and not just impact in his\/her immediate vicinity (per se). The community has the right to demote an executive lead if sufficient number (see <a href=\"https:\/\/docs.google.com\/document\/d\/1247caF8ttr7N9yZLlNbFPTPkSyMDus4_itxt63IrOPQ\/edit#heading=h.kp8sz1p9yooi\">impact sizes<\/a>) think that they\u2019ve had a negative or not much impact in the role.<\/p>\n<p>The impact size agreement levels also apply to the executive leads. Unless the required number agree, the decision must not be made otherwise. Executive leads nullify the need of having to make an executive decision as they are the community representatives.<\/p>\n<p>Depending on size of the community and type of the organisation executive leads can have more responsibilities that help them justify the need for them to stay being an executive lead. This could range from empowering individuals beyond their potential to influencing the range of impact a particular community has.<\/p>\n<p>Ideally the community should cycle their executive leads once every couple of months, depending on cadence.<\/p>\n<h2><strong>Open<\/strong><\/h2>\n<p>Every decision that goes through the community must be communicated very clearly and in \u201cas is\u201d format. Any community member is encouraged to challenge the decision no matter how senior a person who has proposed\/made the decision is. For this, open forums, wiki pages and online chat (eg. slack) can be used.<\/p>\n<p>When an idea is presented, it must be sized according to its impact. This could work similarly to story points (range 1, 3, 5, 8, 13, 20). Breakdowns according to impact are as follows:<\/p>\n<h3><strong>Impact Size: 1, 3<\/strong><\/h3>\n<p>Low impact. Eg. Someone proposing a minor non breaking change in a library that impacts a small project and future projects.<\/p>\n<p>Needs agreement from 60% of the community.<\/p>\n<h3><strong>Impact Size: 8, 13<\/strong><\/h3>\n<p>Medium impact. Eg. Someone proposing a major non breaking change\/feature in a library that may impact multiple existing projects and all future projects.<\/p>\n<p>Needs agreement from 70% of the community.<\/p>\n<h3><strong>Impact Size: 20, 40<\/strong><\/h3>\n<p>High Impact. Eg. Someone proposing a change in framework that is being used. This change impacts current projects and all future projects.<\/p>\n<p>Needs agreement from at least 80% of the community.<\/p>\n<h2><strong>Well-managed<\/strong><\/h2>\n<p>Community refines its own processes to make sure that they stay current with changing times. This can be achieved with a retrospective every once or twice a month (depending on cadence). Defect within a process should be raised like a normal issue or idea and discussed as such. The community as a whole has the power to improve, challenge, criticise and disagree with its own processes. These processes shouldn\u2019t be treated as a barrier but more like a malleable guideline that is in place to grow the community. If it is not being used in line with the ideology then the community is empowered to replace its processes with ones that it sees fit.<\/p>\n<p>The goal of the community is to create a utopia where people can openly voice their ideas, share opinions and collectively improve the organisation as a whole.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 11 Feb 2016 10:53:13 +0000","created_by":1,"updated_at":"Thu, 11 Feb 2016 10:53:13 +0000","updated_by":1,"published_at":"Thu, 11 Feb 2016 10:53:13 +0000","published_by":1},{"id":336,"title":"Notes from AWS Developer Training (Day One)","slug":"notes-from-aws-developer-training-day-one","markdown":"\n\n## Basics\n\nService client API has objects for request and response data. Contrasting old way of retrieving things using the AWS SDK with the new way, it looks like they have switched from SOAP API to a RESTful API. The old way requires you to create Request and Response objects every time you want to do anything with the AWS API. Looking into the implementation, this looks a lot like SOAP.\n\nThe new way is a lot more neater. A request can be built using builders that work on a conceptual model of the request. Response is also a lot more conceptual and easy to read.\n\n### Signing requests with your credentials\n\nSignature prevents the request from being tampered as the signature will become invalid as soon as someone changes the request in transit. It is also time based so that the replay attacks can be prevented.\n\nSDK signs requests automatically so normally no manual intervention is required.\n\n\n## SDKs\n\nRegarding regions, when instantiating the service client, a region must be set. In all languages, except Java, if a region isn\u2019t specified, it will go and find the region specified in .aws\/credentials file under [default].\n\nExceptions are of two types in Java and .Net: AmazonServiceException is exception coming from the service itself, I.e. Remote. AmazonClientException is an exception happening on the client side due to bad request. This is slightly different in JavaScript due to callbacks. Here, callbacks are always in function(error, data) format.\n\nSDK will retry most requests that fail except ones that have service errors like access denied etc.\n\nA set of client credentials won\u2019t be locked out unless all the requests that are coming in look like an attack.\n\n\n## AWS Data Storage\n\nS3, Glacier, DynamoDB, ElastiCache, RDS, RedShift.\n\nEvery byte you change will need a reupload of the whole file. Not good for data that is changing constantly. Use S3 if you want your data to be read a lot more than changed. Highly scalable and durable. Allows hosting of static websites. Two plans: S3 Standard and S3 Standard \u2013 Infrequent Access.\n\nUse Glacier for files that you want to keep for long time. Archival solution, cheap long term storage. Retrieval is slow, about 3-5 hours to process a file retrieval. Old backups, auditing data, log files for keep sake should go in Glacier. Create a job in S3 that moves a file that hasn\u2019t been read in one or two months to Glacier and then another policy that deletes old data after 3-5 years from Glacier.\n\nDynamoDB NoSQL Key Value pair solution. For data that\u2019s not too complex or relational or transactional. Scales horizontally as per need.\n\nRDS is traditional relational database. Use for transactional or relational data. Scales horizontally with automatic failover to secondary instances. Allows variety of engines like Amazon Aurora, Oracle, MS SQL Server, Postgres, MySQL etc.\n\nElastiCache caching solution. Store lightweight short lived data. Blazing fast retrieval. Allows creation and scale cache cluster. Based on memcache and redis.\n\nRedShift allows complex queries on massive data sets (petabyte scale). Data warehousing solution. Massively parallel model. Takes your query, compiles it and runs it in parallel across a large number of instances. Customers could take their load balancer logs, put them into RedShift and then compile list of customers and their frequency including their country of origin. RedShift provides JDBC and ODBC drivers to enable you to use familiar SQL client tools.\n\n\n## Developing storage solutions with Amazon S3\n\nData is stored in S3 buckets as objects. The bucket names must be globally unique. Bucket names: 3-63 chars, lowercase letters, numbers, hyphens and no periods.\n\nWhen storing files\/objects in a bucket, prefer to use the path structure for file names so that rules can be created easily. Paths actually has no meaning to S3 but it\u2019s useful for us as it enables rule creation.\n\nFor retrieval, use path style https:\/\/<region specific endpoint>\/<bucket name>\/<object name> or virtual hosted style http:\/\/<bucket name>.s3.amazonaws.com\/<object name>.\n\nWhile uploading files, if the file size is > 100MB, use multi-part upload. Usually this rule is for files that are > 5GB.\n\nWhile reading, you can read the entire file or only read a specific range of bytes.\n\nWhile deleting, if versioning is enabled, on delete it will revert to the previous version if it exists. However, if versioning is not enabled then the file is permanently deleted. For objects or buckets backed up to glacier, they can be restored back at any time.\n\nPre-signed URLs allows one to provide access to someone to upload or download a file to bucket to\/from S3. Pre-signed URLs expire after a specific time period. This period by default is \u2026.\n\nSSL-encrypted end points allow security of data in transit.\n\nS3 allows CORS to be specified. Add domains to it so that browser requests are allowed. Ex. Allow [www.capitalone.com](http:\/\/www.capitalone.com) to access fonts bucket.\n\nFor high performance, randomise part at the beginning of the key. This is because of the way S3 hashes URLs. If the beginning part is the same, all files end up on the same partition which will create bottleneck.\n\nAvoid unnecessary requests. S3 doesn\u2019t return back 304 for subsequent requests, it will always serve the file. Use a CDN instead.\n\nVerbose logging is allowed for S3. Use this only in development, and not on production system. Every HTTP request generates a request ID pair in logs. Reference this request ID when contacting AWS support.\n\n\n## Storing data using DynamoDB\n\nCan store JSON documents but these must be looked up by the key only.\n\nCommon use cases: Gaming, AdTech, Mobile apps, IoT. Send small data with very high frequency eg. Analytics. Record size must be less than 400KB. Each table has a key attribute which is the partition key. The partition key is hashed and then the data is put in that partition. Hence similar data must have same\/similar partition key.\n\nSort key is optional. For example, date of the transaction can be the sort key. This is the key that can be used for secondary lookup. For instance, if partition key is A and sort key is B, when doing lookup by A, it will return all data matching partition key A. However, we can narrow the search by looking up by partition key A and sort key B.\n\nDynamoDB is eventually consistent, usually within a second of the operation. Request can specify strong consistency but that means data might take longer to write and will cost more. Default is eventual consistency.\n\nRead capacity is reads per second of items up to 4 KB in size. Write capacity unit is in number of 1 KB writes per second.\n\nImportant to choose the right partition key as it directly affects the throughput. The total throughput is divided across partitions so if a set of partition keys are being looked up more often than others Then it means that the partition key must be fixed or changed.\n\nThe secondary index can be changed from the default.\n\nStreams allow firing of events based on an event. For instance, if user Jane changes her exercise from Walking to Spinning, then an update event is fired to all other systems notifying them of the change. Works much like Kinesis streams.\n\nValues can be retrieved matching a specific primary key condition. THis can be refined by providing a filter expression. If the data is looked up by a field that is not index, a scan operation will happen which is much slower than index lookup as it has to scan through the entire database looking for the record.\n\nPagination: returns up to 1 MB of data or limit by number of items to be returned, like 2 records.\n\nQuery can return many items, getItem returns a specific item by key. PutItem adds a new item. UpdateItem updates a specific item. RemoveItem means removing a specific attribute from an item while DeleteItem deletes a whole item. AddItem adds an attribute to an item.\n\nWrites provide free reads as the written value is returned when it\u2019s written. Write operations can be made conditional. For instance perform write operation only when account lock timeout has passed.\n\nIn Java and .NET there\u2019s a persistence model available. This allows the client to do client side operations.\n\nDynamoDB local is a client side application that mimics DynamoDB service. Can be used for testing applications before production and avoid additional cost.\n\nAlso comes with access management to control access to resources and actions.\n\n\n## Best practice\n\nMake sure that there are no hot (overused) partitions. Caching can help relieve this but ideally design partition keys so that this doesn\u2019t happen in first place.\n\nSeparate data that is not used very frequently on a different partitions and then adjust throughout. For instance, latest data can go on a high throughput partitions and old data is moved to one of the older partitions with lower throughput.\n\nUse one to many tables instead of one table with large number of attributes. This is more scalable. Store frequently accessed small attributes in a separate table. Example separate company stock table from company details as the stock table will have information updated and read more frequently than details.\n\nMax size for value is 400KB, can store binary.\n\n\n","html":"<h2 style=\"text-align: justify;\">Basics<\/h2>\n<p style=\"text-align: justify;\">Service client API has objects for request and response data. Contrasting old way of retrieving things using the AWS SDK with the new way, it looks like they have switched from SOAP API to a RESTful API. The old way requires you to create Request and Response objects every time you want to do anything with the AWS API. Looking into the implementation, this looks a lot like SOAP.<\/p>\n<p style=\"text-align: justify;\">The new way is a lot more neater. A request can be built using builders that work on a conceptual model of the request. Response is also a lot more conceptual and easy to read.<!--more--><\/p>\n<h3 style=\"text-align: justify;\">Signing requests with your credentials<\/h3>\n<p style=\"text-align: justify;\">Signature prevents the request from being tampered as the signature will become invalid as soon as someone changes the request in transit. It is also time based so that the replay attacks can be prevented.<\/p>\n<p style=\"text-align: justify;\">SDK signs requests automatically so normally no manual intervention is required.<\/p>\n<h2 style=\"text-align: justify;\">SDKs<\/h2>\n<p style=\"text-align: justify;\">Regarding regions, when instantiating the service client, a region must be set. In all languages, except Java, if a region isn&#8217;t specified, it will go and find the region specified in .aws\/credentials file under [default].<\/p>\n<p style=\"text-align: justify;\">Exceptions are of two types in Java and .Net: AmazonServiceException is exception coming from the service itself, I.e. Remote. AmazonClientException is an exception happening on the client side due to bad request. This is slightly different in JavaScript due to callbacks. Here, callbacks are always in function(error, data) format.<\/p>\n<p style=\"text-align: justify;\">SDK will retry most requests that fail except ones that have service errors like access denied etc.<\/p>\n<p style=\"text-align: justify;\">A set of client credentials won&#8217;t be locked out unless all the requests that are coming in look like an attack.<\/p>\n<h2 style=\"text-align: justify;\">AWS Data Storage<\/h2>\n<p style=\"text-align: justify;\">S3, Glacier, DynamoDB, ElastiCache, RDS, RedShift.<\/p>\n<p style=\"text-align: justify;\">Every byte you change will need a reupload of the whole file. Not good for data that is changing constantly. Use S3 if you want your data to be read a lot more than changed. Highly scalable and durable. Allows hosting of static websites. Two plans: S3 Standard and S3 Standard &#8211; Infrequent Access.<\/p>\n<p style=\"text-align: justify;\">Use Glacier for files that you want to keep for long time. Archival solution, cheap long term storage. Retrieval is slow, about 3-5 hours to process a file retrieval. Old backups, auditing data, log files for keep sake should go in Glacier. Create a job in S3 that moves a file that hasn&#8217;t been read in one or two months to Glacier and then another policy that deletes old data after 3-5 years from Glacier.<\/p>\n<p style=\"text-align: justify;\">DynamoDB NoSQL Key Value pair solution. For data that&#8217;s not too complex or relational or transactional. Scales horizontally as per need.<\/p>\n<p style=\"text-align: justify;\">RDS is traditional relational database. Use for transactional or relational data. Scales horizontally with automatic failover to secondary instances. Allows variety of engines like Amazon Aurora, Oracle, MS SQL Server, Postgres, MySQL etc.<\/p>\n<p style=\"text-align: justify;\">ElastiCache caching solution. Store lightweight short lived data. Blazing fast retrieval. Allows creation and scale cache cluster. Based on memcache and redis.<\/p>\n<p style=\"text-align: justify;\">RedShift allows complex queries on massive data sets (petabyte scale). Data warehousing solution. Massively parallel model. Takes your query, compiles it and runs it in parallel across a large number of instances. Customers could take their load balancer logs, put them into RedShift and then compile list of customers and their frequency including their country of origin. RedShift provides JDBC and ODBC drivers to enable you to use familiar SQL client tools.<\/p>\n<h2 style=\"text-align: justify;\">Developing storage solutions with Amazon S3<\/h2>\n<p style=\"text-align: justify;\">Data is stored in S3 buckets as objects. The bucket names must be globally unique. Bucket names: 3-63 chars, lowercase letters, numbers, hyphens and no periods.<\/p>\n<p style=\"text-align: justify;\">When storing files\/objects in a bucket, prefer to use the path structure for file names so that rules can be created easily. Paths actually has no meaning to S3 but it&#8217;s useful for us as it enables rule creation.<\/p>\n<p style=\"text-align: justify;\">For retrieval, use path style https:\/\/&lt;region specific endpoint&gt;\/&lt;bucket name&gt;\/&lt;object name&gt; or virtual hosted style http:\/\/&lt;bucket name&gt;.s3.amazonaws.com\/&lt;object name&gt;.<\/p>\n<p style=\"text-align: justify;\">While uploading files, if the file size is &gt; 100MB, use multi-part upload. Usually this rule is for files that are &gt; 5GB.<\/p>\n<p style=\"text-align: justify;\">While reading, you can read the entire file or only read a specific range of bytes.<\/p>\n<p style=\"text-align: justify;\">While deleting, if versioning is enabled, on delete it will revert to the previous version if it exists. However, if versioning is not enabled then the file is permanently deleted. For objects or buckets backed up to glacier, they can be restored back at any time.<\/p>\n<p style=\"text-align: justify;\">Pre-signed URLs allows one to provide access to someone to upload or download a file to bucket to\/from S3. Pre-signed URLs expire after a specific time period. This period by default is \u2026.<\/p>\n<p style=\"text-align: justify;\">SSL-encrypted end points allow security of data in transit.<\/p>\n<p style=\"text-align: justify;\">S3 allows CORS to be specified. Add domains to it so that browser requests are allowed. Ex. Allow <a href=\"http:\/\/www.capitalone.com\">www.capitalone.com<\/a> to access fonts bucket.<\/p>\n<p style=\"text-align: justify;\">For high performance, randomise part at the beginning of the key. This is because of the way S3 hashes URLs. If the beginning part is the same, all files end up on the same partition which will create bottleneck.<\/p>\n<p style=\"text-align: justify;\">Avoid unnecessary requests. S3 doesn&#8217;t return back 304 for subsequent requests, it will always serve the file. Use a CDN instead.<\/p>\n<p style=\"text-align: justify;\">Verbose logging is allowed for S3. Use this only in development, and not on production system. Every HTTP request generates a request ID pair in logs. Reference this request ID when contacting AWS support.<\/p>\n<h2 style=\"text-align: justify;\">Storing data using DynamoDB<\/h2>\n<p style=\"text-align: justify;\">Can store JSON documents but these must be looked up by the key only.<\/p>\n<p style=\"text-align: justify;\">Common use cases: Gaming, AdTech, Mobile apps, IoT. Send small data with very high frequency eg. Analytics. Record size must be less than 400KB. Each table has a key attribute which is the partition key. The partition key is hashed and then the data is put in that partition. Hence similar data must have same\/similar partition key.<\/p>\n<p style=\"text-align: justify;\">Sort key is optional. For example, date of the transaction can be the sort key. This is the key that can be used for secondary lookup. For instance, if partition key is A and sort key is B, when doing lookup by A, it will return all data matching partition key A. However, we can narrow the search by looking up by partition key A and sort key B.<\/p>\n<p style=\"text-align: justify;\">DynamoDB is eventually consistent, usually within a second of the operation. Request can specify strong consistency but that means data might take longer to write and will cost more. Default is eventual consistency.<\/p>\n<p style=\"text-align: justify;\">Read capacity is reads per second of items up to 4 KB in size. Write capacity unit is in number of 1 KB writes per second.<\/p>\n<p style=\"text-align: justify;\">Important to choose the right partition key as it directly affects the throughput. The total throughput is divided across partitions so if a set of partition keys are being looked up more often than others Then it means that the partition key must be fixed or changed.<\/p>\n<p style=\"text-align: justify;\">The secondary index can be changed from the default.<\/p>\n<p style=\"text-align: justify;\">Streams allow firing of events based on an event. For instance, if user Jane changes her exercise from Walking to Spinning, then an update event is fired to all other systems notifying them of the change. Works much like Kinesis streams.<\/p>\n<p style=\"text-align: justify;\">Values can be retrieved matching a specific primary key condition. THis can be refined by providing a filter expression. If the data is looked up by a field that is not index, a scan operation will happen which is much slower than index lookup as it has to scan through the entire database looking for the record.<\/p>\n<p style=\"text-align: justify;\">Pagination: returns up to 1 MB of data or limit by number of items to be returned, like 2 records.<\/p>\n<p style=\"text-align: justify;\">Query can return many items, getItem returns a specific item by key. PutItem adds a new item. UpdateItem updates a specific item. RemoveItem means removing a specific attribute from an item while DeleteItem deletes a whole item. AddItem adds an attribute to an item.<\/p>\n<p style=\"text-align: justify;\">Writes provide free reads as the written value is returned when it&#8217;s written. Write operations can be made conditional. For instance perform write operation only when account lock timeout has passed.<\/p>\n<p style=\"text-align: justify;\">In Java and .NET there&#8217;s a persistence model available. This allows the client to do client side operations.<\/p>\n<p style=\"text-align: justify;\">DynamoDB local is a client side application that mimics DynamoDB service. Can be used for testing applications before production and avoid additional cost.<\/p>\n<p style=\"text-align: justify;\">Also comes with access management to control access to resources and actions.<\/p>\n<h2 style=\"text-align: justify;\">Best practice<\/h2>\n<p style=\"text-align: justify;\">Make sure that there are no hot (overused) partitions. Caching can help relieve this but ideally design partition keys so that this doesn&#8217;t happen in first place.<\/p>\n<p style=\"text-align: justify;\">Separate data that is not used very frequently on a different partitions and then adjust throughout. For instance, latest data can go on a high throughput partitions and old data is moved to one of the older partitions with lower throughput.<\/p>\n<p style=\"text-align: justify;\">Use one to many tables instead of one table with large number of attributes. This is more scalable. Store frequently accessed small attributes in a separate table. Example separate company stock table from company details as the stock table will have information updated and read more frequently than details.<\/p>\n<p style=\"text-align: justify;\">Max size for value is 400KB, can store binary.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 02 Mar 2016 22:17:34 +0000","created_by":1,"updated_at":"Wed, 02 Mar 2016 22:21:27 +0000","updated_by":1,"published_at":"Wed, 02 Mar 2016 22:17:34 +0000","published_by":1},{"id":340,"title":"Notes from AWS Developer Training (Day Two)","slug":"notes-from-aws-developer-training-day-two","markdown":"\n\n## Achieving loose coupling with Events\n\nAmazon SQS, SNS, DynamoDB Streams, Kinesis Streams, Lambda.\n\nWith the event driven architecture, two systems don\u2019t need to know about each other. Each of them can fire events while the other responds to that specific event.\n\nSNS has publish\/subscribe model. When publisher pushes, all subscribers immediately get the message. This can be email, SMS, SQS, Lambda etc.\n\nSQS queuing for delivery method. Messages are persisted until they are polled. Extremely scalable. Can potentially contain millions of messages.\n\nSNS and SQS can be used together. SNS delivers to the SQS queues each for one EC2 instance who then carry out a specific task. On push, SQS provides no guarantee for ordering.\n\nDynamoDB streams allow firing of events in a stream. For example, on image upload, the image is sent to S3 and the metadata is stored in DynamoDB. When the metadata is stored, DynamoDB fires off event in a stream which is read by Lambda. This can then do some computation around the data coming from DynamoDB.\n\nAmazon Kinesis Streams is a fully managed streaming data service. Helps aggregate data. Data stored in a Kinesis stream is ordered. Client can read a small segment from the stream. Allows adding of checkpoints so that if the client that is reading from a stream fails, it can continue from the last checkpoint.\n\nLambda is a event handler. Not a replacement but a variation of event driven server less code.\n\n\n## Kinesis\n\nOffers services to load and process large amounts of data. Allows building of streaming data applications.\n\nFirst thing that is to be created in Kinesis is a stream. Data records are put into the stream which are then read out from the other side. Stream has shards. Each shard has 1 MBPS write and 100 MBPS. Reads 5 transactions per second and writes are 1000 records per second. Shards can be merged afterwards if they are not in use. This action is reversible.\n\nEach data record has three parts. Sequence number (automatically created), partition key and then the data to be streamed. Data can be JSON\/XML\/Binary etc. The partition key determines which shard gets the data.\n\nOperations on stream: Create\/List\/Delete. Operations on shard: Retrieve\/Reshard (increase or merge shard count)\/Change data retention period. Every data record has its own expiration time.\n\nWhen data is inserted, the sequence number is automatically assigned and based on the partition key hash it is put in a shard that it selects.\n\nKPL (Kinesis Producer Library) acts as an intermediary between producer and a stream, Kinda transactional in the sense that when asked to insert multiple records, if one of them fails insert due to an issue (eg throughput) it denies request. However, the application can still choose to insert a value regardless of other values failing insert.\n\nKCL (Kinesis Consumer Library) is used by consumers to read\/consume values from the shards. Consumers can read from the beginning of the stream\/latest\/specific part of a stream based on the sequence number. The GetRecordsAPI requires an iterator.\n\nIf the data is being read from the tip of the stream, start the consumers first and then the producers otherwise the consumers won\u2019t read beginning of the data.\n\nWhen storing data randomise partition keys to make sure of even distribution of data between shards. Provision few extra shards to handle unexpected demands. More shards are needed to support several consumers simultaneously reading from a stream.\n\nBatch data before writes for efficiency. On throughput exceeded error, retry\/log and monitor\/reshard shards. Increase DynamoDB table throughput in extreme cases. Best to have DynamoDB handle duplicates as two records with same keys will be an overwrite.\n\nCheck permissions to make sure that the code has access to Amazon Kinesis streams, DynamoDB and CloudWatch services.\n\nHandle exceptions in processRecords method of IRecordProcessor object.\n\n\n## Amazon Simple Work Flow (SWF)\n\nAutomate flow of events. Could also have long running human tasks. For instance, have a workflow to create and email an invoice and then wait for someone to click approve link within the email to then resume the workflow to further dispatch the product.\n\nTwo types of work flow tasks: worker and decider. Decider can have code running behind it. Whatever thing the decider touches, it must have permission to access it. For instance, a decider to read from DynamoDB must have access to read data from DynamoDB. As the name suggests, the decider decides outcome. In the invoicing system, if the user clicks reject button in the invoice email, the decider can then cancel the order but continue with dispatch if the user clicks approve button.\n\nWhen the workflow starts the first time, SWF calls the decider which decides which step to go next. The control is then handed back to SWF from the decider and then based on the outcome of decider, SWF then goes to that step. The tasks that are executed can be executed asynchronously.\n\nNo visual way to create the work flows out of the box. Requires programmatic creation via SDK.\n\n\n## Amazon Simple Queue Service (SQS)\n\nAt least once delivery. Does not have all features of a fully fledged queuing solution. Highly scalable.\n\nProvides dead letter queues for messages that haven\u2019t been delivered.\n\nDistributed nature means that some messages may not be available across all SQS servers.\n\nMessage can appear in the queue after some time specified in delay seconds. Queue can restrict size of messages being put by specifying MaximumMessageSize. MessageRetentionPeriod specifies how long should a message be kept in the queue. This can be up to 14 days.\n\nWhile receiving messages, one can use short polling or long polling. Short polling means that SQS returns messages in short bursts (couple at a time), however, with Long polling, it blocks you until it finds significant number of messages. The block time is specified by the client, could be 5-10 seconds. Use this if processing large number of messages in batch is easier than in small batches.\n\nVisibility timeout is the time in which the message is invisible for a certain time and then becomes visible. This is for systems that need more time before they can pick up a message. The message handle can be used to extend this timeout if more time is needed.\n\nIn Java, client side buffering is allowed where the messages can be pre fetched into a local buffer. Automatically batches messages for send or delete operations.\n\nQueues can be shared to another AWS accounts with or without credentials (anonymously). Admin can also create policies that grant access to a specific queue. This can then be assigned to a role.\n\nRequests incoming to a queue cannot be directly throttled. So if a queue is shared with someone else, they could potentially DDoS the queue and have you charged for each request which could be disastrous. However, this can be implicitly controlled by giving someone access to an Amazon API Gateway endpoint which then throttles all requests from there on to the SQS.\n\n\n## Simple Notification Service (SNS)\n\nPub-sub model. Messages are sent to the SNS which then propagates them down to different services or applications. SNS messages are not persisted while SQS messages are. Also, interaction with SNS is passive (push) while the one with SQS is active (poll). Also, usually in SNS, one publisher talks to multiple consumers while SQS is usually meant for one publisher talking to one subscriber.\n\nMassively useful in decoupling applications. Supports event driven architecture where based on the message, the receivers could each do different things. For instance when an invoice is placed in SNS, one system that is subscribed could process the order associated with the invoice while second system could send email confirming the receipt of the order.\n\nSNS provides good fan out for massively parallel tasks. Example, lambda can listen for SNS messages, do the processing and then push to S3.\n\nDoes not batch messages. Every notification is a message. Does not guarantee ordering of messages. SNS Delivery policy, that applies to HTTP subscribers can be used to control retries in case of message delivery failure. Messages can contain up to 256KB of data.\n\nMessage sizes or format can be controlled using the message structure attribute of the message. For instance, in case of SMS message, the characters can be restricted to 160 characters. Refer to the documentation on the AWS website.\n\nUse cloud watch to keep track of stats about SNS.\n\n\n## Lambda\n\nCompute service similar to EC2. Automatically manages compute resources. Requires zero administration. Supports Python, Java (v8) and Node.js. Lets people focus on code, not administration. Removes need to have a lot of servers for a simple task. Hooks very well into Amazon AWS services. Known as connective tissue for AWS services. Eg. Fetch a file when it\u2019s uploaded to S3, do something with it and then send a SNS push notification with the resulting output of data.\n\nOnce created, lambda function can be invoked by:\n\n1. Push model by publishing an event.\n2. Pull model by watching a stream or source (Kinesis stream or SQS queue polls)\n3. Synchronous invocation by calling a function or API directly from Lambda.\n\n### Push event model\n\nEvent source must itself invoke a lambda function directly. From Amazon Echo, S3, SNS or Cognito. Example: User pushes an item to S3. S3 then pushes an event to Lambda which then assumes an execution role to process the file and upload the processed file back to lambda.\n\n### Pull model\n\nLambda polls the event service and then on event does things. For example polling a DynamoDB or Kinesis stream. The polling does not affect the cost associated with making requests. Upon event detection, it assumes Execution role and processes the event.\n\n### Synchronous model\n\nInvoke function in lambda using RequestResposne invocation type. Function executes and then returns immediately.\n\nWhen granting permissions, the source of the event must have permission to execute lambda functions. Whatever lambda then does must be able to execute operations that it is trying to do. For example, s3 upload event must have access to execute lambda. Then if the lambda function is uploading records to DynamoDB, it should have access to insert a DynamoDB record.\n\nIf the role isn\u2019t specified, it assumes basic lambda role. The basic role has the least privilege of being able to write a log file.\n\nWhen writing a function, choose the amount of memory that you want to allocate. It then allocates CPU power. Additional memory can be configured in 64 MB increments upto 1536 MB. All calls are limited to five minutes of execution time. The default timeout is 3 seconds but can be set to any value between 1 and 300ms. Duration is calculated to nearest 100 milliseconds.\n\nLambda functions be scheduled for execution. For example, get MI data every hour. Schedule can be set for fixed rate at every hour or 15 minutes. A crown expression can be provided.\n\nPackages for lambda functions can go up to 50 MB in size.\n\nFormat for specifying handler for a Java application is <package name>.<class name>::<function name>\n\nFunctions take longer to execute the first time because it creates a new container and allocates resources to it. However its faster from there on.\n\nVery easy integration with AWS API gateway where based on a request, a lambda function can be executed. The incoming request parameters can be mapped to lambda function parameters using a mapping template. For incoming JSON data, the mapping template allows mapping using json path.\n\nSounds like TDD is the best practice for writing lambda functions. All the functions should be strictly tested before publishing them.\n\nWhen subsequent updates are pushed to a lambda function, it automatically versions them. It allows reverting back to an old version if the latest is broken. One can also specify a specific version to lambda to execute. One can also define aliases to lambda versions. Example, dev can point to latest version 5, system test points to 4 and production points to 3. Aliases are just pointers so if one wants to put the latest version in production then one only needs to change the production alias to latest version, eg. 5.\n\nDepending on what a lambda function is doing, it could be a cheaper alternative to an EC2 server.\n\nLambda functions must be written in a stateless way with minimal overhead as they will be executed in a stateless manner. Default soft limit for executing lambda functions in parallel is 100. Depending on use, this could be extended by a simple request. Its advised to use versioning and aliases to deploy lambda functions.\n\nDry run is a feature with Amazon Lambda that allows one to check permissions without actually executing the lambda function.\n\n\n","html":"<h2 style=\"text-align: justify;\">Achieving loose coupling with Events<\/h2>\n<p style=\"text-align: justify;\">Amazon SQS, SNS, DynamoDB Streams, Kinesis Streams, Lambda.<\/p>\n<p style=\"text-align: justify;\">With the event driven architecture, two systems don&#8217;t need to know about each other. Each of them can fire events while the other responds to that specific event.<\/p>\n<p style=\"text-align: justify;\">SNS has publish\/subscribe model. When publisher pushes, all subscribers immediately get the message. This can be email, SMS, SQS, Lambda etc.<\/p>\n<p style=\"text-align: justify;\">SQS queuing for delivery method. Messages are persisted until they are polled. Extremely scalable. Can potentially contain millions of messages.<!--more--><\/p>\n<p style=\"text-align: justify;\">SNS and SQS can be used together. SNS delivers to the SQS queues each for one EC2 instance who then carry out a specific task. On push, SQS provides no guarantee for ordering.<\/p>\n<p style=\"text-align: justify;\">DynamoDB streams allow firing of events in a stream. For example, on image upload, the image is sent to S3 and the metadata is stored in DynamoDB. When the metadata is stored, DynamoDB fires off event in a stream which is read by Lambda. This can then do some computation around the data coming from DynamoDB.<\/p>\n<p style=\"text-align: justify;\">Amazon Kinesis Streams is a fully managed streaming data service. Helps aggregate data. Data stored in a Kinesis stream is ordered. Client can read a small segment from the stream. Allows adding of checkpoints so that if the client that is reading from a stream fails, it can continue from the last checkpoint.<\/p>\n<p style=\"text-align: justify;\">Lambda is a event handler. Not a replacement but a variation of event driven server less code.<\/p>\n<h2 style=\"text-align: justify;\">Kinesis<\/h2>\n<p style=\"text-align: justify;\">Offers services to load and process large amounts of data. Allows building of streaming data applications.<\/p>\n<p style=\"text-align: justify;\">First thing that is to be created in Kinesis is a stream. Data records are put into the stream which are then read out from the other side. Stream has shards. Each shard has 1 MBPS write and 100 MBPS. Reads 5 transactions per second and writes are 1000 records per second. Shards can be merged afterwards if they are not in use. This action is reversible.<\/p>\n<p style=\"text-align: justify;\">Each data record has three parts. Sequence number (automatically created), partition key and then the data to be streamed. Data can be JSON\/XML\/Binary etc. The partition key determines which shard gets the data.<\/p>\n<p style=\"text-align: justify;\">Operations on stream: Create\/List\/Delete. Operations on shard: Retrieve\/Reshard (increase or merge shard count)\/Change data retention period. Every data record has its own expiration time.<\/p>\n<p style=\"text-align: justify;\">When data is inserted, the sequence number is automatically assigned and based on the partition key hash it is put in a shard that it selects.<\/p>\n<p style=\"text-align: justify;\">KPL (Kinesis Producer Library) acts as an intermediary between producer and a stream, Kinda transactional in the sense that when asked to insert multiple records, if one of them fails insert due to an issue (eg throughput) it denies request. However, the application can still choose to insert a value regardless of other values failing insert.<\/p>\n<p style=\"text-align: justify;\">KCL (Kinesis Consumer Library) is used by consumers to read\/consume values from the shards. Consumers can read from the beginning of the stream\/latest\/specific part of a stream based on the sequence number. The GetRecordsAPI requires an iterator.<\/p>\n<p style=\"text-align: justify;\">If the data is being read from the tip of the stream, start the consumers first and then the producers otherwise the consumers won&#8217;t read beginning of the data.<\/p>\n<p style=\"text-align: justify;\">When storing data randomise partition keys to make sure of even distribution of data between shards. Provision few extra shards to handle unexpected demands. More shards are needed to support several consumers simultaneously reading from a stream.<\/p>\n<p style=\"text-align: justify;\">Batch data before writes for efficiency. On throughput exceeded error, retry\/log and monitor\/reshard shards. Increase DynamoDB table throughput in extreme cases. Best to have DynamoDB handle duplicates as two records with same keys will be an overwrite.<\/p>\n<p style=\"text-align: justify;\">Check permissions to make sure that the code has access to Amazon Kinesis streams, DynamoDB and CloudWatch services.<\/p>\n<p style=\"text-align: justify;\">Handle exceptions in processRecords method of IRecordProcessor object.<\/p>\n<h2 style=\"text-align: justify;\">Amazon Simple Work Flow (SWF)<\/h2>\n<p style=\"text-align: justify;\">Automate flow of events. Could also have long running human tasks. For instance, have a workflow to create and email an invoice and then wait for someone to click approve link within the email to then resume the workflow to further dispatch the product.<\/p>\n<p style=\"text-align: justify;\">Two types of work flow tasks: worker and decider. Decider can have code running behind it. Whatever thing the decider touches, it must have permission to access it. For instance, a decider to read from DynamoDB must have access to read data from DynamoDB. As the name suggests, the decider decides outcome. In the invoicing system, if the user clicks reject button in the invoice email, the decider can then cancel the order but continue with dispatch if the user clicks approve button.<\/p>\n<p style=\"text-align: justify;\">When the workflow starts the first time, SWF calls the decider which decides which step to go next. The control is then handed back to SWF from the decider and then based on the outcome of decider, SWF then goes to that step. The tasks that are executed can be executed asynchronously.<\/p>\n<p style=\"text-align: justify;\">No visual way to create the work flows out of the box. Requires programmatic creation via SDK.<\/p>\n<h2 style=\"text-align: justify;\">Amazon Simple Queue Service (SQS)<\/h2>\n<p style=\"text-align: justify;\">At least once delivery. Does not have all features of a fully fledged queuing solution. Highly scalable.<\/p>\n<p style=\"text-align: justify;\">Provides dead letter queues for messages that haven\u2019t been delivered.<\/p>\n<p style=\"text-align: justify;\">Distributed nature means that some messages may not be available across all SQS servers.<\/p>\n<p style=\"text-align: justify;\">Message can appear in the queue after some time specified in delay seconds. Queue can restrict size of messages being put by specifying MaximumMessageSize. MessageRetentionPeriod specifies how long should a message be kept in the queue. This can be up to 14 days.<\/p>\n<p style=\"text-align: justify;\">While receiving messages, one can use short polling or long polling. Short polling means that SQS returns messages in short bursts (couple at a time), however, with Long polling, it blocks you until it finds significant number of messages. The block time is specified by the client, could be 5-10 seconds. Use this if processing large number of messages in batch is easier than in small batches.<\/p>\n<p style=\"text-align: justify;\">Visibility timeout is the time in which the message is invisible for a certain time and then becomes visible. This is for systems that need more time before they can pick up a message. The message handle can be used to extend this timeout if more time is needed.<\/p>\n<p style=\"text-align: justify;\">In Java, client side buffering is allowed where the messages can be pre fetched into a local buffer. Automatically batches messages for send or delete operations.<\/p>\n<p style=\"text-align: justify;\">Queues can be shared to another AWS accounts with or without credentials (anonymously). Admin can also create policies that grant access to a specific queue. This can then be assigned to a role.<\/p>\n<p style=\"text-align: justify;\">Requests incoming to a queue cannot be directly throttled. So if a queue is shared with someone else, they could potentially DDoS the queue and have you charged for each request which could be disastrous. However, this can be implicitly controlled by giving someone access to an Amazon API Gateway endpoint which then throttles all requests from there on to the SQS.<\/p>\n<h2 style=\"text-align: justify;\">Simple Notification Service (SNS)<\/h2>\n<p style=\"text-align: justify;\">Pub-sub model. Messages are sent to the SNS which then propagates them down to different services or applications. SNS messages are not persisted while SQS messages are. Also, interaction with SNS is passive (push) while the one with SQS is active (poll). Also, usually in SNS, one publisher talks to multiple consumers while SQS is usually meant for one publisher talking to one subscriber.<\/p>\n<p style=\"text-align: justify;\">Massively useful in decoupling applications. Supports event driven architecture where based on the message, the receivers could each do different things. For instance when an invoice is placed in SNS, one system that is subscribed could process the order associated with the invoice while second system could send email confirming the receipt of the order.<\/p>\n<p style=\"text-align: justify;\">SNS provides good fan out for massively parallel tasks. Example, lambda can listen for SNS messages, do the processing and then push to S3.<\/p>\n<p style=\"text-align: justify;\">Does not batch messages. Every notification is a message. Does not guarantee ordering of messages. SNS Delivery policy, that applies to HTTP subscribers can be used to control retries in case of message delivery failure. Messages can contain up to 256KB of data.<\/p>\n<p style=\"text-align: justify;\">Message sizes or format can be controlled using the message structure attribute of the message. For instance, in case of SMS message, the characters can be restricted to 160 characters. Refer to the documentation on the AWS website.<\/p>\n<p style=\"text-align: justify;\">Use cloud watch to keep track of stats about SNS.<\/p>\n<h2 style=\"text-align: justify;\">Lambda<\/h2>\n<p style=\"text-align: justify;\">Compute service similar to EC2. Automatically manages compute resources. Requires zero administration. Supports Python, Java (v8) and Node.js. Lets people focus on code, not administration. Removes need to have a lot of servers for a simple task. Hooks very well into Amazon AWS services. Known as connective tissue for AWS services. Eg. Fetch a file when it&#8217;s uploaded to S3, do something with it and then send a SNS push notification with the resulting output of data.<\/p>\n<p style=\"text-align: justify;\">Once created, lambda function can be invoked by:<\/p>\n<ol style=\"text-align: justify;\">\n<li>Push model by publishing an event.<\/li>\n<li>Pull model by watching a stream or source (Kinesis stream or SQS queue polls)<\/li>\n<li>Synchronous invocation by calling a function or API directly from Lambda.<\/li>\n<\/ol>\n<h3 style=\"text-align: justify;\">Push event model<\/h3>\n<p style=\"text-align: justify;\">Event source must itself invoke a lambda function directly. From Amazon Echo, S3, SNS or Cognito. Example: User pushes an item to S3. S3 then pushes an event to Lambda which then assumes an execution role to process the file and upload the processed file back to lambda.<\/p>\n<h3 style=\"text-align: justify;\">Pull model<\/h3>\n<p style=\"text-align: justify;\">Lambda polls the event service and then on event does things. For example polling a DynamoDB or Kinesis stream. The polling does not affect the cost associated with making requests. Upon event detection, it assumes Execution role and processes the event.<\/p>\n<h3 style=\"text-align: justify;\">Synchronous model<\/h3>\n<p style=\"text-align: justify;\">Invoke function in lambda using RequestResposne invocation type. Function executes and then returns immediately.<\/p>\n<p style=\"text-align: justify;\">When granting permissions, the source of the event must have permission to execute lambda functions. Whatever lambda then does must be able to execute operations that it is trying to do. For example, s3 upload event must have access to execute lambda. Then if the lambda function is uploading records to DynamoDB, it should have access to insert a DynamoDB record.<\/p>\n<p style=\"text-align: justify;\">If the role isn&#8217;t specified, it assumes basic lambda role. The basic role has the least privilege of being able to write a log file.<\/p>\n<p style=\"text-align: justify;\">When writing a function, choose the amount of memory that you want to allocate. It then allocates CPU power. Additional memory can be configured in 64 MB increments upto 1536 MB. All calls are limited to five minutes of execution time. The default timeout is 3 seconds but can be set to any value between 1 and 300ms. Duration is calculated to nearest 100 milliseconds.<\/p>\n<p style=\"text-align: justify;\">Lambda functions be scheduled for execution. For example, get MI data every hour. Schedule can be set for fixed rate at every hour or 15 minutes. A crown expression can be provided.<\/p>\n<p style=\"text-align: justify;\">Packages for lambda functions can go up to 50 MB in size.<\/p>\n<p style=\"text-align: justify;\">Format for specifying handler for a Java application is &lt;package name&gt;.&lt;class name&gt;::&lt;function name&gt;<\/p>\n<p style=\"text-align: justify;\">Functions take longer to execute the first time because it creates a new container and allocates resources to it. However its faster from there on.<\/p>\n<p style=\"text-align: justify;\">Very easy integration with AWS API gateway where based on a request, a lambda function can be executed. The incoming request parameters can be mapped to lambda function parameters using a mapping template. For incoming JSON data, the mapping template allows mapping using json path.<\/p>\n<p style=\"text-align: justify;\">Sounds like TDD is the best practice for writing lambda functions. All the functions should be strictly tested before publishing them.<\/p>\n<p style=\"text-align: justify;\">When subsequent updates are pushed to a lambda function, it automatically versions them. It allows reverting back to an old version if the latest is broken. One can also specify a specific version to lambda to execute. One can also define aliases to lambda versions. Example, dev can point to latest version 5, system test points to 4 and production points to 3. Aliases are just pointers so if one wants to put the latest version in production then one only needs to change the production alias to latest version, eg. 5.<\/p>\n<p style=\"text-align: justify;\">Depending on what a lambda function is doing, it could be a cheaper alternative to an EC2 server.<\/p>\n<p style=\"text-align: justify;\">Lambda functions must be written in a stateless way with minimal overhead as they will be executed in a stateless manner. Default soft limit for executing lambda functions in parallel is 100. Depending on use, this could be extended by a simple request. Its advised to use versioning and aliases to deploy lambda functions.<\/p>\n<p style=\"text-align: justify;\">Dry run is a feature with Amazon Lambda that allows one to check permissions without actually executing the lambda function.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 02 Mar 2016 22:21:03 +0000","created_by":1,"updated_at":"Wed, 02 Mar 2016 22:21:47 +0000","updated_by":1,"published_at":"Wed, 02 Mar 2016 22:21:03 +0000","published_by":1},{"id":346,"title":"Notes from AWS Developer Training (Day Three)","slug":"notes-from-aws-developer-training-day-three","markdown":"\n### Creating Serverless Projects\n\nIn a server less environment, Amazon Lambda can be used in conjunction with Amazon API Gateway for HTTP interfacing, Amazon S3 for storage, Amazon ElastiCache for caching and DynamoDB\/RDS for database storage. Checkout the Servless Application Framework at [serverless.com](http:\/\/serverless.com) for more info.\n\n\n## Securing data in AWS\n\nInfrastructure should be treated as code, I.e. Version control systems. Automate security and increase testing frequency via CI\/CD. Fail early and fast. Test at production scale. No need to keep the test servers alive. Spin up the entire production environment in test, deploy the code, run the tests and then tear down the environment.\n\nDevOps provides efficiency that speeds up the development lifecycle (build > test > release > monitor > plan). Validate security in each step to avoid slowing down the whole lifecycle.\n\nAmazon\u2019s shared responsibility model.\n\nHow do we get certificates for the internal private DNS for an AWS EC2 instance?\n\nAll requests to AWS must be signed using access key ID and secret access key. If using the SDK, this is done automatically. However, if this is done manually, use signature version 4 to sign requests.\n\nSupport v2 and v4, most services support v4. V4 is more secure as it derives a key from secret access key and then uses that derived key to sign requests. V2 on the other hand does this very plainly as it uses the secret key directly to sign requests.\n\nOne way to provide permissions is to add users to a group that has policies attached to it. All users in that group have permissions assigned to them as long as they are in that group. Drawback is that this is a static set of permissions. As in if two developers are out on holiday for \u00a0weeks and only one is available, all three developers still have all the access. Also if one of the developers need access to EMR, and if policy is changed to include that permission, all the developers, whether or not they need it, inherit that permission.\n\nBetter way to do this is to use IAM roles. Create a role called EMR with the policy. The user that needs permission assumes that role. This provides that user with a new set of access key and secret key that has access to EMR.\n\nWhen authenticating with SSO or AD, the authenticating application checks if the user is in the right group to access the target application. If the user isn\u2019t in the right group then it must check if there are roles that can be assumed to access the target application. If the roles exist, the authenticating application then checks if the user is able or allowed to assume any of those roles. If the user is allowed, the application assumes the role for him, granting access. One of the ways this can be done is by using SAML assertion with IAM.\n\n\n## Getting started with IAM\n\nOnce you have the root account, create a general IAM account for yourself. Assign permissions that you need to yourself. If a admin is needed, create a group called Admins and then assign basic admin permissions to that role. When a new admin joins, he\/she can be added to Admins group. Give permissions to admins group as needed.\n\nFor a developer who joins the team, again create a Developers group and assign permissions to that group as needed. When a special permission is needed to access a specific thing, create a role for that and allow people to assume roles for a limited time or as long as they need it for. For temporary contractors and consultants, assign one permission called Assume Role. This allows them to temporarily assume roles when needed.\n\nThe assume role permission can be restricted by IP, as in you\u2019re only allowed to assume a role when you\u2019re on site.\n\nFor authentication, AWS management console needs username and password. Other tools like AWS CLI, SDKs and API queries need access key and secret keys.\n\nNEVER USE ROOT ACCOUNT CREDENTIALS ANYWHERE.\n\nTwo types of IAM permission types. User centric where each user is given access to specific resources. Second type of permission type is resource centric where specific access to a resource is opened up to a set of users.\n\nUse AWS provided library of pre-defined Policy Templates or AWS policy generator. Preferably, create policies in object oriented way using AWS Cloud Formation or AWS SDKs.\n\nIAM policy rule procedure is to deny access by default until access is explicitly allowed in all policies. If something is explicitly denied, it can never be allowed. Explicit deny cannot be overwritten.\n\nMight be possible to restrict users from uploading content above a certain size.\n\nFor external users, use OpenID Connect for authentication with external services like Google\/Facebook\/Twitter login along with IAM for authorisation.\n\n\n## Caching\n\nCloudFront caches content based on edge location. If content is available, it will serve from its edge location, if not, it will serve from S3.\n\nEach caching behaviour can be configured. Params like Path Pattern to be cached, request origin to forward to, if forward URLs should have query strings in them etc.\n\nPath patterns max is 255 chars. Case sensitive. Eg. \/*.jpg means cache all JPG files.\n\nWith regards to query strings, cache behaviour will differ based on value of query strings in the request. Example: \/hello?query1=a and \/hello?query2=b will result in different cache copies served. Cache can also serve different URLs based on different request header values or cookies.\n\nThe time to live (TTL) value controls how long the value should stay in the cache. Longer TTLs for static content, short for dynamic.\n\nA custom error page page can be configured to be serve if the cache fails to find the origin content (if origin changed but cache didn\u2019t for some reason and its expecting the file to be present on origin).\n\nAuthentication works via public\/private key pair. CloudFront has the public key while the application has the private key. The URLs must be signed before sending to cloud front. The application signs using its private key, sends URL to cloud front. If the URL is valid, cloud front serves the content.\n\nUser sessions can be stored in DynamoDB or ElastiCache, depending on need.\n\nElastiCache runs, underneath the covers, me cache or redis cluster. Provides managed service that include patching and backup.\n\nMemcache is known for high performance and scale. Allows running of large nodes with multiple cores or threads. Ability to scale in or out. Data can be partitioned across multiple shards. One downside is that the data model is very simple.\n\nRedis is quite similar to Memcache where it\u2019s open source and in memory. Redis can only scale vertically (check!!). Has simple master\/slave model, not shards (check!!). Comes with complex data types. Has pub\/sub capability \u2013 the client being informed of events on the server. Not very good for multi-threaded performance (check!!).\n\nCache is loaded by lazy loading as in data is only loaded when required.\n\n\n## CloudWatch\n\nOne can use CloudWatch agent to aggregate logs from EC2 instances to the central CloudWatch server. To do this, install the cloudwatch agent on the EC2 instance.\n\nFrom any log streams, custom cloudwatch metric can be created. Once the metric is created, an alarm can be configured.\n\nCloud watch alarms can call HTTP endpoints, like calling web hooks to trigger different kind of behaviours based on the event.\n\nCloud watch metrics can be aggregated on the server side by providing params like start and end times, period and metric names.\n\nCloudWatch dashboards can be used to provide a bird\u2019s eye view of all metrics pertaining to a specific application. It also allows adding of text widgets which one can use to provide some free text or images.\n\nJSON path expression can be used to search logs within CloudWatch. This style of expression can also be used to extract values from logs. Once values are extracted, they can be used to create a metric. For instance, use Regex to extract response times from tomcat logs, create a CloudWatch metric out of it called Latency and then set a CloudWatch alarm that fires off an email when latency exceeds 2 seconds.\n\nCloudWatch can be integrated into other services like Elasticsearch and Kibana. It can also work with AppDynamics or New Relic.\n\n\n## EC2 and Auto scaling\n\nEC2 instances can be launched with a script that is executed upon launch. This script or piece of code is called user data.\n\nWhen architecting a VPC, make sure that the auto scaling group is spread across two availability zones and each logical component (database server or application server) is in its own private subnet.\n\nAuto scaling can scale applications based on cloud watch metrics or time. Time based works like\u2026 Scaling out on Monday morning and scaling in on Friday evenings.\n\nAuto scaling forms two parts: Launch configuration and Amazon Machine Image (AMI). Launch configuration tells it how to launch something and what to do when it\u2019s launched. AMI tells it what to launch. AMI is in three categories: Bare (nothing but base image), Silver (optimised based OS image with prerequisites (Java\/Node) installed, Gold (Fully fledged application with application code and prerequisites.\n\nIf one wants the deployment to fail if something in user data fails, then one needs to initiate the cfn-signal with -e option specifying the error code. One can also use cfn-in it to initialise the script. This attribute in cloud formation will do roll back on its own without the sub-script having to call cfn-signal.\n\n\n","html":"<h3 style=\"text-align: justify;\">Creating Serverless Projects<\/h3>\n<p style=\"text-align: justify;\">In a server less environment, Amazon Lambda can be used in conjunction with Amazon API Gateway for HTTP interfacing, Amazon S3 for storage, Amazon ElastiCache for caching and DynamoDB\/RDS for database storage. Checkout the Servless Application Framework at <a href=\"http:\/\/serverless.com\">serverless.com<\/a> for more info.<\/p>\n<h2 style=\"text-align: justify;\">Securing data in AWS<\/h2>\n<p style=\"text-align: justify;\">Infrastructure should be treated as code, I.e. Version control systems. Automate security and increase testing frequency via CI\/CD. Fail early and fast. Test at production scale. No need to keep the test servers alive. Spin up the entire production environment in test, deploy the code, run the tests and then tear down the environment.<!--more--><\/p>\n<p style=\"text-align: justify;\">DevOps provides efficiency that speeds up the development lifecycle (build &gt; test &gt; release &gt; monitor &gt; plan). Validate security in each step to avoid slowing down the whole lifecycle.<\/p>\n<p style=\"text-align: justify;\">Amazon\u2019s shared responsibility model.<\/p>\n<p style=\"text-align: justify;\">How do we get certificates for the internal private DNS for an AWS EC2 instance?<\/p>\n<p style=\"text-align: justify;\">All requests to AWS must be signed using access key ID and secret access key. If using the SDK, this is done automatically. However, if this is done manually, use signature version 4 to sign requests.<\/p>\n<p style=\"text-align: justify;\">Support v2 and v4, most services support v4. V4 is more secure as it derives a key from secret access key and then uses that derived key to sign requests. V2 on the other hand does this very plainly as it uses the secret key directly to sign requests.<\/p>\n<p style=\"text-align: justify;\">One way to provide permissions is to add users to a group that has policies attached to it. All users in that group have permissions assigned to them as long as they are in that group. Drawback is that this is a static set of permissions. As in if two developers are out on holiday for \u00a0weeks and only one is available, all three developers still have all the access. Also if one of the developers need access to EMR, and if policy is changed to include that permission, all the developers, whether or not they need it, inherit that permission.<\/p>\n<p style=\"text-align: justify;\">Better way to do this is to use IAM roles. Create a role called EMR with the policy. The user that needs permission assumes that role. This provides that user with a new set of access key and secret key that has access to EMR.<\/p>\n<p style=\"text-align: justify;\">When authenticating with SSO or AD, the authenticating application checks if the user is in the right group to access the target application. If the user isn&#8217;t in the right group then it must check if there are roles that can be assumed to access the target application. If the roles exist, the authenticating application then checks if the user is able or allowed to assume any of those roles. If the user is allowed, the application assumes the role for him, granting access. One of the ways this can be done is by using SAML assertion with IAM.<\/p>\n<h2 style=\"text-align: justify;\">Getting started with IAM<\/h2>\n<p style=\"text-align: justify;\">Once you have the root account, create a general IAM account for yourself. Assign permissions that you need to yourself. If a admin is needed, create a group called Admins and then assign basic admin permissions to that role. When a new admin joins, he\/she can be added to Admins group. Give permissions to admins group as needed.<\/p>\n<p style=\"text-align: justify;\">For a developer who joins the team, again create a Developers group and assign permissions to that group as needed. When a special permission is needed to access a specific thing, create a role for that and allow people to assume roles for a limited time or as long as they need it for. For temporary contractors and consultants, assign one permission called Assume Role. This allows them to temporarily assume roles when needed.<\/p>\n<p style=\"text-align: justify;\">The assume role permission can be restricted by IP, as in you\u2019re only allowed to assume a role when you&#8217;re on site.<\/p>\n<p style=\"text-align: justify;\">For authentication, AWS management console needs username and password. Other tools like AWS CLI, SDKs and API queries need access key and secret keys.<\/p>\n<p style=\"text-align: justify;\">NEVER USE ROOT ACCOUNT CREDENTIALS ANYWHERE.<\/p>\n<p style=\"text-align: justify;\">Two types of IAM permission types. User centric where each user is given access to specific resources. Second type of permission type is resource centric where specific access to a resource is opened up to a set of users.<\/p>\n<p style=\"text-align: justify;\">Use AWS provided library of pre-defined Policy Templates or AWS policy generator. Preferably, create policies in object oriented way using AWS Cloud Formation or AWS SDKs.<\/p>\n<p style=\"text-align: justify;\">IAM policy rule procedure is to deny access by default until access is explicitly allowed in all policies. If something is explicitly denied, it can never be allowed. Explicit deny cannot be overwritten.<\/p>\n<p style=\"text-align: justify;\">Might be possible to restrict users from uploading content above a certain size.<\/p>\n<p style=\"text-align: justify;\">For external users, use OpenID Connect for authentication with external services like Google\/Facebook\/Twitter login along with IAM for authorisation.<\/p>\n<h2 style=\"text-align: justify;\">Caching<\/h2>\n<p style=\"text-align: justify;\">CloudFront caches content based on edge location. If content is available, it will serve from its edge location, if not, it will serve from S3.<\/p>\n<p style=\"text-align: justify;\">Each caching behaviour can be configured. Params like Path Pattern to be cached, request origin to forward to, if forward URLs should have query strings in them etc.<\/p>\n<p style=\"text-align: justify;\">Path patterns max is 255 chars. Case sensitive. Eg. \/*.jpg means cache all JPG files.<\/p>\n<p style=\"text-align: justify;\">With regards to query strings, cache behaviour will differ based on value of query strings in the request. Example: \/hello?query1=a and \/hello?query2=b will result in different cache copies served. Cache can also serve different URLs based on different request header values or cookies.<\/p>\n<p style=\"text-align: justify;\">The time to live (TTL) value controls how long the value should stay in the cache. Longer TTLs for static content, short for dynamic.<\/p>\n<p style=\"text-align: justify;\">A custom error page page can be configured to be serve if the cache fails to find the origin content (if origin changed but cache didn&#8217;t for some reason and its expecting the file to be present on origin).<\/p>\n<p style=\"text-align: justify;\">Authentication works via public\/private key pair. CloudFront has the public key while the application has the private key. The URLs must be signed before sending to cloud front. The application signs using its private key, sends URL to cloud front. If the URL is valid, cloud front serves the content.<\/p>\n<p style=\"text-align: justify;\">User sessions can be stored in DynamoDB or ElastiCache, depending on need.<\/p>\n<p style=\"text-align: justify;\">ElastiCache runs, underneath the covers, me cache or redis cluster. Provides managed service that include patching and backup.<\/p>\n<p style=\"text-align: justify;\">Memcache is known for high performance and scale. Allows running of large nodes with multiple cores or threads. Ability to scale in or out. Data can be partitioned across multiple shards. One downside is that the data model is very simple.<\/p>\n<p style=\"text-align: justify;\">Redis is quite similar to Memcache where it&#8217;s open source and in memory. Redis can only scale vertically (check!!). Has simple master\/slave model, not shards (check!!). Comes with complex data types. Has pub\/sub capability &#8211; the client being informed of events on the server. Not very good for multi-threaded performance (check!!).<\/p>\n<p style=\"text-align: justify;\">Cache is loaded by lazy loading as in data is only loaded when required.<\/p>\n<h2 style=\"text-align: justify;\">CloudWatch<\/h2>\n<p style=\"text-align: justify;\">One can use CloudWatch agent to aggregate logs from EC2 instances to the central CloudWatch server. To do this, install the cloudwatch agent on the EC2 instance.<\/p>\n<p style=\"text-align: justify;\">From any log streams, custom cloudwatch metric can be created. Once the metric is created, an alarm can be configured.<\/p>\n<p style=\"text-align: justify;\">Cloud watch alarms can call HTTP endpoints, like calling web hooks to trigger different kind of behaviours based on the event.<\/p>\n<p style=\"text-align: justify;\">Cloud watch metrics can be aggregated on the server side by providing params like start and end times, period and metric names.<\/p>\n<p style=\"text-align: justify;\">CloudWatch dashboards can be used to provide a bird&#8217;s eye view of all metrics pertaining to a specific application. It also allows adding of text widgets which one can use to provide some free text or images.<\/p>\n<p style=\"text-align: justify;\">JSON path expression can be used to search logs within CloudWatch. This style of expression can also be used to extract values from logs. Once values are extracted, they can be used to create a metric. For instance, use Regex to extract response times from tomcat logs, create a CloudWatch metric out of it called Latency and then set a CloudWatch alarm that fires off an email when latency exceeds 2 seconds.<\/p>\n<p style=\"text-align: justify;\">CloudWatch can be integrated into other services like Elasticsearch and Kibana. It can also work with AppDynamics or New Relic.<\/p>\n<h2 style=\"text-align: justify;\">EC2 and Auto scaling<\/h2>\n<p style=\"text-align: justify;\">EC2 instances can be launched with a script that is executed upon launch. This script or piece of code is called user data.<\/p>\n<p style=\"text-align: justify;\">When architecting a VPC, make sure that the auto scaling group is spread across two availability zones and each logical component (database server or application server) is in its own private subnet.<\/p>\n<p style=\"text-align: justify;\">Auto scaling can scale applications based on cloud watch metrics or time. Time based works like\u2026 Scaling out on Monday morning and scaling in on Friday evenings.<\/p>\n<p style=\"text-align: justify;\">Auto scaling forms two parts: Launch configuration and Amazon Machine Image (AMI). Launch configuration tells it how to launch something and what to do when it&#8217;s launched. AMI tells it what to launch. AMI is in three categories: Bare (nothing but base image), Silver (optimised based OS image with prerequisites (Java\/Node) installed, Gold (Fully fledged application with application code and prerequisites.<\/p>\n<p style=\"text-align: justify;\">If one wants the deployment to fail if something in user data fails, then one needs to initiate the cfn-signal with -e option specifying the error code. One can also use cfn-in it to initialise the script. This attribute in cloud formation will do roll back on its own without the sub-script having to call cfn-signal.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 02 Mar 2016 22:24:16 +0000","created_by":1,"updated_at":"Wed, 02 Mar 2016 22:24:28 +0000","updated_by":1,"published_at":"Wed, 02 Mar 2016 22:24:16 +0000","published_by":1},{"id":350,"title":"Useful Docker commands","slug":"useful-docker-commands","markdown":"\n\n# Managing containers\n\nRemove all old containers:\n\ndocker ps -a | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rm\n\nRemove all stopped containers:\n\ndocker ps -a | grep 'Exited' | awk '{print $1}' | xargs --no-run-if-empty docker rm\n\n\n# Managing images\n\nRemove all untagged images:\n\ndocker rmi $(docker images -a | grep \"^<none>\" | awk '{print $3}')\n\nRemove all untagged images (force):\n\ndocker rmi -f $(docker images -a | grep \"^<none>\" | awk '{print $3}')\n\n\u00a0\n\n\n","html":"<h1>Managing containers<\/h1>\n<p>Remove all old containers:<\/p>\n<pre class=\"lang:sh decode:true\">docker ps -a | grep 'weeks ago' | awk '{print $1}' | xargs --no-run-if-empty docker rm<\/pre>\n<p>Remove all stopped containers:<\/p>\n<p><!--more--><\/p>\n<pre class=\"lang:sh decode:true\">docker ps -a | grep 'Exited' | awk '{print $1}' | xargs --no-run-if-empty docker rm<\/pre>\n<h1>Managing images<\/h1>\n<p>Remove all untagged images:<\/p>\n<pre class=\"lang:sh decode:true \">docker rmi $(docker images -a | grep \"^&lt;none&gt;\" | awk '{print $3}')<\/pre>\n<p>Remove all untagged images (force):<\/p>\n<pre class=\"lang:sh decode:true \">docker rmi -f $(docker images -a | grep \"^&lt;none&gt;\" | awk '{print $3}')<\/pre>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 16 Mar 2016 21:23:40 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:23:56 +0000","updated_by":1,"published_at":"Wed, 16 Mar 2016 21:23:40 +0000","published_by":1},{"id":349,"title":"Service script for Tomcat","slug":"service-script-for-tomcat","markdown":"\nQuick script to allow starting and stopping tomcat from the service command.\n\nCreate a script called <span class=\"lang:default decode:true crayon-inline \">tomcat7<\/span>\u00a0 in <span class=\"lang:default decode:true crayon-inline \">\/etc\/init.d<\/span>\u00a0 like so:\n\nvim \/etc\/init.d\/tomcat7\n\nHere\u2019s what goes into the script:\n\n#!\/bin\/bash # # chkconfig: - 85 15 # description: Jakarta Tomcat Java Servlets and JSP server # processname: tomcat # pidfile: \/var\/www\/tomcat\/tomcat.pid # Source function library. . \/etc\/init.d\/functions TOMCAT_USER=tomcat CATALINA_HOME=\/var\/tomcat RETVAL=$? case \"$1\" in start) echo \"Starting Tomcat\" exec su -l $TOMCAT_USER -c \"$CATALINA_HOME\/bin\/catalina.sh start\" ;; stop) echo \"Stopping Tomcat\" exec su -l $TOMCAT_USER -c \"$CATALINA_HOME\/bin\/catalina.sh stop -force\" numproc=`ps -ef | grep \"$CATALINA_HOME\/bin\/bootstrap.jar\" | grep -v grep |awk -F' ' '{ print $2 }'`; if [ $numproc ]; then echo \"###kill pid: $numproc\" kill -9 $numproc fi ;; debug) echo \"Starting Tomcat in debug\" exec su -l $TOMCAT_USER -c \"$CATALINA_HOME\/bin\/catalina.sh jpda start\" ;; restart) $0 stop sleep 15 $0 start ;; *) echo $\"Usage: $0 {start|stop|restart}\" exit 1 ;; esac exit $RETVAL\n\nThe commands start, stop, debug and restart can be run via service. Example:\n\nservice tomcat7 start\n\nThen apply appropriate permissions:\n\nchmod 755 \/etc\/init.d\/tomcat7 chown root:root \/etc\/init.d\/tomcat7\n\n\u00a0\n\n\n","html":"<p>Quick script to allow starting and stopping tomcat from the service command.<\/p>\n<p>Create a script called <span class=\"lang:default decode:true crayon-inline \">tomcat7<\/span>\u00a0 in <span class=\"lang:default decode:true crayon-inline \">\/etc\/init.d<\/span>\u00a0 like so:<\/p>\n<pre class=\"lang:sh decode:true \">vim \/etc\/init.d\/tomcat7<\/pre>\n<p>Here&#8217;s what goes into the script:<!--more--><\/p>\n<pre class=\"lang:sh decode:true\">#!\/bin\/bash\r\n#\r\n# chkconfig: - 85 15\r\n# description: Jakarta Tomcat Java Servlets and JSP server\r\n# processname: tomcat\r\n# pidfile: \/var\/www\/tomcat\/tomcat.pid\r\n\r\n# Source function library.\r\n. \/etc\/init.d\/functions\r\n\r\nTOMCAT_USER=tomcat\r\nCATALINA_HOME=\/var\/tomcat\r\nRETVAL=$?\r\n\r\ncase \"$1\" in\r\nstart)\r\n        echo \"Starting Tomcat\"\r\n        exec su -l $TOMCAT_USER -c \"$CATALINA_HOME\/bin\/catalina.sh start\"\r\n;;\r\n\r\nstop)\r\n        echo \"Stopping Tomcat\"\r\n        exec su -l $TOMCAT_USER -c \"$CATALINA_HOME\/bin\/catalina.sh stop -force\"\r\n\r\n        numproc=`ps -ef | grep \"$CATALINA_HOME\/bin\/bootstrap.jar\" | grep -v grep |awk -F' ' '{ print $2 }'`;\r\n          if [ $numproc ]; then\r\n          echo \"###kill pid: $numproc\"\r\n            kill -9 $numproc\r\n          fi\r\n;;\r\n\r\ndebug)\r\n        echo \"Starting Tomcat in debug\"\r\n        exec su -l $TOMCAT_USER -c \"$CATALINA_HOME\/bin\/catalina.sh jpda start\"\r\n;;\r\n\r\nrestart)\r\n        $0 stop\r\n        sleep 15\r\n        $0 start\r\n;;\r\n\r\n*)\r\n        echo $\"Usage: $0 {start|stop|restart}\"\r\n        exit 1\r\n;;\r\nesac\r\n\r\nexit $RETVAL<\/pre>\n<p>The commands start, stop, debug and restart can be run via service. Example:<\/p>\n<pre class=\"lang:sh decode:true\">service tomcat7 start<\/pre>\n<p>Then apply appropriate permissions:<\/p>\n<pre class=\"lang:sh decode:true\">chmod 755 \/etc\/init.d\/tomcat7\r\n\r\nchown root:root \/etc\/init.d\/tomcat7<\/pre>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 18 Mar 2016 11:13:50 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:23:28 +0000","updated_by":1,"published_at":"Fri, 18 Mar 2016 11:13:50 +0000","published_by":1},{"id":359,"title":"AppDynamics Power User Training Notes - Day One","slug":"appdynamics-power-user-training-notes-day-one","markdown":"\nNode is mapped to individual JVM or CLR application in environment. If an environment has multiple JVMs running then each maps to a node.\n\nTier is a logical piece of application. For example a piece of functionality. Each web server has node. A single tier can span multiple web servers and each of those web servers can contain multiple nodes.\n\nApplication traffic is organised into business transactions. Each transaction is a distinct user activity like Login or register etc. When a request comes in for the first time, it is tagged with a GUID and that GUID is tracked across the environment. Requests with similar patterns are grouped together. This group is then given a default name. This is default name is based on how the application is designed.\n\nSince each business transaction works across nodes, each has its own flow map.\n\nA business transaction is a specific user activity. It represents a flow of data across the application. A tier, on the other hand represents a static, logical set of services.\n\nTransaction snapshot is a static state of a transaction at any given time. It collects detailed information regarding one business transaction at any given time.\n\nPeriodic snapshot is a snapshot that\u2019s taken once every 10 minutes. This is taken regardless of any errors occurring within the application.\n\nSlow and error transaction snapshot is a snapshot that is taken when calls happening below a certain threshold or when errors are happening above a certain error rate.\n\nSnapshots can be archived. Once a snapshot is archived, it is permanently visible in the UI and won\u2019t get deleted with periodic cleanup. A transaction that has been archived cannot be unarchived and must be deleted if archive is no longer needed.\n\n### AppDynamics architecture\n\nApp server agent. This could be .NET, Java, PHP etc. This is mandatory.\n\nMachine agent is optional. Collects raw metrics regarding the machine that it is on.\n\nAppDynamics supports role based authentication. Also supports other integrations like LDAP\/SAML etc.\n\nAppDynamics controller never actively initiates connection to an agent. The agent connects to the AppDynamics controller which then responds with any additional requests.\n\nBaseline is a set of values calculated from metrics within a time range. We could capture \u201cideal\u201d scenario by recording a baseline when application was running smoothly. Then at a later date, when application isn\u2019t performing optimally, one can compare the current performance with the historic \u201cideal\u201d baseline to identify problems.\n\nBaseline can also be a dynamic, rolling baseline where, for example, every monday 2pm, baseline is taken. Now next monday at 2pm metrics are compared to the baseline that was taken on the previous monday. This is a dynamic baseline so if the current performance peaks, the baseline will adjust itself to compensate for the peak.\n\nWhen a baseline is defined, if the deviation of the current data is certain level above the baseline, AppDynamics will automatically raise an alarm indicating that.\n\nRather than deleting a transaction, it is better to exclude it as restoring a deleted transaction is much harder (needs re-creating) than restoring a excluded one.\n\n\n","html":"<p style=\"text-align: justify;\">Node is mapped to individual JVM or CLR application in environment. If an environment has multiple JVMs running then each maps to a node.<\/p>\n<p style=\"text-align: justify;\">Tier is a logical piece of application. For example a piece of functionality. Each web server has node. A single tier can span multiple web servers and each of those web servers can contain multiple nodes.<\/p>\n<p style=\"text-align: justify;\">Application traffic is organised into business transactions. Each transaction is a distinct user activity like Login or register etc. When a request comes in for the first time, it is tagged with a GUID and that GUID is tracked across the environment. Requests with similar patterns are grouped together. This group is then given a default name. This is default name is based on how the application is designed.<\/p>\n<p style=\"text-align: justify;\">Since each business transaction works across nodes, each has its own flow map.<!--more--><\/p>\n<p style=\"text-align: justify;\">A business transaction is a specific user activity. It represents a flow of data across the application. A tier, on the other hand represents a static, logical set of services.<\/p>\n<p style=\"text-align: justify;\">Transaction snapshot is a static state of a transaction at any given time. It collects detailed information regarding one business transaction at any given time.<\/p>\n<p style=\"text-align: justify;\">Periodic snapshot is a snapshot that\u2019s taken once every 10 minutes. This is taken regardless of any errors occurring within the application.<\/p>\n<p style=\"text-align: justify;\">Slow and error transaction snapshot is a snapshot that is taken when calls happening below a certain threshold or when errors are happening above a certain error rate.<\/p>\n<p style=\"text-align: justify;\">Snapshots can be archived. Once a snapshot is archived, it is permanently visible in the UI and won\u2019t get deleted with periodic cleanup. A transaction that has been archived cannot be unarchived and must be deleted if archive is no longer needed.<\/p>\n<h3 style=\"text-align: justify;\">AppDynamics architecture<\/h3>\n<p style=\"text-align: justify;\">App server agent. This could be .NET, Java, PHP etc. This is mandatory.<\/p>\n<p style=\"text-align: justify;\">Machine agent is optional. Collects raw metrics regarding the machine that it is on.<\/p>\n<p style=\"text-align: justify;\">AppDynamics supports role based authentication. Also supports other integrations like LDAP\/SAML etc.<\/p>\n<p style=\"text-align: justify;\">AppDynamics controller never actively initiates connection to an agent. The agent connects to the AppDynamics controller which then responds with any additional requests.<\/p>\n<p style=\"text-align: justify;\">Baseline is a set of values calculated from metrics within a time range. We could capture \u201cideal\u201d scenario by recording a baseline when application was running smoothly. Then at a later date, when application isn\u2019t performing optimally, one can compare the current performance with the historic \u201cideal\u201d baseline to identify problems.<\/p>\n<p style=\"text-align: justify;\">Baseline can also be a dynamic, rolling baseline where, for example, every monday 2pm, baseline is taken. Now next monday at 2pm metrics are compared to the baseline that was taken on the previous monday. This is a dynamic baseline so if the current performance peaks, the baseline will adjust itself to compensate for the peak.<\/p>\n<p style=\"text-align: justify;\">When a baseline is defined, if the deviation of the current data is certain level above the baseline, AppDynamics will automatically raise an alarm indicating that.<\/p>\n<p style=\"text-align: justify;\">Rather than deleting a transaction, it is better to exclude it as restoring a deleted transaction is much harder (needs re-creating) than restoring a excluded one.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 23 Mar 2016 23:34:44 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:23:10 +0000","updated_by":1,"published_at":"Wed, 23 Mar 2016 23:34:44 +0000","published_by":1},{"id":361,"title":"AppDynamics Power User Training Notes - Day Two","slug":"appdynamics-power-user-training-notes-day-two","markdown":"\nHealth rules are conditions that if fulfilled relate to good health for a particular node. The conditions can be based on load (calls\/min), response times (x ms average) or error rate. AppDynamics provides 7 health rules out of the box. When one or many health rules are violated, an alert is issued. Depending on the integrations configured, this alert can be an email or SMS.\n\nHealth rule can be based on transaction performance or node health. In case of transaction performance, it can target specific transactions or all transactions in a tier. However, in case of node heath, this can target specific node or nodes in a tier or nodes by type.\n\nThe data that health rule uses can also be configured. This can be a time period that it should use. This can be in free form numeric minutes. One can also specify wait time after a violation, i.e. how long should it wait to reassess health rule after it has been violated. This should be long enough so that it has enough new data to positively reassess the condition.\n\nDiagnostic session is a time period in which AppDynamics collects extra snapshots for one or more business transactions. All transactions have full call graphs. Diagnostic sessions are normally used after a health rule violation.\n\nDiagnostic sessions can be policy based where it is automatically executed as soon as a particular health rule violation has happened. Depending on the situation, the policy can then have actions where it can send an email or a HTTP request with event details.\n\nDevelopment mode is where it creates snapshots where it creates transactions for every business transaction. Highly advised that it is only used in testing environments as it has more overhead compared normal production mode. It has safeguards built in where it automatically turns itself off to production mode if 500 calls\/min or 90% heap utilisation happens.\n\nAll HTTP error codes from 400 to 505 are treated as errors. Also, all exceptions within JVM that are handled but logged as fatal using Log4J or java.util.logging are also treated as errors.\n\n**Scenario**: Customer calls regarding an error that they saw on the page.\n\n**Solution**: Start a diagnostic session or periodic collection or brief development mode to get snapshots with full call graphs. Go to application dashboard and on the right pane, click errors. Or, go to snapshots and filter by errors. Once you locate the snapshot with errors that has full call graph (blue document icon next to it), \u00a0double click on it to view and go to error details. Make sure you turn off the development mode.\n\nA business transaction is detected at the point when it enters the application. If a transaction or node is not detected, it can manually be added. Configuration > Instrumentation > Transaction detection. In the custom rules section, a custom rule for detecting a transaction can be added.\n\nExit point is a call that leaves node or environment either to a new monitored environment or a tier. Example database\/mail server etc.\n\nUse Data Collectors to collect data that\u2019s passed into methods or payloads. Its available in Application > Configuration > Instrumentation > Data Collectors tab. For methods, use getter chain and specify object getter to invoke which is likely to provide data that\u2019s needed. For instance, to collect movie title from an object that\u2019s passed into search(movieObject) method, if the getter chain is getTitle() then the full invocation will be movieObject.getTitle().\n\n\n","html":"<p style=\"text-align: justify;\">Health rules are conditions that if fulfilled relate to good health for a particular node. The conditions can be based on load (calls\/min), response times (x ms average) or error rate. AppDynamics provides 7 health rules out of the box. When one or many health rules are violated, an alert is issued. Depending on the integrations configured, this alert can be an email or SMS.<\/p>\n<p style=\"text-align: justify;\">Health rule can be based on transaction performance or node health. In case of transaction performance, it can target specific transactions or all transactions in a tier. However, in case of node heath, this can target specific node or nodes in a tier or nodes by type.<\/p>\n<p style=\"text-align: justify;\">The data that health rule uses can also be configured. This can be a time period that it should use. This can be in free form numeric minutes. One can also specify wait time after a violation, i.e. how long should it wait to reassess health rule after it has been violated. This should be long enough so that it has enough new data to positively reassess the condition.<!--more--><\/p>\n<p style=\"text-align: justify;\">Diagnostic session is a time period in which AppDynamics collects extra snapshots for one or more business transactions. All transactions have full call graphs. Diagnostic sessions are normally used after a health rule violation.<\/p>\n<p style=\"text-align: justify;\">Diagnostic sessions can be policy based where it is automatically executed as soon as a particular health rule violation has happened. Depending on the situation, the policy can then have actions where it can send an email or a HTTP request with event details.<\/p>\n<p style=\"text-align: justify;\">Development mode is where it creates snapshots where it creates transactions for every business transaction. Highly advised that it is only used in testing environments as it has more overhead compared normal production mode. It has safeguards built in where it automatically turns itself off to production mode if 500 calls\/min or 90% heap utilisation happens.<\/p>\n<p style=\"text-align: justify;\">All HTTP error codes from 400 to 505 are treated as errors. Also, all exceptions within JVM that are handled but logged as fatal using Log4J or java.util.logging are also treated as errors.<\/p>\n<p style=\"text-align: justify;\"><b>Scenario<\/b>: Customer calls regarding an error that they saw on the page.<\/p>\n<p style=\"text-align: justify;\"><b>Solution<\/b>: Start a diagnostic session or periodic collection or brief development mode to get snapshots with full call graphs. Go to application dashboard and on the right pane, click errors. Or, go to snapshots and filter by errors. Once you locate the snapshot with errors that has full call graph (blue document icon next to it), \u00a0double click on it to view and go to error details. Make sure you turn off the development mode.<\/p>\n<p style=\"text-align: justify;\">A business transaction is detected at the point when it enters the application. If a transaction or node is not detected, it can manually be added. Configuration &gt; Instrumentation &gt; Transaction detection. In the custom rules section, a custom rule for detecting a transaction can be added.<\/p>\n<p style=\"text-align: justify;\">Exit point is a call that leaves node or environment either to a new monitored environment or a tier. Example database\/mail server etc.<\/p>\n<p style=\"text-align: justify;\">Use Data Collectors to collect data that&#8217;s passed into methods or payloads. Its available in Application &gt; Configuration &gt; Instrumentation &gt; Data Collectors tab. For methods, use getter chain and specify object getter to invoke which is likely to provide data that&#8217;s needed. For instance, to collect movie title from an object that\u2019s passed into search(movieObject) method, if the getter chain is getTitle() then the full invocation will be movieObject.getTitle().<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 23 Mar 2016 23:35:55 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:22:55 +0000","updated_by":1,"published_at":"Wed, 23 Mar 2016 23:35:55 +0000","published_by":1},{"id":364,"title":"AppDynamics Power User Training Notes - Day Three","slug":"appdynamics-power-user-training-notes-day-three","markdown":"\nDashboards. Useful for displaying and sharing information across teams or departments. Allows integrating other tools via iFrame widget. Dashboards can also be embedded into other services or even web pages.\n\nA dashboard can also be converted into a report. One can also schedule periodic emailing of reports to a recipient list. This might be useful to create a \u201cweekly\/monthly digest\u201d of data relating to an environment.\n\nPolicy triggers define events that trigger actions. When the criteria in a policy is met, a corresponding trigger is activated. This can be used to control behaviour such as increaing thread pool size as load increases etc. The trigger must be associated with a health rule violation. For example, if a health rule specifies \u201ccalls higher than normal\u201d then a policy can be created based on this heath rule. The trigger gets executed when violation of the specified health rule occurs. Triggers can span different stages of health rule violations, for example, start and\/or end. More than one health rule violation event can be specified.\n\nWhen defining actions for policy triggers, one can configure the action to require human approval before executing a particular action. In this case, one must specify email address of the approver within the action.\n\nRunbook automation is a remediation action that is executed on a node on a policy trigger. For instance, if for some reason there\u2019s a java out of memory error, the runbook automation can backup the logs and restart the JVM. Everything that the runbook automation does is logged with the action that was executed, time and the local script that the automation ran to carry out the specified action. There\u2019s three types:\n\n**Remediation action**: Run arbitrary script on a node\n\n**HTTP request**: Make a HTTP request to some server notifying it of the issue.\n\n**Cloud auto-scaling**: Scale out or scale in on cloud services like AWS.\n\n\n","html":"<p style=\"text-align: justify;\">Dashboards. Useful for displaying and sharing information across teams or departments. Allows integrating other tools via iFrame widget. Dashboards can also be embedded into other services or even web pages.<\/p>\n<p style=\"text-align: justify;\">A dashboard can also be converted into a report. One can also schedule periodic emailing of reports to a recipient list. This might be useful to create a \u201cweekly\/monthly digest\u201d of data relating to an environment.<\/p>\n<p style=\"text-align: justify;\">Policy triggers define events that trigger actions. When the criteria in a policy is met, a corresponding trigger is activated. This can be used to control behaviour such as increaing thread pool size as load increases etc. The trigger must be associated with a health rule violation. For example, if a health rule specifies \u201ccalls higher than normal\u201d then a policy can be created based on this heath rule. The trigger gets executed when violation of the specified health rule occurs. Triggers can span different stages of health rule violations, for example, start and\/or end. More than one health rule violation event can be specified.<!--more--><\/p>\n<p style=\"text-align: justify;\">When defining actions for policy triggers, one can configure the action to require human approval before executing a particular action. In this case, one must specify email address of the approver within the action.<\/p>\n<p style=\"text-align: justify;\">Runbook automation is a remediation action that is executed on a node on a policy trigger. For instance, if for some reason there\u2019s a java out of memory error, the runbook automation can backup the logs and restart the JVM. Everything that the runbook automation does is logged with the action that was executed, time and the local script that the automation ran to carry out the specified action. There\u2019s three types:<\/p>\n<p style=\"text-align: justify;\"><b>Remediation action<\/b>: Run arbitrary script on a node<\/p>\n<p style=\"text-align: justify;\"><b>HTTP request<\/b>: Make a HTTP request to some server notifying it of the issue.<\/p>\n<p style=\"text-align: justify;\"><b>Cloud auto-scaling<\/b>: Scale out or scale in on cloud services like AWS.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 23 Mar 2016 23:37:24 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:22:42 +0000","updated_by":1,"published_at":"Wed, 23 Mar 2016 23:37:24 +0000","published_by":1},{"id":367,"title":"Simplifying management of forks with npm forkpool module","slug":"simplifying-management-of-forks-with-npm-forkpool-module","markdown":"\n**Backstory**\n\nRecently, I\u2019ve been working on a project that does a lot of heavy weight image processing. In a nutshell, a user uploads an image and then the website does a whole load of processing on that image and returns with the results. Depending on the size of the image it could take a while for it to load the results. The websites does other stuff as well so the scenario where a few users are concurrently uploading images, waiting for response while a whole load of other set of users are navigating to other parts of the website is very real. The performance was respectable, however, I wanted more.\n\nAfter a couple of hours of investigating around the issue, I finally settled on using forks for handling this type of load. A fork is a process that is spun off of a parent process. In this case, when a user uploads an image on the website, the node server creates a fork that then does the image processing. Meanwhile, the parent process hooks into the \u201cexit\u201d callback of the forked process so that it knows when the forked process has exited. When this happens, the parent process returns the results to the user.\n\nNow, every node process, no matter how asynchronous, is single threaded. This means that it utilises at most a single core of your CPU. So the fact that the new fork powered architecture creates a fork process to handle the heavy image processing means lower load on the single node process. It also means that the load is spread evenly across cpu cores.\n\nHowever, here lies the problem. Every time a new customer arrives on the website and uploads a new image, the node server spins up a new process. So, if many customers arrive and upload large images which could take 10-15 seconds to load, it will spin up a new process for each of those customers. Since each process maps to roughly one CPU core, having too many processes doing CPU intensive things will start to slow down everything thats running on that machine. This means that they will slow down each other as they race to get as much CPU time as they can.\n\nThis problem also has a simple solution. Limit the number of forks so that they do not exceed a certain number. This number will vary upon usage and circumstances but after doing some benchmarking, in my case, this was 3. This meant that my application should, at any given time run at most 3 forks (processes).\n\nIn technical speak, this is called pooling of resources. The idea behind this is that you create a pool of fixed size and then take things in and out of pool. Translating this into what I needed to do with my fork problem, I needed a pool of size 3 to manage my forks so that it will allow me to put at most 3 things in it. If I tried to put more things in it, it should queue those things, wait for current things that are running in it to finish and then automatically start the queued things.\n\nI googled for some time, trying to find a good way to pool my forks. I found a couple of solutions but none were as elegant as I\u2019d like them to be. I did find some good ones but they either were deprecated or weren\u2019t being maintained anymore. So, I got my IDE ready and started programming what will be known as the [forkpool](https:\/\/www.npmjs.com\/package\/forkpool) module.\n\n**Forkpool**\n\nThe module comprises of two basic things. <span class=\"lang:default decode:true crayon-inline\">Forkpool<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">Forklet<\/span>\u00a0. <span class=\"lang:default decode:true crayon-inline \">Forklet<\/span>\u00a0 is a definition of a fork instance. For example, if you wanted to fork a node module named <span class=\"lang:default decode:true crayon-inline \">worker.js<\/span>\u00a0, your forklet will comprise of three things. First being the location of the module, second being additional environment configuration needed to run the fork and third (optional)\u00a0being the time within which the fork must complete its execution (if it doesn\u2019t, it gets killed). The third option of specifying timeout was the one I couldn\u2019t find available in any of the existing fork pools and hence decided to create it.\n\nSo once you have a <span class=\"lang:default decode:true crayon-inline \">forklet<\/span>\u00a0, you can use the forkpool to create a fork. Upon creating the forkpool, you can define the size of the pool in its constructor. This value is the maximum number of forks you want running in parallel at any given time.\n\nOnce you have a <span class=\"lang:default decode:true crayon-inline \">forkpool<\/span>\u00a0 and <span class=\"lang:default decode:true crayon-inline \">forklet<\/span>\u00a0 objects you can execute forks. If you execute more forks than the capacity, it will queue the rest of the forks up and execute them one\u00a0by one as the running forks finish their execution.\n\nThe module also comes with a event system comprised of a set of events. Every time something happens with the forks, a event is triggered. You can subscribe to events on a specific fork or to all forks across the entire pool. Working within the asynchronous architecture of node, this just made sense. There are various kinds of events that let you know what is happening to a fork. These are <span class=\"lang:default decode:true crayon-inline\">scheduled<\/span>\u00a0, <span class=\"lang:default decode:true crayon-inline\">started<\/span>\u00a0, <span class=\"lang:default decode:true crayon-inline\">timedout<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline \">exited<\/span>\u00a0. Event callbacks also come with additional contextual information that provide more information regarding that specific event.\n\nCheckout these links below for more information regarding the module:\n\n[NPM](https:\/\/www.npmjs.com\/package\/forkpool), [Github](https:\/\/github.com\/manthanhd\/forkpool), [Travis CI](https:\/\/travis-ci.org\/manthanhd\/forkpool)\n\n\n","html":"<p style=\"text-align: justify;\"><strong>Backstory<\/strong><\/p>\n<p style=\"text-align: justify;\">Recently, I&#8217;ve been working on a project that does a lot of heavy weight image processing. In a nutshell, a user uploads an image and then the website does a whole load of processing on that image and returns with the results. Depending on the size of the image it could take a while for it to load the results. The websites does other stuff as well so the scenario where a few users are concurrently uploading images, waiting for response while a whole load of other set of users are navigating to other parts of the website is very real. The performance was respectable, however, I wanted more.<!--more--><\/p>\n<p style=\"text-align: justify;\">After a couple of hours of investigating around the issue, I finally settled on using forks for handling this type of load. A fork is a process that is spun off of a parent process. In this case, when a user uploads an image on the website, the node server creates a fork that then does the image processing. Meanwhile, the parent process hooks into the &#8220;exit&#8221; callback of the forked process so that it knows when the forked process has exited. When this happens, the parent process returns the results to the user.<\/p>\n<p style=\"text-align: justify;\">Now, every node process, no matter how asynchronous, is single threaded. This means that it utilises at most a single core of your CPU. So the fact that the new fork powered architecture creates a fork process to handle the heavy image processing means lower load on the single node process. It also means that the load is spread evenly across cpu cores.<\/p>\n<p style=\"text-align: justify;\">However, here lies the problem. Every time a new customer arrives on the website and uploads a new image, the node server spins up a new process. So, if many customers arrive and upload large images which could take 10-15 seconds to load, it will spin up a new process for each of those customers. Since each process maps to roughly one CPU core, having too many processes doing CPU intensive things will start to slow down everything thats running on that machine. This means that they will slow down each other as they race to get as much CPU time as they can.<\/p>\n<p style=\"text-align: justify;\">This problem also has a simple solution. Limit the number of forks so that they do not exceed a certain number. This number will vary upon usage and circumstances but after doing some benchmarking, in my case, this was 3. This meant that my application should, at any given time run at most 3 forks (processes).<\/p>\n<p style=\"text-align: justify;\">In technical speak, this is called pooling of resources. The idea behind this is that you create a pool of fixed size and then take things in and out of pool. Translating this into what I needed to do with my fork problem, I needed a pool of size 3 to manage my forks so that it will allow me to put at most 3 things in it. If I tried to put more things in it, it should queue those things, wait for current things that are running in it to finish and then automatically start the queued things.<\/p>\n<p style=\"text-align: justify;\">I googled for some time, trying to find a good way to pool my forks. I found a couple of solutions but none were as elegant as I&#8217;d like them to be. I did find some good ones but they either were deprecated or weren&#8217;t being maintained anymore. So, I got my IDE ready and started programming what will be known as the <a href=\"https:\/\/www.npmjs.com\/package\/forkpool\" target=\"_blank\">forkpool<\/a> module.<\/p>\n<p style=\"text-align: justify;\"><strong>Forkpool<\/strong><\/p>\n<p style=\"text-align: justify;\">The module comprises of two basic things. <span class=\"lang:default decode:true crayon-inline\">Forkpool<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">Forklet<\/span>\u00a0. <span class=\"lang:default decode:true crayon-inline \">Forklet<\/span>\u00a0 is a definition of a fork instance. For example, if you wanted to fork a node module named <span class=\"lang:default decode:true crayon-inline \">worker.js<\/span>\u00a0, your forklet will comprise of three things. First being the location of the module, second being additional environment configuration needed to run the fork and third (optional)\u00a0being the time within which the fork must complete its execution (if it doesn&#8217;t, it gets killed). The third option of specifying timeout was the one I couldn&#8217;t find available in any of the existing fork pools and hence decided to create it.<\/p>\n<p style=\"text-align: justify;\">So once you have a <span class=\"lang:default decode:true crayon-inline \">forklet<\/span>\u00a0, you can use the forkpool to create a fork. Upon creating the forkpool, you can define the size of the pool in its constructor. This value is the maximum number of forks you want running in parallel at any given time.<\/p>\n<p style=\"text-align: justify;\">Once you have a <span class=\"lang:default decode:true crayon-inline \">forkpool<\/span>\u00a0 and <span class=\"lang:default decode:true crayon-inline \">forklet<\/span>\u00a0 objects you can execute forks. If you execute more forks than the capacity, it will queue the rest of the forks up and execute them one\u00a0by one as the running forks finish their execution.<\/p>\n<p style=\"text-align: justify;\">The module also comes with a event system comprised of a set of events. Every time something happens with the forks, a event is triggered. You can subscribe to events on a specific fork or to all forks across the entire pool. Working within the asynchronous architecture of node, this just made sense. There are various kinds of events that let you know what is happening to a fork. These are <span class=\"lang:default decode:true crayon-inline\">scheduled<\/span>\u00a0, <span class=\"lang:default decode:true crayon-inline\">started<\/span>\u00a0, <span class=\"lang:default decode:true crayon-inline\">timedout<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline \">exited<\/span>\u00a0. Event callbacks also come with additional contextual information that provide more information regarding that specific event.<\/p>\n<p style=\"text-align: justify;\">Checkout these links below for more information regarding the module:<\/p>\n<p><a href=\"https:\/\/www.npmjs.com\/package\/forkpool\">NPM<\/a>, <a href=\"https:\/\/github.com\/manthanhd\/forkpool\" target=\"_blank\">Github<\/a>, <a href=\"https:\/\/travis-ci.org\/manthanhd\/forkpool\" target=\"_blank\">Travis CI<\/a><\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 08 Apr 2016 18:23:58 +0000","created_by":1,"updated_at":"Mon, 29 Aug 2016 16:18:33 +0000","updated_by":1,"published_at":"Fri, 08 Apr 2016 18:23:58 +0000","published_by":1},{"id":376,"title":"Clearing passwords on Linux","slug":"clearing-passwords-on-linux","markdown":"\nAs <span class=\"lang:sh decode:true crayon-inline \">root<\/span>, quickest way is to use the <span class=\"lang:default decode:true crayon-inline \">passwd<\/span>\u00a0 command:\n\npasswd -d <user>\n\nIf there are failed attempts, you\u2019d need to reset those as well:\n\npam_tally2 --user=<user> --reset\n\nTo verify:\n\npam_tally2 --user=<user>\n\n\n","html":"<p>As <span class=\"lang:sh decode:true crayon-inline \">root<\/span>, quickest way is to use the <span class=\"lang:default decode:true crayon-inline \">passwd<\/span>\u00a0 command:<\/p>\n<pre class=\"lang:sh decode:true\">passwd -d &lt;user&gt;<\/pre>\n<p>If there are failed attempts, you&#8217;d need to reset those as well:<\/p>\n<pre class=\"lang:sh decode:true\">pam_tally2 --user=&lt;user&gt; --reset<\/pre>\n<p>To verify:<\/p>\n<pre class=\"lang:sh decode:true\">pam_tally2 --user=&lt;user&gt;<\/pre>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 13 May 2016 13:21:13 +0000","created_by":1,"updated_at":"Sun, 26 Jun 2016 01:38:25 +0000","updated_by":1,"published_at":"Fri, 13 May 2016 13:21:13 +0000","published_by":1},{"id":380,"title":"Handling file uploads in Express using multer","slug":"handling-file-uploads-in-express","markdown":"\nFirst of all, download and install <span class=\"lang:default decode:true crayon-inline\">multer<\/span>.\n\nnpm install --save multer\n\nInclude the dependency.\n\nvar multer = require(\"multer\");\n\nInitialise it into a service. The command below will initialise it with default storage type (being <span class=\"lang:default decode:true crayon-inline\">DiskStorage<\/span>) and upload destination being the <span class=\"lang:default decode:true crayon-inline \">uploads\/<\/span> folder.\n\nvar uploadService = multer({ dest: \"uploads\/\" });\n\nIf you don\u2019t want the file to be saved anywhere in the file system and want to use memory storage, initialise it as follows.\n\nvar uploadService = multer({ storage: multer.memoryStorage() });\n\nYou can add file size limits too. The command below sets the file upload limit to be 12 megabytes.\n\nvar uploadService = multer({storage: multer.memoryStorage(), limits: {fileSize: 1000 * 1000 * 12}});\n\nYou can then pass around the <span class=\"lang:default decode:true crayon-inline\">uploadService<\/span> object to other modules that use it.\n\nfunction FileUploadRoute(express, uploadService) {\n\nTo read uploads in a route, add it to the second parameter of the route definition. The following commands processes a single file uploaded under <span class=\"lang:sh decode:true crayon-inline \">file<\/span> attribute within post form-data.\n\nrouter.post(\"\/\", uploadService.single('file'), function (req, res) { var uploadedFile = req.file; });\n\nThe <span class=\"lang:sh decode:true crayon-inline \">uploadedFile<\/span> object here contains data about the file that was uploaded. The <span class=\"lang:default decode:true crayon-inline\">buffer<\/span> attribute of this object contains the actual file content. The object also has other attributes like <span class=\"lang:default decode:true crayon-inline\">filename<\/span>, <span class=\"lang:default decode:true crayon-inline \">mimeType<\/span> and encoding that can be used to process the file according its type.\n\nFor more information, see [multer on npm](https:\/\/www.npmjs.com\/package\/multer).\n\n\n","html":"<p style=\"text-align: justify;\">First of all, download and install <span class=\"lang:default decode:true crayon-inline\">multer<\/span>.<\/p>\n<pre class=\"lang:sh decode:true\">npm install --save multer<\/pre>\n<p style=\"text-align: justify;\">Include the dependency.<\/p>\n<pre class=\"lang:sh decode:true \">var multer = require(\"multer\");<\/pre>\n<p style=\"text-align: justify;\">Initialise it into a service. The command below will initialise it with default storage type (being <span class=\"lang:default decode:true crayon-inline\">DiskStorage<\/span>) and upload destination being the <span class=\"lang:default decode:true crayon-inline \">uploads\/<\/span> folder.<!--more--><\/p>\n<pre class=\"lang:sh decode:true \">var uploadService = multer({ dest: \"uploads\/\" });<\/pre>\n<p style=\"text-align: justify;\">If you don&#8217;t want the file to be saved anywhere in the file system and want to use memory storage, initialise it as follows.<\/p>\n<pre class=\"lang:sh decode:true \">var uploadService = multer({ storage: multer.memoryStorage() });<\/pre>\n<p style=\"text-align: justify;\">You can add file size limits too. The command below sets the file upload limit to be 12 megabytes.<\/p>\n<pre class=\"\">var uploadService = multer({storage: multer.memoryStorage(), limits: {fileSize: 1000 * 1000 * 12}});<\/pre>\n<p style=\"text-align: justify;\">You can then pass around the <span class=\"lang:default decode:true crayon-inline\">uploadService<\/span> object to other modules that use it.<\/p>\n<pre class=\"\">function FileUploadRoute(express, uploadService) {<\/pre>\n<p style=\"text-align: justify;\">To read uploads in a route, add it to the second parameter of the route definition. The following commands processes a single file uploaded under <span class=\"lang:sh decode:true crayon-inline \">file<\/span> attribute within post form-data.<\/p>\n<pre class=\"\">router.post(\"\/\", uploadService.single('file'), function (req, res) {\r\n    var uploadedFile = req.file;\r\n});<\/pre>\n<p style=\"text-align: justify;\">The <span class=\"lang:sh decode:true crayon-inline \">uploadedFile<\/span> object here contains data about the file that was uploaded. The <span class=\"lang:default decode:true crayon-inline\">buffer<\/span> attribute of this object contains the actual file content. The object also has other attributes like <span class=\"lang:default decode:true crayon-inline\">filename<\/span>, <span class=\"lang:default decode:true crayon-inline \">mimeType<\/span> and encoding that can be used to process the file according its type.<\/p>\n<p style=\"text-align: justify;\">For more information, see <a href=\"https:\/\/www.npmjs.com\/package\/multer\" target=\"_blank\">multer on npm<\/a>.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 26 Jun 2016 01:33:26 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:21:11 +0000","updated_by":1,"published_at":"Sun, 26 Jun 2016 01:33:26 +0000","published_by":1},{"id":386,"title":"Taking out Netflix's gradle-lint-plugin for a spin","slug":"taking-out-netflixs-gradle-lint-plugin-for-a-spin","markdown":"\nDependency management is a complex thing. A typical gradle project has several dependencies, each of which in turn has multiple other dependencies and so on. This continues and what forms as a result is called a dependency graph. While this is great, unused dependencies is a big problem. For instance, say your project depends on dependency A which then depends on B, C and D. Your project also depends directly with another dependency E which in turn depends on F and G. The dependency graph would look something like this:\n\nYour Project -> A -> (B, C, D)  \n Your Project -> E -> (F, G)\n\nNow, your project probably only uses code in A which only then directly depends on some code in B. Also, maybe you also have some code that depends on E but that code in E doesn\u2019t depend directly on neither F, nor G. This means that C, D, F and G are your unused dependencies.\n\nIf you knew this from the start, you could just exclude those libraries. However, tracking this manually is incredibly difficult as the more complex the libraries are, the more complex the web of inter-dependencies becomes. I\u2019ve been looking for an automated solution to this for a while and recently, I found that Netflix has a plugin that could help. This is called the gradle-lint-plugin.\n\nIts part of Netflix\u2019s OSS and is available on [github](https:\/\/github.com\/nebula-plugins\/gradle-lint-plugin) to download. Its artifacts are available in [JCenter](https:\/\/bintray.com\/bintray\/jcenter) maven repository.\n\n[![](https:\/\/api.bintray.com\/packages\/nebula\/gradle-plugins\/gradle-lint-plugin\/images\/download.svg)](https:\/\/bintray.com\/nebula\/gradle-plugins\/gradle-lint-plugin\/_latestVersion)\n\nTo test it out, I quickly got a gradle project up and running by following the [spring boot tutorial](https:\/\/spring.io\/guides\/gs\/spring-boot\/). To make things a slightly more interesting, I added the <span class=\"lang:default decode:true crayon-inline \">org.apache:httpcomponents:httpclient<\/span> library as a dependency in my <span class=\"lang:default decode:true crayon-inline \">build.gradle<\/span> file:\n\ncompile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'\n\nAlso, for reference, here\u2019s my gradle version:\n\nC:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test>gradle -v ------------------------------------------------------------ Gradle 2.12 ------------------------------------------------------------ Build time: 2016-03-14 08:32:03 UTC Build number: none Revision: b29fbb64ad6b068cb3f05f7e40dc670472129bc0 Groovy: 2.4.4 Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_10 (Oracle Corporation 23.6-b04) OS: Windows 8 6.2 amd64\n\nI ran the gradle clean build command to assess the initial size of the build. As expected, the war file was massive at **14.3 megabytes**. For a small project that has a simple healthcheck endpoint, this is huge. So, to streamline this, I followed the [gradle-lint-plugin guide](https:\/\/github.com\/nebula-plugins\/gradle-lint-plugin\/wiki\/Using-Lint) to use and apply the plugin. I added some rules of my own too. Here\u2019s what my <span class=\"lang:default decode:true crayon-inline\">build.gradle<\/span> file looks like:\n\nbuildscript { repositories { mavenCentral() jcenter() } dependencies { classpath(\"org.springframework.boot:spring-boot-gradle-plugin:1.3.5.RELEASE\") } } plugins { id 'nebula.lint' version '0.30.12' } allprojects { apply plugin: 'nebula.lint' gradleLint.rules = ['all-dependency', 'unused-dependency', 'unused-exclude-by-dep'] \/\/ add as many rules here as you'd like } apply plugin: 'java' apply plugin: 'eclipse' apply plugin: 'idea' apply plugin: 'spring-boot' jar { baseName = 'gs-spring-boot' version = '0.1.0' } repositories { mavenCentral() jcenter() } sourceCompatibility = 1.7 targetCompatibility = 1.7 dependencies { compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2' \/\/ tag::jetty[] compile(\"org.springframework.boot:spring-boot-starter-web\") { exclude module: \"spring-boot-starter-tomcat\" } compile(\"org.springframework.boot:spring-boot-starter-jetty\") \/\/ end::jetty[] \/\/ tag::actuator[] compile(\"org.springframework.boot:spring-boot-starter-actuator\") \/\/ end::actuator[] testCompile(\"junit:junit\") } task wrapper(type: Wrapper) { gradleVersion = '2.1' }\n\nOnce that was out of the way, I simply ran <span class=\"lang:default decode:true crayon-inline \">gradle clean build<\/span> command.\n\nC:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test>gradle clean build :clean :compileJava :processResources UP-TO-DATE :classes :findMainClass :jar :bootRepackage :assemble :compileTestJava UP-TO-DATE :processTestResources UP-TO-DATE :testClasses UP-TO-DATE :test UP-TO-DATE :check UP-TO-DATE :build :lintGradle This project contains lint violations. A complete listing of the violations follows. Because none were serious, the build's overall status was unaffected. warning unused-dependency one or more classes in org.springframework:spring-web:4.2.6.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework.boot:spring-boot:1.3.5.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework:spring-context:4.2.6.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework:spring-web:4.2.6.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework.boot:spring-boot:1.3.5.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE are required by your code directly warning unused-dependency one or more classes in org.springframework:spring-context:4.2.6.RELEASE are required by your code directly warning unused-dependency this dependency is unused and can be removed build.gradle:39 compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2' warning unused-dependency this dependency is unused and can be removed build.gradle:39 compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2' ? build.gradle: 10 problems (0 errors, 10 warnings) To apply fixes automatically, run fixGradleLint, review, and commit the changes. BUILD SUCCESSFUL Total time: 15.742 secs\n\nExcellent! It found the unused dependencies. I was quite pleased with this result so I ran <span class=\"lang:default decode:true crayon-inline\">gradle fixGradleLint<\/span> command. According to the wiki, this command should fix the issues that it found by making changes to the <span class=\"lang:default decode:true crayon-inline\">build.gradle<\/span> file. Unfortunately, this didn\u2019t work as it resulted in an error.\n\nC:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test>gradle fixGradleLint :fixGradleLint FAILED FAILURE: Build failed with an exception. * What went wrong: Execution failed for task ':fixGradleLint'. > com.netflix.nebula.lint.jgit.api.errors.PatchApplyException: Cannot apply: HunkHeader[36,7->36,14] * Try: Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED Total time: 10.808 secs\n\nI\u2019ve already raised a github issue to draw their attention to this problem. For information on progress, feel free to watch or add to the [issue](https:\/\/github.com\/nebula-plugins\/gradle-lint-plugin\/issues\/38).\n\nSo, even if the automated fix didn\u2019t work, I wasn\u2019t done yet. I looked around in the <span class=\"lang:default decode:true crayon-inline \">build<\/span> directory looking for something that I can use to apply for the fix myself. After all, since <span class=\"lang:default decode:true crayon-inline \">gradle lintGradle<\/span> and <span class=\"lang:default decode:true crayon-inline \">gradle fixGradleLint<\/span> are two separate commands, independent of each other, it must store some form of state somewhere. Fortunately I found it. It stores its state in a file called <span class=\"lang:default decode:true crayon-inline\">lint.patch<\/span> directly under the <span class=\"lang:default decode:true crayon-inline\">build<\/span> directory at the root of your project.\n\nHurrah! I quickly opened it. It was a normal git patch file. While looking at it, I was slightly confused with the additions that it was making. Here\u2019s what mine looked like:\n\ndiff --git a\/build.gradle b\/build.gradle --- a\/build.gradle +++ b\/build.gradle @@ -36,7 +36,14 @@ targetCompatibility = 1.7 dependencies { + compile 'org.springframework:spring-web:4.2.6.RELEASE' + compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE' + compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE' + compile 'org.springframework:spring-context:4.2.6.RELEASE' + compile 'org.springframework:spring-web:4.2.6.RELEASE' + compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE' + compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE' + compile 'org.springframework:spring-context:4.2.6.RELEASE' - compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2' \/\/ tag::jetty[] compile(\"org.springframework.boot:spring-boot-starter-web\") { exclude module: \"spring-boot-starter-tomcat\"\n\nLooking at that file, I could immediately tell that it was adding some compile time dependencies and was removing one httpcomponents dependency. While I agree with the removal of the httpcomponents dependency, I was slightly confused why it wasn\u2019t replacing the existing spring-boot dependencies with the ones it was adding. It should\u2019ve replaced them because having both provides no benefit. What its trying to add is more specific than the dependencies that are already there! To prove myself right, I removed all the existing <span class=\"lang:default decode:true crayon-inline \">compile<\/span> dependencies (expect <span class=\"lang:default decode:true crayon-inline \">testCompile<\/span> ones) and added the ones that it was going to add from the patch file. Here\u2019s what the dependency section of my <span class=\"lang:default decode:true crayon-inline \">build.gradle<\/span> looked like:\n\ndependencies { compile 'org.springframework:spring-web:4.2.6.RELEASE' compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE' compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE' compile 'org.springframework:spring-context:4.2.6.RELEASE' compile 'org.springframework:spring-web:4.2.6.RELEASE' compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE' compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE' compile 'org.springframework:spring-context:4.2.6.RELEASE' \/*compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2' \/\/ tag::jetty[] compile(\"org.springframework.boot:spring-boot-starter-web\") { exclude module: \"spring-boot-starter-tomcat\" } compile(\"org.springframework.boot:spring-boot-starter-jetty\") \/\/ end::jetty[] \/\/ tag::actuator[] compile(\"org.springframework.boot:spring-boot-starter-actuator\")*\/ \/\/ end::actuator[] testCompile(\"junit:junit\") }\n\nI quickly ran <span class=\"lang:default decode:true crayon-inline\">gradle clean build<\/span> and sure enough, this time the build size had shrunk to almost a third of its original size \u2013 **5.5 megabytes**! Also, the <span class=\"lang:default decode:true crayon-inline \">lintGradle<\/span> task at the end of the build didn\u2019t report any errors.\n\nI can certainly see the value of this plugin in a continuous integration system as unused libraries cause host of memory related issues like lack of permgen space in java web containers. Also, its better to let developers focus on core development and let the periphery issues such as these be handled by an automated plugin like this one.\n\nI hope the developers on the github project fix the issues related to the <span class=\"lang:default decode:true crayon-inline \">fixGradleLint<\/span> task soon.\n\nCheck out the source for this test on [github](https:\/\/github.com\/manthanhd\/gradle-lint-test).\n\n\n","html":"<p style=\"text-align: justify;\">Dependency management is a complex thing. A typical gradle project has several dependencies, each of which in turn has multiple other dependencies and so on. This continues and what forms as a result is called a dependency graph. While this is great, unused dependencies is a big problem. For instance, say your project depends on dependency A which then depends on B, C and D. Your project also depends directly with another dependency E which in turn depends on F and G. The dependency graph would look something like this:<\/p>\n<p style=\"text-align: justify;\">Your Project -&gt; A -&gt; (B, C, D)<br \/>\nYour Project -&gt; E -&gt; (F, G)<\/p>\n<p style=\"text-align: justify;\">Now, your project probably only uses code in A which only then directly depends on some code in B. Also, maybe you also have some code that depends on E but that code in E doesn&#8217;t depend directly on neither F, nor G. This means that C, D, F and G are your unused dependencies.<!--more--><\/p>\n<p style=\"text-align: justify;\">If you knew this from the start, you could just exclude those libraries. However, tracking this manually is incredibly difficult as the more complex the libraries are, the more complex the web of inter-dependencies becomes. I&#8217;ve been looking for an automated solution to this for a while and recently, I found that Netflix has a plugin that could help. This is called the gradle-lint-plugin.<\/p>\n<p style=\"text-align: justify;\">Its part of Netflix&#8217;s OSS and is available on <a href=\"https:\/\/github.com\/nebula-plugins\/gradle-lint-plugin\" target=\"_blank\">github<\/a> to download. Its artifacts are available in <a href=\"https:\/\/bintray.com\/bintray\/jcenter\" target=\"_blank\">JCenter<\/a> maven repository.<\/p>\n<p style=\"text-align: justify;\"><a href=\"https:\/\/bintray.com\/nebula\/gradle-plugins\/gradle-lint-plugin\/_latestVersion\"> <img src=\"https:\/\/api.bintray.com\/packages\/nebula\/gradle-plugins\/gradle-lint-plugin\/images\/download.svg\" alt=\"\" \/><\/a><\/p>\n<p style=\"text-align: justify;\">To test it out, I quickly got a gradle project up and running by following the <a href=\"https:\/\/spring.io\/guides\/gs\/spring-boot\/\" target=\"_blank\">spring boot tutorial<\/a>. To make things a slightly more interesting, I added the <span class=\"lang:default decode:true crayon-inline \">org.apache:httpcomponents:httpclient<\/span> library as a dependency in my <span class=\"lang:default decode:true crayon-inline \">build.gradle<\/span> file:<\/p>\n<pre class=\"\">compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'<\/pre>\n<p style=\"text-align: justify;\">Also, for reference, here&#8217;s my gradle version:<\/p>\n<pre class=\"lang:default mark:4 decode:true\">C:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test&gt;gradle -v\r\n\r\n------------------------------------------------------------\r\nGradle 2.12\r\n------------------------------------------------------------\r\n\r\nBuild time:   2016-03-14 08:32:03 UTC\r\nBuild number: none\r\nRevision:     b29fbb64ad6b068cb3f05f7e40dc670472129bc0\r\n\r\nGroovy:       2.4.4\r\nAnt:          Apache Ant(TM) version 1.9.3 compiled on December 23 2013\r\nJVM:          1.7.0_10 (Oracle Corporation 23.6-b04)\r\nOS:           Windows 8 6.2 amd64<\/pre>\n<p style=\"text-align: justify;\">I ran the gradle clean build command to assess the initial size of the build. As expected, the war file was massive at <strong>14.3 megabytes<\/strong>. For a small project that has a simple healthcheck endpoint, this is huge. So, to streamline this, I followed the <a href=\"https:\/\/github.com\/nebula-plugins\/gradle-lint-plugin\/wiki\/Using-Lint\" target=\"_blank\">gradle-lint-plugin guide<\/a> to use and apply the plugin. I added some rules of my own too. Here&#8217;s what my <span class=\"lang:default decode:true crayon-inline\">build.gradle<\/span> file looks like:<\/p>\n<pre class=\"lang:default mark:4,11-18,32 decode:true \">buildscript {\r\n    repositories {\r\n        mavenCentral()\r\n        jcenter()\r\n    }\r\n    dependencies {\r\n        classpath(\"org.springframework.boot:spring-boot-gradle-plugin:1.3.5.RELEASE\")\r\n    }\r\n}\r\n\r\nplugins {\r\n    id 'nebula.lint' version '0.30.12'\r\n}\r\n\r\nallprojects {\r\n    apply plugin: 'nebula.lint'\r\n    gradleLint.rules = ['all-dependency', 'unused-dependency', 'unused-exclude-by-dep'] \/\/ add as many rules here as you'd like\r\n}\r\n\r\napply plugin: 'java'\r\napply plugin: 'eclipse'\r\napply plugin: 'idea'\r\napply plugin: 'spring-boot'\r\n\r\njar {\r\n    baseName = 'gs-spring-boot'\r\n    version =  '0.1.0'\r\n}\r\n\r\nrepositories {\r\n    mavenCentral()\r\n    jcenter()\r\n}\r\n\r\nsourceCompatibility = 1.7\r\ntargetCompatibility = 1.7\r\n\r\ndependencies {\r\n    compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'\r\n    \/\/ tag::jetty[]\r\n    compile(\"org.springframework.boot:spring-boot-starter-web\") {\r\n        exclude module: \"spring-boot-starter-tomcat\"\r\n    }\r\n    compile(\"org.springframework.boot:spring-boot-starter-jetty\")\r\n    \/\/ end::jetty[]\r\n    \/\/ tag::actuator[]\r\n    compile(\"org.springframework.boot:spring-boot-starter-actuator\")\r\n    \/\/ end::actuator[]\r\n    testCompile(\"junit:junit\")\r\n}\r\n\r\ntask wrapper(type: Wrapper) {\r\n    gradleVersion = '2.1'\r\n}<\/pre>\n<p style=\"text-align: justify;\">Once that was out of the way, I simply ran <span class=\"lang:default decode:true crayon-inline \">gradle clean build<\/span> command.<\/p>\n<pre class=\"lang:default decode:true \">C:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test&gt;gradle clean build\r\n:clean\r\n:compileJava\r\n:processResources UP-TO-DATE\r\n:classes\r\n:findMainClass\r\n:jar\r\n:bootRepackage\r\n:assemble\r\n:compileTestJava UP-TO-DATE\r\n:processTestResources UP-TO-DATE\r\n:testClasses UP-TO-DATE\r\n:test UP-TO-DATE\r\n:check UP-TO-DATE\r\n:build\r\n:lintGradle\r\n\r\nThis project contains lint violations.  A complete listing of the violations follows.\r\nBecause none were serious, the build's overall status was unaffected.\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework:spring-web:4.2.6.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot:1.3.5.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework:spring-context:4.2.6.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework:spring-web:4.2.6.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot:1.3.5.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  one or more classes in org.springframework:spring-context:4.2.6.RELEASE are required by your code directly\r\n\r\nwarning   unused-dependency                  this dependency is unused and can be removed\r\nbuild.gradle:39\r\ncompile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'\r\n\r\nwarning   unused-dependency                  this dependency is unused and can be removed\r\nbuild.gradle:39\r\ncompile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'\r\n\r\n? build.gradle: 10 problems (0 errors, 10 warnings)\r\n\r\nTo apply fixes automatically, run fixGradleLint, review, and commit the changes.\r\n\r\n\r\nBUILD SUCCESSFUL\r\n\r\nTotal time: 15.742 secs<\/pre>\n<p style=\"text-align: justify;\">Excellent! It found the unused dependencies. I was quite pleased with this result so I ran <span class=\"lang:default decode:true crayon-inline\">gradle fixGradleLint<\/span> command. According to the wiki, this command should fix the issues that it found by making changes to the <span class=\"lang:default decode:true crayon-inline\">build.gradle<\/span> file. Unfortunately, this didn&#8217;t work as it resulted in an error.<\/p>\n<pre class=\"lang:default mark:8 decode:true \">C:\\Users\\manthanhd\\Documents\\projects\\gradle-lint-test&gt;gradle fixGradleLint\r\n:fixGradleLint FAILED\r\n\r\nFAILURE: Build failed with an exception.\r\n\r\n* What went wrong:\r\nExecution failed for task ':fixGradleLint'.\r\n&gt; com.netflix.nebula.lint.jgit.api.errors.PatchApplyException: Cannot apply: HunkHeader[36,7-&gt;36,14]\r\n\r\n* Try:\r\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.\r\n\r\nBUILD FAILED\r\n\r\nTotal time: 10.808 secs<\/pre>\n<p style=\"text-align: justify;\">I&#8217;ve already raised a github issue to draw their attention to this problem. For information on progress, feel free to watch or add to the <a href=\"https:\/\/github.com\/nebula-plugins\/gradle-lint-plugin\/issues\/38\" target=\"_blank\">issue<\/a>.<\/p>\n<p style=\"text-align: justify;\">So, even if the automated fix didn&#8217;t work, I wasn&#8217;t done yet. I looked around in the <span class=\"lang:default decode:true crayon-inline \">build<\/span> directory looking for something that I can use to apply for the fix myself. After all, since <span class=\"lang:default decode:true crayon-inline \">gradle lintGradle<\/span> and <span class=\"lang:default decode:true crayon-inline \">gradle fixGradleLint<\/span> are two separate commands, independent of each other, it must store some form of state somewhere. Fortunately I found it. It stores its state in a file called <span class=\"lang:default decode:true crayon-inline\">lint.patch<\/span> directly under the <span class=\"lang:default decode:true crayon-inline\">build<\/span> directory at the root of your project.<\/p>\n<p style=\"text-align: justify;\">Hurrah! I quickly opened it. It was a normal git patch file. While looking at it, I was slightly confused with the additions that it was making. Here&#8217;s what mine looked like:<\/p>\n<pre class=\"\">diff --git a\/build.gradle b\/build.gradle\r\n--- a\/build.gradle\r\n+++ b\/build.gradle\r\n@@ -36,7 +36,14 @@\r\n targetCompatibility = 1.7\r\n \r\n dependencies {\r\n+   compile 'org.springframework:spring-web:4.2.6.RELEASE'\r\n+   compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'\r\n+   compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'\r\n+   compile 'org.springframework:spring-context:4.2.6.RELEASE'\r\n+   compile 'org.springframework:spring-web:4.2.6.RELEASE'\r\n+   compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'\r\n+   compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'\r\n+   compile 'org.springframework:spring-context:4.2.6.RELEASE'\r\n-    compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'\r\n     \/\/ tag::jetty[]\r\n     compile(\"org.springframework.boot:spring-boot-starter-web\") {\r\n         exclude module: \"spring-boot-starter-tomcat\"<\/pre>\n<p style=\"text-align: justify;\">Looking at that file, I could immediately tell that it was adding some compile time dependencies and was removing one httpcomponents dependency. While I agree with the removal of the httpcomponents dependency, I was slightly confused why it wasn&#8217;t replacing the existing spring-boot dependencies with the ones it was adding. It should&#8217;ve replaced them because having both provides no benefit. What its trying to add is more specific than the dependencies that are already there! To prove myself right, I removed all the existing <span class=\"lang:default decode:true crayon-inline \">compile<\/span> dependencies (expect <span class=\"lang:default decode:true crayon-inline \">testCompile<\/span> ones) and added the ones that it was going to add from the patch file. Here&#8217;s what the dependency section of my <span class=\"lang:default decode:true crayon-inline \">build.gradle<\/span> looked like:<\/p>\n<pre class=\"\">dependencies {\r\n    compile 'org.springframework:spring-web:4.2.6.RELEASE'\r\n    compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'\r\n    compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'\r\n    compile 'org.springframework:spring-context:4.2.6.RELEASE'\r\n    compile 'org.springframework:spring-web:4.2.6.RELEASE'\r\n    compile 'org.springframework.boot:spring-boot:1.3.5.RELEASE'\r\n    compile 'org.springframework.boot:spring-boot-autoconfigure:1.3.5.RELEASE'\r\n    compile 'org.springframework:spring-context:4.2.6.RELEASE'\r\n    \/*compile group: 'org.apache.httpcomponents', name: 'httpclient', version: '4.5.2'\r\n    \/\/ tag::jetty[]\r\n    compile(\"org.springframework.boot:spring-boot-starter-web\") {\r\n        exclude module: \"spring-boot-starter-tomcat\"\r\n    }\r\n    compile(\"org.springframework.boot:spring-boot-starter-jetty\")\r\n    \/\/ end::jetty[]\r\n    \/\/ tag::actuator[]\r\n    compile(\"org.springframework.boot:spring-boot-starter-actuator\")*\/\r\n    \/\/ end::actuator[]\r\n    testCompile(\"junit:junit\")\r\n}<\/pre>\n<p style=\"text-align: justify;\">I quickly ran <span class=\"lang:default decode:true crayon-inline\">gradle clean build<\/span> and sure enough, this time the build size had shrunk to almost a third of its original size &#8211; <strong>5.5 megabytes<\/strong>! Also, the <span class=\"lang:default decode:true crayon-inline \">lintGradle<\/span> task at the end of the build didn&#8217;t report any errors.<\/p>\n<p style=\"text-align: justify;\">I can certainly see the value of this plugin in a continuous integration system as unused libraries cause host of memory related issues like lack of permgen space in java web containers. Also, its better to let developers focus on core development and let the periphery issues such as these be handled by an automated plugin like this one.<\/p>\n<p style=\"text-align: justify;\">I hope the developers on the github project fix the issues related to the <span class=\"lang:default decode:true crayon-inline \">fixGradleLint<\/span> task soon.<\/p>\n<p style=\"text-align: justify;\">Check out the source for this test on <a href=\"https:\/\/github.com\/manthanhd\/gradle-lint-test\" target=\"_blank\">github<\/a>.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 26 Jun 2016 15:46:15 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:20:52 +0000","updated_by":1,"published_at":"Sun, 26 Jun 2016 15:46:15 +0000","published_by":1},{"id":398,"title":"Docker proxy settings","slug":"docker-proxy-settings","markdown":"\nHere\u2019s a quick way to configure docker to work with proxy. Keep in mind that this only works if you are using boot2docker or a docker-machine. This generally applies to Windows or Mac operating systems.\n\nCreate a new docker-machine with proxy:\n\ndocker-machine create -d virtualbox \\ --engine-env HTTP_PROXY=http:\/\/user:password@example.com:port \\ --engine-env HTTPS_PROXY=https:\/\/user:password@example.com:port \\ --engine-env NO_PROXY=example2.com \\ proxybox\n\nEdit user, password, example.com and port variables with appropriate values. Also, make sure you have correct domains excluded within <span class=\"lang:default decode:true crayon-inline \">NO_PROXY<\/span>\u00a0variable.\n\nTo change proxy credentials later on, edit\u00a0<span class=\"lang:default decode:true crayon-inline\">\/var\/lib\/boot2docker\/profile<\/span>\u00a0and set <span class=\"lang:default decode:true crayon-inline \">HTTP_PROXY<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">HTTPS_PROXY<\/span> environment variables.\n\n\n","html":"<p>Here&#8217;s a quick way to configure docker to work with proxy. Keep in mind that this only works if you are using boot2docker or a docker-machine. This generally applies to Windows or Mac operating systems.<\/p>\n<p>Create a new docker-machine with proxy:<!--more--><\/p>\n<pre class=\"lang:sh decode:true\">docker-machine create -d virtualbox \\\r\n    --engine-env HTTP_PROXY=http:\/\/user:password@example.com:port \\\r\n    --engine-env HTTPS_PROXY=https:\/\/user:password@example.com:port \\\r\n    --engine-env NO_PROXY=example2.com \\\r\n    proxybox<\/pre>\n<p>Edit user, password, example.com and port variables with appropriate values. Also, make sure you have correct domains excluded within <span class=\"lang:default decode:true crayon-inline \">NO_PROXY<\/span>\u00a0variable.<\/p>\n<p>To change proxy credentials later on, edit\u00a0<span class=\"lang:default decode:true crayon-inline\">\/var\/lib\/boot2docker\/profile<\/span>\u00a0and set <span class=\"lang:default decode:true crayon-inline \">HTTP_PROXY<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">HTTPS_PROXY<\/span> environment variables.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 12 Aug 2016 23:07:22 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:20:20 +0000","updated_by":1,"published_at":"Fri, 12 Aug 2016 23:07:22 +0000","published_by":1},{"id":402,"title":"Streaming folders from a server over SSH using tar and gzip","slug":"streaming-folders-from-a-server-using-tar-and-gzip","markdown":"\nThis is how you can stream a folder or multiple folders from a server over SSH protocol.\n\nTo stream folders in uncompressed, raw format, run:\n\nssh ec2-user@xx.xxx.xx.xxx \"sudo tar -cO \/folder1 \/opt\/folder2\" | tar -xf -\n\nSince the tar file is being streamed raw, this could potentially take longer as more data is being passed between server and client. In my test for 700 megabytes of data, this command took 1:56 seconds.\n\nThere\u2019s another method. This streams the tar file, pipes it into gzip on the remote server which comes out as compressed gzipped stream down to the client. The client then passes this compressed stream into gunzip which decompresses the stream and finally then pipes it into tar to extract it into its original folders.\n\nssh -i ~\/.ssh\/lao345-sandbox.pem ec2-user@10.122.64.165 \"sudo tar -cO \/prod\/msp \/opt\/beasys | gzip -c -\" | gunzip -c - | tar -xf -\n\nWith the same data as above, this method took smaller amount of time, 1:13 seconds to be exact.\n\n\n","html":"<p>This is how you can stream a folder or multiple folders from a server over SSH protocol.<\/p>\n<p>To stream folders in uncompressed, raw format, run:<\/p>\n<pre class=\"lang:sh decode:true \">ssh ec2-user@xx.xxx.xx.xxx \"sudo tar -cO \/folder1 \/opt\/folder2\" | tar -xf -<\/pre>\n<p>Since the tar file is being streamed raw, this could potentially take longer as more data is being passed between server and client. In my test for 700 megabytes of data, this command took 1:56 seconds.<\/p>\n<p>There&#8217;s another method. <!--more-->This streams the tar file, pipes it into gzip on the remote server which comes out as compressed gzipped stream down to the client. The client then passes this compressed stream into gunzip which decompresses the stream and finally then pipes it into tar to extract it into its original folders.<\/p>\n<pre class=\"lang:sh decode:true \">ssh -i ~\/.ssh\/lao345-sandbox.pem ec2-user@10.122.64.165 \"sudo tar -cO \/prod\/msp \/opt\/beasys | gzip -c -\" | gunzip -c - | tar -xf -<\/pre>\n<p>With the same data as above, this method took smaller amount of time, 1:13 seconds to be exact.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 22 Aug 2016 23:09:35 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:19:20 +0000","updated_by":1,"published_at":"Mon, 22 Aug 2016 23:09:35 +0000","published_by":1},{"id":404,"title":"Some docker helper commands","slug":"some-docker-helper-commands","markdown":"\nSo a lot of times when you\u2019re working with some cool docker containers, you might need to remove all those dead containers that are just lying around. This could happen if you\u2019re running docker containers in background or forgot to use <span class=\"lang:default decode:true crayon-inline\">\u2013rm<\/span> flag.\n\nNot to worry, just run this command to remove ALL running docker containers in one go:\n\n docker ps -a | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}\n\nThat will list all docker containers, cut the first column, remove the one with <span class=\"lang:sh decode:true crayon-inline\">CONTAINER<\/span> in it and pipe all the remaining ones one by one using xargs into docker rm command.\n\nAwesome right? Well, what if you only wanted to remove only the ones that have been STOPPED? Here you go:\n\ndocker ps -a | grep Exited | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}\n\nAs you can see, this command only removes the containers that have \u2018Exited\u2019.\n\nTo stop all running containers:\n\ndocker ps -a | grep Up | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker stop {}\n\nSome helper functions:\n\nfunction dockstopall() { docker ps -a | grep Up | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker stop {} } function dockrmall() { docker ps -a | grep Exited | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {} }\n\nAdd these to your <span class=\"lang:default decode:true crayon-inline \">.profile<\/span> or <span class=\"lang:default decode:true crayon-inline \">.bash_profile<\/span> or whatever shell profile you use. Once added, to apply in existing shell, run:\n\nsource ~\/.profile\n\n\u00a0\n\n\n","html":"<p>So a lot of times when you&#8217;re working with some cool docker containers, you might need to remove all those dead containers that are just lying around. This could happen if you&#8217;re running docker containers in background or forgot to use <span class=\"lang:default decode:true crayon-inline\">&#8211;rm<\/span> flag.<\/p>\n<p>Not to worry, just run this command to remove ALL running docker containers in one go:<!--more--><\/p>\n<pre class=\"lang:sh decode:true\"> docker ps -a | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}<\/pre>\n<p>That will list all docker containers, cut the first column, remove the one with <span class=\"lang:sh decode:true crayon-inline\">CONTAINER<\/span> in it and pipe all the remaining ones one by one using xargs into docker rm command.<\/p>\n<p>Awesome right? Well, what if you only wanted to remove only the ones that have been STOPPED? Here you go:<\/p>\n<pre class=\"lang:sh decode:true \">docker ps -a | grep Exited | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}<\/pre>\n<p>As you can see, this command only removes the containers that have &#8216;Exited&#8217;.<\/p>\n<p>To stop all running containers:<\/p>\n<pre class=\"lang:sh decode:true \">docker ps -a | grep Up | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker stop {}<\/pre>\n<p>Some helper functions:<\/p>\n<pre class=\"lang:sh decode:true\">function dockstopall() {\r\n    docker ps -a | grep Up | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker stop {}\r\n}\r\n\r\nfunction dockrmall() {\r\n    docker ps -a | grep Exited | cut -d ' ' -f1 | grep -v CONTAINER | xargs -n 1 -I {} docker rm {}\r\n}<\/pre>\n<p>Add these to your <span class=\"lang:default decode:true crayon-inline \">.profile<\/span> or <span class=\"lang:default decode:true crayon-inline \">.bash_profile<\/span> or whatever shell profile you use. Once added, to apply in existing shell, run:<\/p>\n<pre class=\"lang:sh decode:true \">source ~\/.profile<\/pre>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 27 Aug 2016 12:43:26 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 13:19:03 +0000","updated_by":1,"published_at":"Sat, 27 Aug 2016 12:43:26 +0000","published_by":1},{"id":426,"title":"Some quick tomcat shell functions","slug":"some-quick-tomcat-shell-functions","markdown":"\nHere are some bash profile functions to help you use tomcat as a pro. The following needs to be done in either your <span class=\"lang:default decode:true crayon-inline \">.profile<\/span> file or <span class=\"lang:default decode:true crayon-inline\">.bash_profile<\/span>, depending on what shell you have and whichever method you prefer. First off, make sure you have exported <span class=\"lang:default decode:true crayon-inline \">CATALINA_HOME<\/span> variable pointing to the base of your tomcat installation.\n\nexport CATALINA_HOME=\/opt\/tomcat\n\nOnce you\u2019ve got that done, you can define the following shell functions.\n\nfunction tomst() { ${CATALINA_HOME}\/bin\/startup.sh } function tomsh() { ${CATALINA_HOME}\/bin\/shutdown.sh } function tomlog() { less ${CATALINA_HOME}\/logs\/catalina.out } function tomls() { ls -l ${CATALINA_HOME}\/webapps | grep -v .war$ } function tomlogf() { tail -f ${CATALINA_HOME}\/logs\/catalina.out } function tomps() { ps -ef | grep tomcat | grep -v grep | cut -d' ' -f2 } function tomkill() { kill `tomps` }\n\nLets break these down. The first command, <span class=\"lang:default decode:true crayon-inline \">tomst<\/span>\u00a0, will start up your tomcat using the <span class=\"lang:default decode:true crayon-inline \">startup.sh<\/span> script. Depending on your setup, you might prefer to use the startup.sh script over the service command. This is a neat way to do it from any directory.\n\nNext is the <span class=\"lang:default decode:true crayon-inline \">tomsh<\/span> command which will help you shut down your tomcat container. Again, like the previous command, this is using the <span class=\"lang:default decode:true crayon-inline\">shutdown.sh<\/span> script instead of the service command. You could replace both of these with the service commands if you prefer.\n\nThe <span class=\"lang:default decode:true crayon-inline \">tomlog<\/span> and <span class=\"lang:default decode:true crayon-inline\">tomlogf<\/span> commands both display the <span class=\"lang:default decode:true crayon-inline \">catalina.log<\/span> log file, however the <span class=\"lang:default decode:true crayon-inline \">tomlogf<\/span> command will follow (<span class=\"lang:default decode:true crayon-inline\">tail<\/span>) the log.\n\nThe <span class=\"lang:default decode:true crayon-inline \">tomls<\/span> command will show whats in your webapps directory. Now, this command will only show you your war files so these might not necessarily be deployed, but its a good way to list your deployments.\n\nThe <span class=\"lang:default decode:true crayon-inline \">tomps<\/span> command will show you the process ID of the running tomcat container. As of this moment, this only looks for the keyword tomcat in your processes. If you have multiple tomcat containers running, you will have to make the <span class=\"lang:default decode:true crayon-inline \">grep<\/span> more specific.\n\nLastly, the <span class=\"lang:default decode:true crayon-inline \">tomkill<\/span> command effectively runs the <span class=\"lang:default decode:true crayon-inline \">tomps<\/span> command but with a prefix of a kill command, effectively executing a kill on your tomcat process.\n\n\n","html":"<p>Here are some bash profile functions to help you use tomcat as a pro. The following needs to be done in either your <span class=\"lang:default decode:true crayon-inline \">.profile<\/span> file or <span class=\"lang:default decode:true crayon-inline\">.bash_profile<\/span>, depending on what shell you have and whichever method you prefer. First off, make sure you have exported <span class=\"lang:default decode:true crayon-inline \">CATALINA_HOME<\/span> variable pointing to the base of your tomcat installation.<\/p>\n<pre class=\"lang:sh decode:true\">export CATALINA_HOME=\/opt\/tomcat<\/pre>\n<p>Once you&#8217;ve got that done, you can define the following shell functions.<!--more--><\/p>\n<pre class=\"lang:sh decode:true\">function tomst() {\r\n  ${CATALINA_HOME}\/bin\/startup.sh\r\n}\r\n\r\nfunction tomsh() {\r\n    ${CATALINA_HOME}\/bin\/shutdown.sh\r\n}\r\n\r\nfunction tomlog() {\r\n    less ${CATALINA_HOME}\/logs\/catalina.out\r\n}\r\n\r\nfunction tomls() {\r\n    ls -l ${CATALINA_HOME}\/webapps | grep -v .war$\r\n}\r\n\r\nfunction tomlogf() {\r\n    tail -f ${CATALINA_HOME}\/logs\/catalina.out\r\n}\r\n\r\nfunction tomps() {\r\n    ps -ef | grep tomcat | grep -v grep | cut -d' ' -f2\r\n}\r\n\r\nfunction tomkill() {\r\n    kill `tomps`\r\n}<\/pre>\n<p>Lets break these down. The first command, <span class=\"lang:default decode:true crayon-inline \">tomst<\/span>\u00a0, will start up your tomcat using the <span class=\"lang:default decode:true crayon-inline \">startup.sh<\/span> script. Depending on your setup, you might prefer to use the startup.sh script over the service command. This is a neat way to do it from any directory.<\/p>\n<p>Next is the <span class=\"lang:default decode:true crayon-inline \">tomsh<\/span> command which will help you shut down your tomcat container. Again, like the previous command, this is using the <span class=\"lang:default decode:true crayon-inline\">shutdown.sh<\/span> script instead of the service command. You could replace both of these with the service commands if you prefer.<\/p>\n<p>The <span class=\"lang:default decode:true crayon-inline \">tomlog<\/span> and <span class=\"lang:default decode:true crayon-inline\">tomlogf<\/span> commands both display the <span class=\"lang:default decode:true crayon-inline \">catalina.log<\/span> log file, however the <span class=\"lang:default decode:true crayon-inline \">tomlogf<\/span> command will follow (<span class=\"lang:default decode:true crayon-inline\">tail<\/span>) the log.<\/p>\n<p>The <span class=\"lang:default decode:true crayon-inline \">tomls<\/span> command will show whats in your webapps directory. Now, this command will only show you your war files so these might not necessarily be deployed, but its a good way to list your deployments.<\/p>\n<p>The <span class=\"lang:default decode:true crayon-inline \">tomps<\/span> command will show you the process ID of the running tomcat container. As of this moment, this only looks for the keyword tomcat in your processes. If you have multiple tomcat containers running, you will have to make the <span class=\"lang:default decode:true crayon-inline \">grep<\/span> more specific.<\/p>\n<p>Lastly, the <span class=\"lang:default decode:true crayon-inline \">tomkill<\/span> command effectively runs the <span class=\"lang:default decode:true crayon-inline \">tomps<\/span> command but with a prefix of a kill command, effectively executing a kill on your tomcat process.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sat, 27 Aug 2016 18:12:02 +0000","created_by":1,"updated_at":"Sat, 27 Aug 2016 18:12:23 +0000","updated_by":1,"published_at":"Sat, 27 Aug 2016 18:12:02 +0000","published_by":1},{"id":430,"title":"Deploying single instance Wordpress site on CoreOS","slug":"deploying-wordpress-on-coreos","markdown":"\nLets start off by deploying a single MySQL DB instance. For this, we\u2019re going to use the default mysql image from docker hub. In order to deploy anything to CoreOS, you need to first create a service file. Here\u2019s one for the MySQL that we\u2019re going to deploy.\n\n[Unit] Description=MySQL database for wordpress [Service] ExecStartPre=-\/usr\/bin\/docker stop wpdb ExecStartPre=-\/usr\/bin\/docker rm wpdb ExecStartPre=-\/usr\/bin\/docker pull mysql:latest ExecStart=\/usr\/bin\/docker run -e MYSQL_ROOT_PASSWORD=glory86 --name wpdb -t mysql:latest [X-Fleet] Conflicts=wpdb.service\n\nSave that file as <span class=\"lang:default decode:true crayon-inline\">wpdb.service<\/span>. Lets examine that file. As you can see, the file is split into three distinct sections. <span class=\"lang:default decode:true crayon-inline\">Unit<\/span>, <span class=\"lang:default decode:true crayon-inline\">Service<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">X-Fleet<\/span>. The Unit section tells CoreOS, or more specifically <span class=\"lang:default decode:true crayon-inline\">fleet<\/span>, what this service is about and what it is for. Since this one is quite simple, we only have a <span class=\"lang:default decode:true crayon-inline \">Description<\/span>\u00a0here.\n\nThe second section is Service. This basically defines the mysql service. We\u2019ve got four lines. The first three cover <span class=\"lang:default decode:true crayon-inline\">ExecStartPre<\/span>\u00a0section where we define commands that it needs to run before starting our service. In this case, we are stopping and removing any existing containers named <span class=\"lang:default decode:true crayon-inline \">wpdb<\/span>\u00a0(as that\u2019s what we\u2019re calling our database instance). We\u2019re also pulling in the latest mysql image from dockerhub so that we can be prepared when the service is started.\n\nNext we\u2019re defining what happens when the service is started by defining the <span class=\"lang:default decode:true crayon-inline\">ExecStart<\/span>\u00a0property. Here, we\u2019re running a simple docker run command. Since the mysql image needs a password to be defined in the environment, we\u2019re passing one here. We\u2019re also naming our database to be <span class=\"lang:default decode:true crayon-inline\">wpdb<\/span>. Note that we are not binding any ports as we don\u2019t want access to the mysql locally on any machine. Later on, we\u2019ll let our wordpress docker container talk directly to the mysql container by using docker <span class=\"lang:default decode:true crayon-inline\">link<\/span>.\n\nNow load up the <span class=\"lang:default decode:true crayon-inline\">wpdb.service<\/span>\u00a0using <span class=\"lang:default decode:true crayon-inline \">fleetctl<\/span>\u00a0command:\n\nfleetctl load wpdb.service\n\nOnce loaded, you can start it by running the <span class=\"lang:default decode:true crayon-inline \">start<\/span>\u00a0command:\n\nfleetctl start wpdb.service\n\nNow that our database container is deployed, lets deploy our wordpress container. To do that, we need another service file.\n\n[Unit] Description=Wordpress blog server for www.manthanhd.com After=wpdb.service Requires=wpdb.service [Service] ExecStartPre=-\/usr\/bin\/docker stop www-manthanhd-com ExecStartPre=-\/usr\/bin\/docker rm www-manthanhd-com ExecStartPre=-\/usr\/bin\/docker pull wordpress:latest ExecStart=\/usr\/bin\/docker run --name www-manthanhd-com --link wpdb:mysql -p 80:80 -e WORDPRESS_DB_USER=root -e WORDPRESS_DB_PASSWORD=glory86 -e WORDPRESS_DB_HOST=mysql -t wordpress:latest [X-Fleet] Conflicts=www-manthanhd.service MachineOf=wpdb.service\n\nSave that file as <span class=\"lang:default decode:true crayon-inline\">www-manthanhd.service<\/span>. As you can see, this file is very similar to the <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0file that we created earlier. However, there are a few changes.\n\nSince we want to make sure that our database container is running before we run our wordpress container, we have to make sure that we have <span class=\"lang:default decode:true crayon-inline \">After<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">Requires<\/span>\u00a0properties defined accordingly. In this case, we\u2019re instructing fleet to run our wordpress container that we require the <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0and to run this container after the <span class=\"lang:default decode:true crayon-inline\">wpdb.service<\/span>.\n\nAnother notable change is in the <span class=\"lang:default decode:true crayon-inline \">X-Fleet<\/span>\u00a0section. Here we\u2019re defining an extra property called <span class=\"lang:default decode:true crayon-inline \">MachineOf<\/span>\u00a0where we\u2019re telling fleet to deploy this wordpress container on the same machine as the <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0container. This is to ensure minimum latency between wordpress and the database.\n\nWe do have a slight change in the <span class=\"lang:default decode:true crayon-inline \">ExecStart<\/span>\u00a0where we\u2019re now linking the <span class=\"lang:default decode:true crayon-inline\">wpdb<\/span>\u00a0container to the wordpress container by using docker link as well as binding port <span class=\"lang:default decode:true crayon-inline\">80<\/span>.\n\nOnce done, load up the wordpress container service using <span class=\"lang:default decode:true crayon-inline\">fleetctl<\/span>:\n\nfleetctl load www-manthanhd.service\n\nAnd then start it:\n\nfleetctl start www-manthanhd.service\n\nLets check if the database and the wordpress containers are actually running. To do that, run:\n\nfleetctl list-units\n\nYou should see output similar to this:\n\nUNIT MACHINE ACTIVE SUB wpdb.service 7528fd75...\/172.17.8.101 active running www-manthanhd.service 7528fd75...\/172.17.8.101 active running\n\nThat\u2019s telling us that <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">www-manthanhd.service<\/span>\u00a0are both running on the same machine \u2013 <span class=\"lang:default decode:true crayon-inline \">172.17.8.101<\/span>\u00a0and are actually running!\n\nCool! Lets try to see if we can access wordpress. You can do this by navigating to port <span class=\"lang:default decode:true crayon-inline \">80<\/span>\u00a0of <span class=\"lang:default decode:true crayon-inline\">172.17.8.101<\/span>\u00a0or whatever ip address is shown above for <span class=\"lang:default decode:true crayon-inline \">www-manthanhd.service<\/span>\u00a0above for you.\n\n\n","html":"<p style=\"text-align: justify;\">Lets start off by deploying a single MySQL DB instance. For this, we&#8217;re going to use the default mysql image from docker hub. In order to deploy anything to CoreOS, you need to first create a service file. Here&#8217;s one for the MySQL that we&#8217;re going to deploy.<\/p>\n<pre class=\"lang:default decode:true\" title=\"wpdb.service\">[Unit]\nDescription=MySQL database for wordpress\n\n[Service]\nExecStartPre=-\/usr\/bin\/docker stop wpdb\nExecStartPre=-\/usr\/bin\/docker rm wpdb\nExecStartPre=-\/usr\/bin\/docker pull mysql:latest\nExecStart=\/usr\/bin\/docker run -e MYSQL_ROOT_PASSWORD=glory86 --name wpdb -t mysql:latest\n\n\n[X-Fleet]\nConflicts=wpdb.service<\/pre>\n<p style=\"text-align: justify;\">Save that file as <span class=\"lang:default decode:true crayon-inline\">wpdb.service<\/span>. Lets examine that file. As you can see, the file is split into three distinct sections. <span class=\"lang:default decode:true crayon-inline\">Unit<\/span>, <span class=\"lang:default decode:true crayon-inline\">Service<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">X-Fleet<\/span>. The Unit section tells CoreOS, or more specifically <span class=\"lang:default decode:true crayon-inline\">fleet<\/span>, what this service is about and what it is for. Since this one is quite simple, we only have a <span class=\"lang:default decode:true crayon-inline \">Description<\/span>\u00a0here.<!--more--><\/p>\n<p style=\"text-align: justify;\">The second section is Service. This basically defines the mysql service. We&#8217;ve got four lines. The first three cover <span class=\"lang:default decode:true crayon-inline\">ExecStartPre<\/span>\u00a0section where we define commands that it needs to run before starting our service. In this case, we are stopping and removing any existing containers named <span class=\"lang:default decode:true crayon-inline \">wpdb<\/span>\u00a0(as that&#8217;s what we&#8217;re calling our database instance). We&#8217;re also pulling in the latest mysql image from dockerhub so that we can be prepared when the service is started.<\/p>\n<p style=\"text-align: justify;\">Next we&#8217;re defining what happens when the service is started by defining the <span class=\"lang:default decode:true crayon-inline\">ExecStart<\/span>\u00a0property. Here, we&#8217;re running a simple docker run command. Since the mysql image needs a password to be defined in the environment, we&#8217;re passing one here. We&#8217;re also naming our database to be <span class=\"lang:default decode:true crayon-inline\">wpdb<\/span>. Note that we are not binding any ports as we don&#8217;t want access to the mysql locally on any machine. Later on, we&#8217;ll let our wordpress docker container talk directly to the mysql container by using docker <span class=\"lang:default decode:true crayon-inline\">link<\/span>.<\/p>\n<p style=\"text-align: justify;\">Now load up the <span class=\"lang:default decode:true crayon-inline\">wpdb.service<\/span>\u00a0using <span class=\"lang:default decode:true crayon-inline \">fleetctl<\/span>\u00a0command:<\/p>\n<pre class=\"lang:default decode:true\">fleetctl load wpdb.service<\/pre>\n<p style=\"text-align: justify;\">Once loaded, you can start it by running the <span class=\"lang:default decode:true crayon-inline \">start<\/span>\u00a0command:<\/p>\n<pre class=\"lang:default decode:true \">fleetctl start wpdb.service<\/pre>\n<p style=\"text-align: justify;\">Now that our database container is deployed, lets deploy our wordpress container. To do that, we need another service file.<\/p>\n<pre class=\"lang:default decode:true\" title=\"www-manthanhd.service\">[Unit]\nDescription=Wordpress blog server for www.manthanhd.com\nAfter=wpdb.service\nRequires=wpdb.service\n\n[Service]\nExecStartPre=-\/usr\/bin\/docker stop www-manthanhd-com\nExecStartPre=-\/usr\/bin\/docker rm www-manthanhd-com\nExecStartPre=-\/usr\/bin\/docker pull wordpress:latest\nExecStart=\/usr\/bin\/docker run --name www-manthanhd-com --link wpdb:mysql -p 80:80 -e WORDPRESS_DB_USER=root -e WORDPRESS_DB_PASSWORD=glory86 -e WORDPRESS_DB_HOST=mysql -t wordpress:latest\n\n[X-Fleet]\nConflicts=www-manthanhd.service\nMachineOf=wpdb.service<\/pre>\n<p style=\"text-align: justify;\">Save that file as <span class=\"lang:default decode:true crayon-inline\">www-manthanhd.service<\/span>. As you can see, this file is very similar to the <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0file that we created earlier. However, there are a few changes.<\/p>\n<p style=\"text-align: justify;\">Since we want to make sure that our database container is running before we run our wordpress container, we have to make sure that we have <span class=\"lang:default decode:true crayon-inline \">After<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">Requires<\/span>\u00a0properties defined accordingly. In this case, we&#8217;re instructing fleet to run our wordpress container that we require the <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0and to run this container after the <span class=\"lang:default decode:true crayon-inline\">wpdb.service<\/span>.<\/p>\n<p style=\"text-align: justify;\">Another notable change is in the <span class=\"lang:default decode:true crayon-inline \">X-Fleet<\/span>\u00a0section. Here we&#8217;re defining an extra property called <span class=\"lang:default decode:true crayon-inline \">MachineOf<\/span>\u00a0where we&#8217;re telling fleet to deploy this wordpress container on the same machine as the <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0container. This is to ensure minimum latency between wordpress and the database.<\/p>\n<p style=\"text-align: justify;\">We do have a slight change in the <span class=\"lang:default decode:true crayon-inline \">ExecStart<\/span>\u00a0where we&#8217;re now linking the <span class=\"lang:default decode:true crayon-inline\">wpdb<\/span>\u00a0container to the wordpress container by using docker link as well as binding port <span class=\"lang:default decode:true crayon-inline\">80<\/span>.<\/p>\n<p style=\"text-align: justify;\">Once done, load up the wordpress container service using <span class=\"lang:default decode:true crayon-inline\">fleetctl<\/span>:<\/p>\n<pre class=\"lang:default decode:true \">fleetctl load www-manthanhd.service<\/pre>\n<p style=\"text-align: justify;\">And then start it:<\/p>\n<pre class=\"lang:default decode:true \">fleetctl start www-manthanhd.service<\/pre>\n<p style=\"text-align: justify;\">Lets check if the database and the wordpress containers are actually running. To do that, run:<\/p>\n<pre class=\"lang:default decode:true \">fleetctl list-units<\/pre>\n<p style=\"text-align: justify;\">You should see output similar to this:<\/p>\n<pre class=\"lang:default decode:true \" title=\"fleetctl list-units\">UNIT MACHINE ACTIVE SUB\nwpdb.service 7528fd75...\/172.17.8.101 active running\nwww-manthanhd.service 7528fd75...\/172.17.8.101 active running<\/pre>\n<p style=\"text-align: justify;\">That&#8217;s telling us that <span class=\"lang:default decode:true crayon-inline \">wpdb.service<\/span>\u00a0and <span class=\"lang:default decode:true crayon-inline\">www-manthanhd.service<\/span>\u00a0are both running on the same machine &#8211; <span class=\"lang:default decode:true crayon-inline \">172.17.8.101<\/span>\u00a0and are actually running!<\/p>\n<p style=\"text-align: justify;\">Cool! Lets try to see if we can access wordpress. You can do this by navigating to port <span class=\"lang:default decode:true crayon-inline \">80<\/span>\u00a0of <span class=\"lang:default decode:true crayon-inline\">172.17.8.101<\/span>\u00a0or whatever ip address is shown above for <span class=\"lang:default decode:true crayon-inline \">www-manthanhd.service<\/span>\u00a0above for you.<\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2016\/09\/core_and_docker.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 08 Sep 2016 20:23:05 +0000","created_by":1,"updated_at":"Wed, 14 Sep 2016 21:53:05 +0000","updated_by":1,"published_at":"Thu, 08 Sep 2016 20:23:05 +0000","published_by":1},{"id":439,"title":"Let's design: Asynchronous service","slug":"temp-slug-89","markdown":"\nHey, here\u2019s a great idea. Let\u2019s design an asynchronous service! You might be thinking; \u201cWhat\u2019s an Asynchronous Service? And why am I designing this mysterious thing?\u201d. Let\u2019s break that down.\n\nWho said that services have to be synchronous and over HTTP? Think of HTTP as just one interface. Just one of many ways of interacting with a service. You could also completely ignore TCP. TCP is also one way of interfacing with a web service. You could be using UDP or whatever custom protocol you want to use as long as it communicates with your web service and it works for you. Many people have done this as well so you won\u2019t be the only one doing it. People have, in real world, ditched TCP because it was too slow and went straight down to UDP to increase communication speeds between their services.\n\nSimply put, an asynchronous service is a service that responds asynchronously. No I\u2019m not talking about the one that runs on JavaScript, although that kind of implementation behind the scenes can help. Traditionally you have a web service that accepts requests and responds to those requests. However, most of the times, the client that is talking to the web service has to wait until the target service responds back. That\u2019s a synchronous service. With asynchronous service, the client doesn\u2019t have to wait for a response. It sort of like a fire and forget kid of communication. The client wants something done so it sends a message somewhere and it is immediately released once the message is received. The server can then process that message whenever it pleases.\n\nMost asynchronous architectures focus heavy on the message aspect of it. This means that it is almost critical that the message that was sent is not lost somewhere once it\u2019s receipt was acknowledged. Simply put, if the server said it is going to do something at some point, it should do it. To make sure that the message is not lost, servers use something like a message queue.\n\n\n","html":"<p>Hey, here&#8217;s a great idea. Let&#8217;s design an asynchronous service! You might be thinking; &#8220;What&#8217;s an Asynchronous Service? And why am I designing this mysterious thing?&#8221;. Let&#8217;s break that down.<\/p>\n<p>Who said that services have to be synchronous and over HTTP? Think of HTTP as just one interface. Just one of many ways of interacting with a service. You could also completely ignore TCP. TCP is also one way of interfacing with a web service. You could be using UDP or whatever custom protocol you want to use as long as it communicates with your web service and it works for you. Many people have done this as well so you won&#8217;t be the only one doing it. People have, in real world, ditched TCP because it was too slow and went straight down to UDP to increase communication speeds between their services.<\/p>\n<p>Simply put, an asynchronous service is a service that responds asynchronously. No I&#8217;m not talking about the one that runs on JavaScript, although that kind of implementation behind the scenes can help. Traditionally you have a web service that accepts requests and responds to those requests. However, most of the times, the client that is talking to the web service has to wait until the target service responds back. That&#8217;s a synchronous service. With asynchronous service, the client doesn&#8217;t have to wait for a response. It sort of like a fire and forget kid of communication. The client wants something done so it sends a message somewhere and it is immediately released once the message is received. The server can then process that message whenever it pleases.<\/p>\n<p>Most asynchronous architectures focus heavy on the message aspect of it. This means that it is almost critical that the message that was sent is not lost somewhere once it&#8217;s receipt was acknowledged. Simply put, if the server said it is going to do something at some point, it should do it. To make sure that the message is not lost, servers use something like a message queue.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 11 Sep 2016 20:35:49 +0000","created_by":1,"updated_at":"Sun, 11 Sep 2016 20:35:49 +0000","updated_by":1,"published_at":"","published_by":1},{"id":441,"title":"Introduction to Docker","slug":"introduction-to-docker","markdown":"\nDeploying applications is a complex task. You have to create some VMs, be it on DigitalOcean or AWS, download and install necessary prerequesites and then deploy your application. It would be easy if it were to end there, however, it doesn\u2019t.\n\nFollowing this you have application maintenance which includes deploying updated versions of your application in addition to other things like bug fixes. And this is where the real complexity starts. Your updated version might need another dependent application to be installed which in turn might need a version of some random tool to be upgraded. You did what was necessary but then you find out that the deployment of the updated application failed because you forgot to do that one null check in some corner of your application. So frantically you download the previous version of your application and try to restore it only to find that it doesn\u2019t work anymore since you upgraded that random tool to support your newer application.\n\nWhile all this is happening either your entire application is unavailable because your application is single instance or if it is indeed multi-instance split by a loadbalancer, all the other instances are being stressed out because one of the node is down.\n\nAnd now you are thinking. Well, there has to be a better way.\n\nWell my friend, there is.\n\n**queue star wars intro soundtrack**\n\nDocker.\n\nDocker allows you to create and deploy application containers. What is an Application Container you ask? Well, you can think of an application container like a VM but unlike a fully fledged VM, its the bit that surrounds your application. So in essence, instead of it being a virtual machine, it is a virtual container for your application. This allows application containers to start up within seconds while a virtual machine could take minutes. Also, application containers take up much less RAM as they don\u2019t have to load entire operating system in memory but only the bits that surround your application.\n\nSo how can these amazing and fast application containers help simplify your deployments? Here\u2019s how.\n\nBecause creating and destroying application containers is a inexpensive process, both in terms of compute resources and time, it encourages a philosophy of disposable infrastructure. The idea is that instead of creating your infrastructure and taking care of it for its lifetime, you create it only when you need it and then destroy it when its not needed. Also, because all containers emerge from their respective Dockerfile(s), which is code, they can be versioned along side your actual application. This means that if you roll back to a previous version, you will deploy a docker container for that application using *that* version of the Dockerfile. Also, because the container contains your project and all of its dependencies, its self contained and can be stood up or torn down without any impact on any of the existing versions.\n\nWe\u2019ve been on about Dockerfile for a while now so before we go any further lets see what it actually looks like. Here\u2019s an example:\n\nFROM ubuntu:14.04 RUN apt-get update -y RUN apt-get install -y openjdk-7-jre-headless ADD build\/libs\/*.jar \/app EXPOSE 8080 ENTRYPOINT java -jar \/app\/*.jar\n\nThat is a simple Dockerfile that deploys a Spring Boot application in a ubuntu based container. Briefly, the above Dockerfile defines a container image. The first line defines that our image is <span class=\"lang:default decode:true crayon-inline \">FROM<\/span> (based on) ubuntu 14.04 base image. Upon creating the image, it <span class=\"lang:default decode:true crayon-inline\">RUN<\/span>s <span class=\"lang:default decode:true crayon-inline \">apt-get update -y<\/span>\u00a0 and <span class=\"lang:default decode:true crayon-inline\">apt-get install -y openjdk-7-headless<\/span> commands. In addition to that, it adds (read copies) <span class=\"lang:default decode:true crayon-inline\">*.jar<\/span> file from the present working directory to <span class=\"lang:default decode:true crayon-inline \">\/app<\/span> folder to make it available in the container. Also, because we know that the application is going to run on port <span class=\"lang:default decode:true crayon-inline\">8080<\/span>, we\u2019re <span class=\"lang:default decode:true crayon-inline\">EXPOSE<\/span>(ing) (defining) that port so that it can be accessed from outside the container. Finally, we\u2019re defining the <span class=\"lang:default decode:true crayon-inline \">ENTRYPOINT<\/span> (command that docker will execute when the container is started) as a <span class=\"lang:default decode:true crayon-inline \">java -jar<\/span> command to execute our Spring Boot application.\n\nI\u2019ll go into the details about all the sections that comprise a Dockerfile some other time but for now, the thing to take away is that a Dockerfile forms basis of your container and is used to define what your application container image looks like. The image can then be used to create and run the container.\n\nI am using a custom spring boot application, however, if you follow the [REST tutorial ](https:\/\/spring.io\/guides\/gs\/rest-service\/)on Spring Boot Tutorials website, you should end up with the same project as me. Within your project, make sure that the Dockerfile is located at the root of the project (in same directory as the <span class=\"lang:default decode:true crayon-inline \">src<\/span> and <span class=\"lang:default decode:true crayon-inline\">build<\/span> folders).\n\nSo once you have a Dockerfile, you can run the following command to create your image. Make sure that your Dockerfile is called exactly as Dockerfile and you are executing the following command from the same folder as the Dockerfile.\n\ndocker build -t bootdemo2 .\n\nThe above command builds our image and tags it (defined by <span class=\"lang:default decode:true crayon-inline \">-t<\/span> option) with <span class=\"lang:default decode:true crayon-inline\">bootdemo2<\/span> tag. When the version (tag) is unspecified, docker assumes <span class=\"lang:default decode:true crayon-inline\">latest<\/span>. However, if you do want to specify one, you just need to replace <span class=\"lang:default decode:true crayon-inline\">bootdemo2<\/span> with <span class=\"lang:default decode:true crayon-inline\">bootdemo2:1.0<\/span> or rather, more generically, <span class=\"lang:default decode:true crayon-inline\">bootdemo2:<version><\/span>. So building the first version of the application would be like so:\n\ndocker build -t bootdemo2:1.0 .\n\nAnd then subsequently, the next minor version would be like:\n\ndocker build -t bootdemo2:1.2 .\n\nIn this case, just leave the tag blank to allow it to default to latest.\n\nIf everything went OK, you\u2019ll be able to see your image listed within the list of docker images on your machine. This can be viewed using the following command:\n\ndocker images\n\nYou should see something like this in your output:\n\nREPOSITORY TAG IMAGE ID CREATED SIZE bootdemo2 latest 4150e61520b2 27 hours ago 339.3 MB\n\nAmazing! You\u2019ve created your first Docker image. Hurrah! Lets run it to start a container off of that image. Simply run:\n\ndocker run -p 8080:8080 --name myspringbootapp --rm -it bootdemo2\n\nYou should see output like following:\n\n . ____ _ __ _ _ \/\\\\ \/ ___'_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | '_ | '_| | '_ \\\/ _` | \\ \\ \\ \\ \\\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) ' |____| .__|_| |_|_| |_\\__, | \/ \/ \/ \/ =========|_|==============|___\/=\/_\/_\/_\/ :: Spring Boot :: (v1.4.0.RELEASE) 2016-09-14 15:02:01.125 INFO 6 --- [ main] c.manthanhd.eventsmgmtapis.Application : Starting Application on 4c6454648246 with PID 6 (\/project\/build\/libs\/events-0.1.0.jar started by root in \/) 2016-09-14 15:02:01.129 INFO 6 --- [ main] c.manthanhd.eventsmgmtapis.Application : No active profile set, falling back to default profiles: default 2016-09-14 15:02:01.265 INFO 6 --- [ main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5b8ed9b6: startup date [Wed Sep 14 15:02:01 UTC 2016]; root of context hierarchy 2016-09-14 15:02:03.278 INFO 6 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http) 2016-09-14 15:02:03.294 INFO 6 --- [ main] o.apache.catalina.core.StandardService : Starting service Tomcat 2016-09-14 15:02:03.296 INFO 6 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat\/8.5.4 2016-09-14 15:02:03.424 INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[\/] : Initializing Spring embedded WebApplicationContext 2016-09-14 15:02:03.425 INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 2163 ms 2016-09-14 15:02:03.591 INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean : Mapping servlet: 'dispatcherServlet' to [\/] 2016-09-14 15:02:03.598 INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'characterEncodingFilter' to: [\/*] 2016-09-14 15:02:03.598 INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'hiddenHttpMethodFilter' to: [\/*] 2016-09-14 15:02:03.599 INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'httpPutFormContentFilter' to: [\/*] 2016-09-14 15:02:03.599 INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean : Mapping filter: 'requestContextFilter' to: [\/*] 2016-09-14 15:02:04.003 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5b8ed9b6: startup date [Wed Sep 14 15:02:01 UTC 2016]; root of context hierarchy 2016-09-14 15:02:04.087 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/events],methods=[GET]}\" onto public org.springframework.http.ResponseEntity<com.manthanhd.eventsmgmtapis.events.models.EventList> com.manthanhd.eventsmgmtapis.events.controllers.EventsController.getEvents() 2016-09-14 15:02:04.088 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/events\/{id}],methods=[GET]}\" onto public org.springframework.http.ResponseEntity<com.manthanhd.eventsmgmtapis.events.models.Event> com.manthanhd.eventsmgmtapis.events.controllers.EventsController.getSingleEvent(java.lang.String) 2016-09-14 15:02:04.088 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/events],methods=[POST]}\" onto public org.springframework.http.ResponseEntity<com.manthanhd.eventsmgmtapis.events.models.Event> com.manthanhd.eventsmgmtapis.events.controllers.EventsController.createNewEvent(com.manthanhd.eventsmgmtapis.events.models.Event) 2016-09-14 15:02:04.088 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/error],methods=[GET]}\" onto public org.springframework.http.ResponseEntity<java.lang.String> com.manthanhd.eventsmgmtapis.events.controllers.EventsController.error() 2016-09-14 15:02:04.091 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/error],produces=[text\/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse) 2016-09-14 15:02:04.091 INFO 6 --- [ main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/error]}\" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest) 2016-09-14 15:02:04.126 INFO 6 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [\/webjars\/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2016-09-14 15:02:04.126 INFO 6 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [\/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2016-09-14 15:02:04.171 INFO 6 --- [ main] o.s.w.s.handler.SimpleUrlHandlerMapping : Mapped URL path [\/**\/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 2016-09-14 15:02:04.396 INFO 6 --- [ main] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 2016-09-14 15:02:04.465 INFO 6 --- [ main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http) 2016-09-14 15:02:04.470 INFO 6 --- [ main] c.manthanhd.eventsmgmtapis.Application : Started Application in 4.07 seconds (JVM running for 4.811)\n\nHurrah! Your container is running. Before we open our champagne bottles, lets break down that command to better understand what we just did.\n\nThe base command is docker run which tells it to run something. All the parameters that follow tells it what to run.\n\nFirst one is the <span class=\"lang:default decode:true crayon-inline \">-p<\/span> parameter. Remember that <span class=\"lang:default decode:true crayon-inline\">EXPOSE 8080<\/span>? Well that was exposed on the container. However, if you want that port to be available on your host (similar to port forwarding), you\u2019d need this bit. In the <span class=\"lang:default decode:true crayon-inline\">8080:8080<\/span>, the first part (before the colon) is the port on the container and the second part (after the colon) is the port on your machine. This mapping is optional. If you don\u2019t provide this value, it won\u2019t bind the port to your local port so to access your container you\u2019ll have to specify your container IP address instead of just <span class=\"lang:default decode:true crayon-inline\">localhost<\/span>.\n\nNext is the name of the running container. This is entirely optional. If you don\u2019t specify one, it will generate a random name.\n\nAfter that is the <span class=\"lang:default decode:true crayon-inline \">\u2013rm<\/span> parameter. Again, this is optional as it tells Docker to remove the container after its stopped. By default, Docker will keep a container around so that you can inspect its logs to determine why it was shut down. For our test we just need it to run our container and then remove it when we don\u2019t need it.\n\nLast set of parameters, <span class=\"lang:default decode:true crayon-inline\">-i<\/span> and <span class=\"lang:default decode:true crayon-inline\">-t<\/span> (combined into <span class=\"lang:default decode:true crayon-inline\">-it<\/span>) tells Docker to run our container in interactive mode (<span class=\"lang:default decode:true crayon-inline\">-i<\/span>) with a psuedo TTY (<span class=\"lang:default decode:true crayon-inline\">-t<\/span>). We\u2019ve chosen interactive mode to make it easier for us to see the output and terminate the container as we can just press <span class=\"lang:default decode:true crayon-inline \">Ctrl + C<\/span> to stop it instead of running <span class=\"lang:default decode:true crayon-inline\">docker stop <containerId><\/span> command.\n\nLastly, we specify the image we want to create a container from. In this case its <span class=\"lang:default decode:true crayon-inline \">bootdemo2<\/span>. Since we haven\u2019t specified a tag, docker assumes its latest. If we do want to specify one, say, 1.0, it would be like <span class=\"lang:default decode:true crayon-inline\">bootdemo2:1.0<\/span>.\n\nTo check everything that Docker is currently running, you can run the following in a new tab.\n\ndocker ps -a\n\nYou should get something like:\n\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES be82e8618345 bootdemo2 \"\/bin\/sh -c 'java -Dj\" 33 seconds ago Up 31 seconds 0.0.0.0:8080->8080\/tcp myspringbootapp\n\nAlso, just quickly, if you get the following error:\n\nCannot connect to the Docker daemon. Is the docker daemon running on this host?\n\nJust update your host environment with the following command:\n\neval $(docker-machine env default)\n\nand then re-run the above <span class=\"lang:default decode:true crayon-inline \">docker ps -a<\/span> command.\n\nTo check whether or not the application is working fine, you could just navigate to your greeting endpoint (in my case <span class=\"lang:default decode:true crayon-inline\">http:\/\/localhost:8080\/events<\/span>) in your browser. This should work if you\u2019ve passed in the <span class=\"lang:default decode:true crayon-inline \">-p<\/span> parameter. However, if you are using <span class=\"lang:default decode:true crayon-inline \">boot2docker<\/span> or docker on a non-linux based operating system, you will have to make sure that your docker-machine has port forwarding setup for <span class=\"lang:default decode:true crayon-inline\">8080<\/span> (or whatever port you are trying to setup).\n\nAs you can see, we\u2019ve got one docker container running. Because we are running it in interactive mode, there are two ways of stopping it. You can stop is by pressing <span class=\"lang:default decode:true crayon-inline\">Ctrl + C<\/span> command in the window where its running interactively, but as you guessed it, this only works when its running in interactive mode. Normal way to stop your container is by using the docker stop command.\n\ndocker stop <container name \/ ID>\n\nYou can obtain the container name from whatever you named your container as, or if you didn\u2019t explicitly name it, you can obtain the ID from the <span class=\"lang:default decode:true crayon-inline\">docker ps<\/span> command.\n\nAlso, because we started it with <span class=\"lang:default decode:true crayon-inline \">\u2013rm<\/span> command, doing docker stop will remove the container as well. However, if we didn\u2019t have that flag in, you\u2019d have to run the below command after the stop command:\n\ndocker stop <container name \/ ID>\n\nOnce you\u2019ve stopped your container, you could try re-running it but this time without the <span class=\"lang:default decode:true crayon-inline\">\u2013rm<\/span> flag and see the difference when it comes to stopping your container.\n\nRelating back to our earlier analogy of problems with deploying different versions of a single application, using docker, since you would have a docker image for every version of your application, you\u2019d just swap out an old container with a new one. Even better than that, you could try out the any version of your application locally and since its a container, it would run in exactly the same way in production. If things don\u2019t work out, just remove that version of the container and re-deploy a version that works.\n\nSimple! I hope this post has given you a good introduction to Docker and the problem it solves for you. I\u2019d love to hear about your experiences with Docker. Drop a note down in the comments below or on my twitter account [@davemanthan](https:\/\/twitter.com\/davemanthan).\n\n\n","html":"<p style=\"text-align: justify;\">Deploying applications is a complex task. You have to create some VMs, be it on DigitalOcean or AWS, download and install necessary prerequesites and then deploy your application. It would be easy if it were to end there, however, it doesn&#8217;t.<\/p>\n<p style=\"text-align: justify;\">Following this you have application maintenance which includes deploying updated versions of your application in addition to other things like bug fixes. And this is where the real complexity starts. Your updated version might need another dependent application to be installed which in turn might need a version of some random tool to be upgraded. You did what was necessary but then you find out that the deployment of the updated application failed because you forgot to do that one null check in some corner of your application. So frantically you download the previous version of your application and try to restore it only to find that it doesn&#8217;t work anymore since you upgraded that random tool to support your newer application.<\/p>\n<p style=\"text-align: justify;\">While all this is happening either your entire application is unavailable because your application is single instance or if it is indeed multi-instance split by a loadbalancer, all the other instances are being stressed out because one of the node is down.<\/p>\n<p style=\"text-align: justify;\">And now you are thinking. Well, there has to be a better way.<\/p>\n<p style=\"text-align: justify;\">Well my friend, there is.<\/p>\n<p style=\"text-align: justify;\"><!--more--><\/p>\n<p style=\"text-align: justify;\"><em>*queue star wars intro soundtrack*<\/em><\/p>\n<p style=\"text-align: justify;\">Docker.<\/p>\n<p style=\"text-align: justify;\">Docker allows you to create and deploy application containers. What is an Application Container you ask? Well, you can think of an application container like a VM but unlike a fully fledged VM, its the bit that surrounds your application. So in essence, instead of it being a virtual machine, it is a virtual container for your application. This allows application containers to start up within seconds while a virtual machine could take minutes. Also, application containers take up much less RAM as they don&#8217;t have to load entire operating system in memory but only the bits that surround your application.<\/p>\n<p style=\"text-align: justify;\">So how can these amazing and fast application containers help simplify your deployments? Here&#8217;s how.<\/p>\n<p style=\"text-align: justify;\">Because creating and destroying application containers is a inexpensive process, both in terms of compute resources and time, it encourages a philosophy of disposable infrastructure. The idea is that instead of creating your infrastructure and taking care of it for its lifetime, you create it only when you need it and then destroy it when its not needed. Also, because all containers emerge from their respective Dockerfile(s), which is code, they can be versioned along side your actual application. This means that if you roll back to a previous version, you will deploy a docker container for that application using <em>that<\/em> version of the Dockerfile. Also, because the container contains your project and all of its dependencies, its self contained and can be stood up or torn down without any impact on any of the existing versions.<\/p>\n<p style=\"text-align: justify;\">We&#8217;ve been on about Dockerfile for a while now so before we go any further lets see what it actually looks like. Here&#8217;s an example:<\/p>\n<pre class=\"lang:default decode:true\">FROM ubuntu:14.04\n\nRUN apt-get update -y\nRUN apt-get install -y openjdk-7-jre-headless\n\nADD build\/libs\/*.jar \/app\n\nEXPOSE 8080\n\nENTRYPOINT java -jar \/app\/*.jar<\/pre>\n<p style=\"text-align: justify;\">That is a simple Dockerfile that deploys a Spring Boot application in a ubuntu based container. Briefly, the above Dockerfile defines a container image. The first line defines that our image is <span class=\"lang:default decode:true crayon-inline \">FROM<\/span> (based on) ubuntu 14.04 base image. Upon creating the image, it <span class=\"lang:default decode:true crayon-inline\">RUN<\/span>s <span class=\"lang:default decode:true crayon-inline \">apt-get update -y<\/span>\u00a0 and <span class=\"lang:default decode:true crayon-inline\">apt-get install -y openjdk-7-headless<\/span> commands. In addition to that, it adds (read copies) <span class=\"lang:default decode:true crayon-inline\">*.jar<\/span> file from the present working directory to <span class=\"lang:default decode:true crayon-inline \">\/app<\/span> folder to make it available in the container. Also, because we know that the application is going to run on port <span class=\"lang:default decode:true crayon-inline\">8080<\/span>, we&#8217;re <span class=\"lang:default decode:true crayon-inline\">EXPOSE<\/span>(ing) (defining) that port so that it can be accessed from outside the container. Finally, we&#8217;re defining the <span class=\"lang:default decode:true crayon-inline \">ENTRYPOINT<\/span> (command that docker will execute when the container is started) as a <span class=\"lang:default decode:true crayon-inline \">java -jar<\/span> command to execute our Spring Boot application.<\/p>\n<p style=\"text-align: justify;\">I&#8217;ll go into the details about all the sections that comprise a Dockerfile some other time but for now, the thing to take away is that a Dockerfile forms basis of your container and is used to define what your application container image looks like. The image can then be used to create and run the container.<\/p>\n<p style=\"text-align: justify;\">I am using a custom spring boot application, however, if you follow the <a href=\"https:\/\/spring.io\/guides\/gs\/rest-service\/\">REST tutorial <\/a>on Spring Boot Tutorials website, you should end up with the same project as me. Within your project, make sure that the Dockerfile is located at the root of the project (in same directory as the <span class=\"lang:default decode:true crayon-inline \">src<\/span> and <span class=\"lang:default decode:true crayon-inline\">build<\/span> folders).<\/p>\n<p style=\"text-align: justify;\">So once you have a Dockerfile, you can run the following command to create your image. Make sure that your Dockerfile is called exactly as Dockerfile and you are executing the following command from the same folder as the Dockerfile.<\/p>\n<pre class=\"lang:sh decode:true\">docker build -t bootdemo2 .<\/pre>\n<p style=\"text-align: justify;\">The above command builds our image and tags it (defined by <span class=\"lang:default decode:true crayon-inline \">-t<\/span> option) with <span class=\"lang:default decode:true crayon-inline\">bootdemo2<\/span> tag. When the version (tag) is unspecified, docker assumes <span class=\"lang:default decode:true crayon-inline\">latest<\/span>. However, if you do want to specify one, you just need to replace <span class=\"lang:default decode:true crayon-inline\">bootdemo2<\/span> with <span class=\"lang:default decode:true crayon-inline\">bootdemo2:1.0<\/span> or rather, more generically, <span class=\"lang:default decode:true crayon-inline\">bootdemo2:&lt;version&gt;<\/span>. So building the first version of the application would be like so:<\/p>\n<pre class=\"lang:sh decode:true\">docker build -t bootdemo2:1.0 .<\/pre>\n<p style=\"text-align: justify;\">And then subsequently, the next minor version would be like:<\/p>\n<pre class=\"lang:sh decode:true \">docker build -t bootdemo2:1.2 .<\/pre>\n<p style=\"text-align: justify;\">In this case, just leave the tag blank to allow it to default to latest.<\/p>\n<p style=\"text-align: justify;\">If everything went OK, you&#8217;ll be able to see your image listed within the list of docker images on your machine. This can be viewed using the following command:<\/p>\n<pre class=\"lang:sh decode:true \">docker images<\/pre>\n<p style=\"text-align: justify;\">You should see something like this in your output:<\/p>\n<pre class=\"lang:default decode:true \">REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\nbootdemo2               latest              4150e61520b2        27 hours ago        339.3 MB\n<\/pre>\n<p style=\"text-align: justify;\">Amazing! You&#8217;ve created your first Docker image. Hurrah! Lets run it to start a container off of that image. Simply run:<\/p>\n<pre class=\"lang:sh decode:true\">docker run -p 8080:8080 --name myspringbootapp --rm -it bootdemo2<\/pre>\n<p style=\"text-align: justify;\">You should see output like following:<\/p>\n<pre class=\"lang:default decode:true \">  .   ____          _            __ _ _\n \/\\\\ \/ ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\n( ( )\\___ | '_ | '_| | '_ \\\/ _` | \\ \\ \\ \\\n \\\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\n  '  |____| .__|_| |_|_| |_\\__, | \/ \/ \/ \/\n =========|_|==============|___\/=\/_\/_\/_\/\n :: Spring Boot ::        (v1.4.0.RELEASE)\n\n2016-09-14 15:02:01.125  INFO 6 --- [           main] c.manthanhd.eventsmgmtapis.Application   : Starting Application on 4c6454648246 with PID 6 (\/project\/build\/libs\/events-0.1.0.jar started by root in \/)\n2016-09-14 15:02:01.129  INFO 6 --- [           main] c.manthanhd.eventsmgmtapis.Application   : No active profile set, falling back to default profiles: default\n2016-09-14 15:02:01.265  INFO 6 --- [           main] ationConfigEmbeddedWebApplicationContext : Refreshing org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5b8ed9b6: startup date [Wed Sep 14 15:02:01 UTC 2016]; root of context hierarchy\n2016-09-14 15:02:03.278  INFO 6 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat initialized with port(s): 8080 (http)\n2016-09-14 15:02:03.294  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service Tomcat\n2016-09-14 15:02:03.296  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat\/8.5.4\n2016-09-14 15:02:03.424  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[\/]       : Initializing Spring embedded WebApplicationContext\n2016-09-14 15:02:03.425  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 2163 ms\n2016-09-14 15:02:03.591  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Mapping servlet: 'dispatcherServlet' to [\/]\n2016-09-14 15:02:03.598  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [\/*]\n2016-09-14 15:02:03.598  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [\/*]\n2016-09-14 15:02:03.599  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [\/*]\n2016-09-14 15:02:03.599  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [\/*]\n2016-09-14 15:02:04.003  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.context.embedded.AnnotationConfigEmbeddedWebApplicationContext@5b8ed9b6: startup date [Wed Sep 14 15:02:01 UTC 2016]; root of context hierarchy\n2016-09-14 15:02:04.087  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/events],methods=[GET]}\" onto public org.springframework.http.ResponseEntity&lt;com.manthanhd.eventsmgmtapis.events.models.EventList&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.getEvents()\n2016-09-14 15:02:04.088  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/events\/{id}],methods=[GET]}\" onto public org.springframework.http.ResponseEntity&lt;com.manthanhd.eventsmgmtapis.events.models.Event&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.getSingleEvent(java.lang.String)\n2016-09-14 15:02:04.088  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/events],methods=[POST]}\" onto public org.springframework.http.ResponseEntity&lt;com.manthanhd.eventsmgmtapis.events.models.Event&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.createNewEvent(com.manthanhd.eventsmgmtapis.events.models.Event)\n2016-09-14 15:02:04.088  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/error],methods=[GET]}\" onto public org.springframework.http.ResponseEntity&lt;java.lang.String&gt; com.manthanhd.eventsmgmtapis.events.controllers.EventsController.error()\n2016-09-14 15:02:04.091  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/error],produces=[text\/html]}\" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)\n2016-09-14 15:02:04.091  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped \"{[\/error]}\" onto public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.BasicErrorController.error(javax.servlet.http.HttpServletRequest)\n2016-09-14 15:02:04.126  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [\/webjars\/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2016-09-14 15:02:04.126  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [\/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2016-09-14 15:02:04.171  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [\/**\/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]\n2016-09-14 15:02:04.396  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup\n2016-09-14 15:02:04.465  INFO 6 --- [           main] s.b.c.e.t.TomcatEmbeddedServletContainer : Tomcat started on port(s): 8080 (http)\n2016-09-14 15:02:04.470  INFO 6 --- [           main] c.manthanhd.eventsmgmtapis.Application   : Started Application in 4.07 seconds (JVM running for 4.811)\n<\/pre>\n<p style=\"text-align: justify;\">Hurrah! Your container is running. Before we open our champagne bottles, lets break down that command to better understand what we just did.<\/p>\n<p style=\"text-align: justify;\">The base command is docker run which tells it to run something. All the parameters that follow tells it what to run.<\/p>\n<p style=\"text-align: justify;\">First one is the <span class=\"lang:default decode:true crayon-inline \">-p<\/span> parameter. Remember that <span class=\"lang:default decode:true crayon-inline\">EXPOSE 8080<\/span>? Well that was exposed on the container. However, if you want that port to be available on your host (similar to port forwarding), you&#8217;d need this bit. In the <span class=\"lang:default decode:true crayon-inline\">8080:8080<\/span>, the first part (before the colon) is the port on the container and the second part (after the colon) is the port on your machine. This mapping is optional. If you don&#8217;t provide this value, it won&#8217;t bind the port to your local port so to access your container you&#8217;ll have to specify your container IP address instead of just <span class=\"lang:default decode:true crayon-inline\">localhost<\/span>.<\/p>\n<p style=\"text-align: justify;\">Next is the name of the running container. This is entirely optional. If you don&#8217;t specify one, it will generate a random name.<\/p>\n<p style=\"text-align: justify;\">After that is the <span class=\"lang:default decode:true crayon-inline \">&#8211;rm<\/span> parameter. Again, this is optional as it tells Docker to remove the container after its stopped. By default, Docker will keep a container around so that you can inspect its logs to determine why it was shut down. For our test we just need it to run our container and then remove it when we don&#8217;t need it.<\/p>\n<p style=\"text-align: justify;\">Last set of parameters, <span class=\"lang:default decode:true crayon-inline\">-i<\/span> and <span class=\"lang:default decode:true crayon-inline\">-t<\/span> (combined into <span class=\"lang:default decode:true crayon-inline\">-it<\/span>) tells Docker to run our container in interactive mode (<span class=\"lang:default decode:true crayon-inline\">-i<\/span>) with a psuedo TTY (<span class=\"lang:default decode:true crayon-inline\">-t<\/span>). We&#8217;ve chosen interactive mode to make it easier for us to see the output and terminate the container as we can just press <span class=\"lang:default decode:true crayon-inline \">Ctrl + C<\/span> to stop it instead of running <span class=\"lang:default decode:true crayon-inline\">docker stop &lt;containerId&gt;<\/span> command.<\/p>\n<p style=\"text-align: justify;\">Lastly, we specify the image we want to create a container from. In this case its <span class=\"lang:default decode:true crayon-inline \">bootdemo2<\/span>. Since we haven&#8217;t specified a tag, docker assumes its latest. If we do want to specify one, say, 1.0, it would be like <span class=\"lang:default decode:true crayon-inline\">bootdemo2:1.0<\/span>.<\/p>\n<p style=\"text-align: justify;\">To check everything that Docker is currently running, you can run the following in a new tab.<\/p>\n<pre class=\"lang:sh decode:true\">docker ps -a<\/pre>\n<p style=\"text-align: justify;\">You should get something like:<\/p>\n<pre class=\"lang:default decode:true\">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                    NAMES\nbe82e8618345        bootdemo2           \"\/bin\/sh -c 'java -Dj\"   33 seconds ago      Up 31 seconds       0.0.0.0:8080-&gt;8080\/tcp   myspringbootapp<\/pre>\n<p style=\"text-align: justify;\">Also, just quickly, if you get the following error:<\/p>\n<pre class=\"lang:default decode:true\">Cannot connect to the Docker daemon. Is the docker daemon running on this host?<\/pre>\n<p style=\"text-align: justify;\">Just update your host environment with the following command:<\/p>\n<pre class=\"lang:sh decode:true\">eval $(docker-machine env default)<\/pre>\n<p style=\"text-align: justify;\">and then re-run the above <span class=\"lang:default decode:true crayon-inline \">docker ps -a<\/span> command.<\/p>\n<p style=\"text-align: justify;\">To check whether or not the application is working fine, you could just navigate to your greeting endpoint (in my case <span class=\"lang:default decode:true crayon-inline\">http:\/\/localhost:8080\/events<\/span>) in your browser. This should work if you&#8217;ve passed in the <span class=\"lang:default decode:true crayon-inline \">-p<\/span> parameter. However, if you are using <span class=\"lang:default decode:true crayon-inline \">boot2docker<\/span> or docker on a non-linux based operating system, you will have to make sure that your docker-machine has port forwarding setup for <span class=\"lang:default decode:true crayon-inline\">8080<\/span> (or whatever port you are trying to setup).<\/p>\n<p style=\"text-align: justify;\">As you can see, we&#8217;ve got one docker container running. Because we are running it in interactive mode, there are two ways of stopping it. You can stop is by pressing <span class=\"lang:default decode:true crayon-inline\">Ctrl + C<\/span> command in the window where its running interactively, but as you guessed it, this only works when its running in interactive mode. Normal way to stop your container is by using the docker stop command.<\/p>\n<pre class=\"lang:sh decode:true\">docker stop &lt;container name \/ ID&gt;<\/pre>\n<p style=\"text-align: justify;\">You can obtain the container name from whatever you named your container as, or if you didn&#8217;t explicitly name it, you can obtain the ID from the <span class=\"lang:default decode:true crayon-inline\">docker ps<\/span> command.<\/p>\n<p style=\"text-align: justify;\">Also, because we started it with <span class=\"lang:default decode:true crayon-inline \">&#8211;rm<\/span> command, doing docker stop will remove the container as well. However, if we didn&#8217;t have that flag in, you&#8217;d have to run the below command after the stop command:<\/p>\n<pre class=\"lang:sh decode:true \">docker stop &lt;container name \/ ID&gt;<\/pre>\n<p style=\"text-align: justify;\">Once you&#8217;ve stopped your container, you could try re-running it but this time without the <span class=\"lang:default decode:true crayon-inline\">&#8211;rm<\/span> flag and see the difference when it comes to stopping your container.<\/p>\n<p style=\"text-align: justify;\">Relating back to our earlier analogy of problems with deploying different versions of a single application, using docker, since you would have a docker image for every version of your application, you&#8217;d just swap out an old container with a new one. Even better than that, you could try out the any version of your application locally and since its a container, it would run in exactly the same way in production. If things don&#8217;t work out, just remove that version of the container and re-deploy a version that works.<\/p>\n<p style=\"text-align: justify;\">Simple! I hope this post has given you a good introduction to Docker and the problem it solves for you. I&#8217;d love to hear about your experiences with Docker. Drop a note down in the comments below or on my twitter account <a href=\"https:\/\/twitter.com\/davemanthan\">@davemanthan<\/a>.<\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2016\/09\/docker-banner.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 14 Sep 2016 20:49:45 +0000","created_by":1,"updated_at":"Wed, 14 Sep 2016 21:43:35 +0000","updated_by":1,"published_at":"Wed, 14 Sep 2016 20:49:45 +0000","published_by":1},{"id":432,"title":"Tomcat Valve Programming","slug":"temp-slug-91","markdown":"\nA Valve, in Tomcat terms is a piece of code that gets executed on every incoming request and outgoing response within Apache Tomcat. Valves get executed at much lower levels\n\n\n","html":"<p>A Valve, in Tomcat terms is a piece of code that gets executed on every incoming request and outgoing response within Apache Tomcat. Valves get executed at much lower levels<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 16 Sep 2016 13:57:18 +0000","created_by":1,"updated_at":"Fri, 16 Sep 2016 13:57:18 +0000","updated_by":1,"published_at":"","published_by":1},{"id":473,"title":"Jenkins pipeline build number groovy","slug":"temp-slug-92","markdown":"\nenv.BUILD_NUMBER  \n ensure sandbox is unchecked  \n https:\/\/groups.google.com\/forum\/#!topic\/jenkinsci-users\/rQe45k6Uu3E\n\n\n","html":"<p>env.BUILD_NUMBER<br \/>\nensure sandbox is unchecked<br \/>\nhttps:\/\/groups.google.com\/forum\/#!topic\/jenkinsci-users\/rQe45k6Uu3E<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 19 Sep 2016 11:39:55 +0000","created_by":1,"updated_at":"Mon, 19 Sep 2016 11:39:55 +0000","updated_by":1,"published_at":"","published_by":1},{"id":475,"title":"Interesting bit of C++ code","slug":"temp-slug-93","markdown":"\nFound this awesome bit of code:\n\n```\n#include\nmain(int t,int _,char *a)\n{\nreturn!0<t?t<3?main(-79,-13,a+main(-87,1-_,main(-86,0,a+1)+a)):\n1,t<_?main(t+1,_,a):3,main(-94,-27+t,a)&&t==2?_<13?\nmain(2,_+1,\"%s %d %d\\n\"):9:16:t<0?t<-72?main(_,t,\n\"@n'+,#'\/*{}w+\/w#cdnr\/+,{}r\/*de}+,\/*{*+,\/w{%+,\/w#q#n+,\/#{l+,\/n{n+,\/+#n+,\/#\\\n;#q#n+,\/+k#;*+,\/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \\\nq#'+d'K#!\/+k#;q#'r}eKK#}w'r}eKK{nl]'\/#;#q#n'){)#}w'){){nl]'\/+#n';d}rw' i;# \\\n){nl]!\/n{n#'; r{#w'r nc{nl]'\/#{l,+'K {rw' iK{;[{nl]'\/w#q#n'wk nw' \\\niwk{KK{nl]!\/w{%'l' i; :{nl]'\/*{q#'ld;r'}{nlwb!\/*de}'c \\\n;;{nl'-{}rw]'\/+,}##'*}#nc,',#nw]'\/+kd'+e}+;#'rdq#w! nr'\/ ') }+}{rl#'{n' ')# \\\n}'+}##(!!\/\")\n:t<-50?_==*a?putchar(32[a-1]):main(-65,_,a+1):main((*a=='\/')+t,_,a+1)\n:0<t?main(2,2,\"%s\"):*a=='\/'||main(0,main(-61,*a,\n\"!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\\nuwloca-O;m .vpbks,fxntdCeghiry\"),a+1);\n}\n```\n\n\n","html":"<p>Found this awesome bit of code:<\/p>\n<pre><code>#include\nmain(int t,int _,char *a)\n{\nreturn!0&lt;t?t&lt;3?main(-79,-13,a+main(-87,1-_,main(-86,0,a+1)+a)):\n1,t&lt;_?main(t+1,_,a):3,main(-94,-27+t,a)&amp;&amp;t==2?_&lt;13?\nmain(2,_+1,\"%s %d %d\\n\"):9:16:t&lt;0?t&lt;-72?main(_,t,\n\"@n'+,#'\/*{}w+\/w#cdnr\/+,{}r\/*de}+,\/*{*+,\/w{%+,\/w#q#n+,\/#{l+,\/n{n+,\/+#n+,\/#\\\n;#q#n+,\/+k#;*+,\/'r :'d*'3,}{w+K w'K:'+}e#';dq#'l \\\nq#'+d'K#!\/+k#;q#'r}eKK#}w'r}eKK{nl]'\/#;#q#n'){)#}w'){){nl]'\/+#n';d}rw' i;# \\\n){nl]!\/n{n#'; r{#w'r nc{nl]'\/#{l,+'K {rw' iK{;[{nl]'\/w#q#n'wk nw' \\\niwk{KK{nl]!\/w{%'l' i; :{nl]'\/*{q#'ld;r'}{nlwb!\/*de}'c \\\n;;{nl'-{}rw]'\/+,}##'*}#nc,',#nw]'\/+kd'+e}+;#'rdq#w! nr'\/ ') }+}{rl#'{n' ')# \\\n}'+}##(!!\/\")\n:t&lt;-50?_==*a?putchar(32[a-1]):main(-65,_,a+1):main((*a=='\/')+t,_,a+1)\n:0&lt;t?main(2,2,\"%s\"):*a=='\/'||main(0,main(-61,*a,\n\"!ek;dc i@bK'(q)-[w]*%n+r3#l,{}:\\nuwloca-O;m .vpbks,fxntdCeghiry\"),a+1);\n}\n<\/code><\/pre>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 22 Sep 2016 18:59:02 +0000","created_by":1,"updated_at":"Thu, 22 Sep 2016 18:59:02 +0000","updated_by":1,"published_at":"","published_by":1},{"id":465,"title":"AWS cloud formation describe* permission error","slug":"aws-cloud-formation-describe-permission-error","markdown":"\nSo you know sometimes, its difficult to work with AWS cloud formation scripts. On surface,\u00a0errors are seemingly random and unrelated to the script. This is what happened today. I wrote an awesome parameterised cloud formation script. I was quite proud of it, mainly because most of my parameters were\u00a0typed. This mean that the parameters like, ec2_security_groups had a type of <span class=\"lang:default decode:true crayon-inline\">List<AWS::EC2::SecurityGroup::Id><\/span>. This not only makes it easier to work with that cloud formation script from the AWS console, but also makes it very easy to work with from a CI\/CD pipeline as if those parameters are invalid, the script will fail instantly, instead of waiting for resources to deploy.\n\nHowever, while doing this, I completely missed the fact that in order for cloud formation to validate your input parameters, it needs to look them up first. What IAM policy permission does a resource lookup need? <span class=\"lang:default decode:true crayon-inline\">describe<\/span>!\n\nFor once, my IAM role had tightly restricted permissions and because of this I had to go on to expand them slightly to allow for describe permissions.\n\nI kept getting this error about the role not having the describe permission and I kept wondering why it needed that. Well now you know too!\n\n\n","html":"<p>So you know sometimes, its difficult to work with AWS cloud formation scripts. On surface,\u00a0errors are seemingly random and unrelated to the script. This is what happened today. I wrote an awesome parameterised cloud formation script. I was quite proud of it, mainly because most of my parameters were\u00a0typed. This mean that the parameters like, ec2_security_groups had a type of <span class=\"lang:default decode:true crayon-inline\">List&lt;AWS::EC2::SecurityGroup::Id&gt;<\/span>. This not only makes it easier to work with that cloud formation script from the AWS console, but also makes it very easy to work with from a CI\/CD pipeline as if those parameters are invalid, the script will fail instantly, instead of waiting for resources to deploy.<\/p>\n<p>However, while doing this, I completely missed the fact that in order for cloud formation to validate your input parameters, it needs to look them up first. What IAM policy permission does a resource lookup need? <span class=\"lang:default decode:true crayon-inline\">describe<\/span>!<\/p>\n<p>For once, my IAM role had tightly restricted permissions and because of this I had to go on to expand them slightly to allow for describe permissions.<\/p>\n<p>I kept getting this error about the role not having the describe permission and I kept wondering why it needed that. Well now you know too!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Sun, 16 Oct 2016 19:38:04 +0000","created_by":1,"updated_at":"Sun, 16 Oct 2016 19:39:17 +0000","updated_by":1,"published_at":"Sun, 16 Oct 2016 19:38:04 +0000","published_by":1},{"id":492,"title":"Why are my ExpectJS spies blocking calls?","slug":"why-are-my-expectjs-spies-blocking-calls","markdown":"\nWhile I love writing tests in JavaScript, it is sometimes incredibly painful to debug through the asynchronous test code. Today, a weirdness with ExpectJS happened. I had the following test code:\n\nvar contextStore = { put: function (id, context, callback) { return callback(undefined, context); }, get: function (id, callback) { return callback(undefined, undefined); } }; var contextStore_putSpy = expect.spyOn(contextStore, 'put'); var contextStore_getSpy = expect.spyOn(contextStore, 'get'); var bot = new Bot({contextStore: contextStore}); return bot.resolve(123, \"Hello. Hi\", function (err, messages) { if (err) return done(err); expect(contextStore_putSpy).toHaveBeenCalled(); expect(contextStore_getSpy).toHaveBeenCalled(); return done(); });\n\nSimple right? Nope. Some how my callback was not being called which was preventing the test from finishing which in turn caused a test failure. I spent good amount of time debugging step by step and finally found the issue.\n\nMy fake contextStore was not being called. Hmm. How can this happen? I\u2019ve defined functions that do respond asynchronously and have set spies on them as well. If anything, my spies should tell me that the function has been called!\n\nNothing could be further away from the truth! Well, you see, spies work differently in Java than in JavaScript. While using Mockito with Java, I\u2019ve found that Spies never block the calls on the object that they are spying on unless explicitly told to. However, this behaviour different in JavaScript. ExpectJS blocks the calls to spies unless told otherwise!\n\nAt the end, solution was simple. I changed lines 11 and 12 to:\n\nvar contextStore_putSpy = expect.spyOn(contextStore, 'put').andCallThrough(); var contextStore_getSpy = expect.spyOn(contextStore, 'get').andCallThrough();\n\nBoom! And it worked!\n\n\n","html":"<p>While I love writing tests in JavaScript, it is sometimes incredibly painful to debug through the asynchronous test code. Today, a weirdness with ExpectJS happened. I had the following test code:<\/p>\n<pre class=\"lang:js decode:true\">var contextStore = {\n    put: function (id, context, callback) {\n        return callback(undefined, context);\n    },\n\n    get: function (id, callback) {\n        return callback(undefined, undefined);\n    }\n};\n\nvar contextStore_putSpy = expect.spyOn(contextStore, 'put');\nvar contextStore_getSpy = expect.spyOn(contextStore, 'get');\n\nvar bot = new Bot({contextStore: contextStore});\n\nreturn bot.resolve(123, \"Hello. Hi\", function (err, messages) {\n    if (err) return done(err);\n\n    expect(contextStore_putSpy).toHaveBeenCalled();\n    expect(contextStore_getSpy).toHaveBeenCalled();\n    return done();\n});<\/pre>\n<p>Simple right? Nope. Some how my callback was not being called which was preventing the test from finishing which in turn caused a test failure. I spent good amount of time debugging step by step and finally found the issue.<\/p>\n<p>My fake contextStore was not being called. Hmm. How can this happen? I&#8217;ve defined functions that do respond asynchronously and have set spies on them as well. If anything, my spies should tell me that the function has been called!<\/p>\n<p>Nothing could be further away from the truth! Well, you see, spies work differently in Java than in JavaScript. While using Mockito with Java, I&#8217;ve found that Spies never block the calls on the object that they are spying on unless explicitly told to. However, this behaviour different in JavaScript. ExpectJS blocks the calls to spies unless told otherwise!<\/p>\n<p>At the end, solution was simple. I changed lines 11 and 12 to:<\/p>\n<pre class=\"lang:js decode:true \">var contextStore_putSpy = expect.spyOn(contextStore, 'put').andCallThrough();\nvar contextStore_getSpy = expect.spyOn(contextStore, 'get').andCallThrough();<\/pre>\n<p>Boom! And it worked!<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Wed, 19 Oct 2016 21:31:54 +0000","created_by":1,"updated_at":"Wed, 19 Oct 2016 21:31:54 +0000","updated_by":1,"published_at":"Wed, 19 Oct 2016 21:31:54 +0000","published_by":1},{"id":496,"title":"Getting started with chat bots using Talkify","slug":"getting-started-with-chat-bots-using-talkify","markdown":"\nChat bots have always seemed so complex. They process natural language from text so it must be hard right? After all, how can you make sense of loose words into computer instructions and then back? It must be hard.\n\nWell, it is hard. Kinda. A lot has already been solved around natural language processing so the amount you have to do to get started has reduced by significant amount. Tools are already there, you just have to use them.\n\nI used those tools and still found it to be difficult. I wanted to make the process of building chat bots as easy as getting started with web development. So, I built [Talkify](https:\/\/github.com\/manthanhd\/talkify).\n\nTakify is an Open Source framework for building chat bots. It is written in node.js and here\u2019s how you can build your very own chat bot in a couple of minutes.\n\nWe\u2019ll be building a chat bot that I like to call \u201csidekick\u201d. This is a simple bot that tells you knock knock and chuck norris jokes.\n\nThe template for this already exists on my [GitHub](https:\/\/github.com\/manthanhd\/talkify-example-sidekick) so lets just clone it for now. Assuming you have git command line installed, run:\n\ngit clone https:\/\/github.com\/manthanhd\/talkify-example-sidekick.git\n\nThis will create a folder called <span class=\"lang:default decode:true crayon-inline \">talkify-example-sidekick<\/span> within your current working directory. Go into that folder using the <span class=\"lang:sh decode:true crayon-inline \">cd<\/span> command.\n\nYou\u2019ll now need to install module dependencies. This should take a couple of seconds depending on your internet connection. Run the following command to tell npm to do this for you.\n\nnpm install\n\nNow if you list your current directory (using <span class=\"lang:sh decode:true crayon-inline\">ls<\/span> if using linux\/mac or <span class=\"lang:sh decode:true crayon-inline \">dir<\/span> for windows), you\u2019ll notice two JavaScript files. These are <span class=\"lang:default decode:true crayon-inline\">index.js<\/span> and <span class=\"lang:default decode:true crayon-inline\">cli.js<\/span>. The <span class=\"lang:default decode:true crayon-inline \">index.js<\/span> file contains the main bot code while the <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span>\u00a0 provides an interface to talk the bot via the command line interface. For this tutorial, lets run the <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span> file using node.\n\nnode cli.js\n\nThis should give you a prompt.\n\nC:\\Users\\manthanhd\\Documents\\projects\\sidekick>node cli.js BOT> Ready. YOU>\n\nGive it a try. Ask the bot to tell you a knock knock joke. Or ask it to tell you a Chuck Norris joke. You should get responses like so:\n\nYOU> tell me a knock knock joke BOT> Knock, knock. BOT> Who\u2019s there? BOT> Rufus. BOT> Rufus who? BOT> Rufus the most important part of your house. YOU> tell me a chuck norris joke BOT> There are only two things that can cut diamonds: other diamonds, and Chuck Norris.\n\nIf you keep asking for knock knock jokes, it will eventually run out. Try it!\n\nAwesome work! You\u2019ve setup the bot correctly!\n\nNow lets look under the hood inside the <span class=\"lang:default decode:true crayon-inline \">index.js<\/span> to better understand whats going on. First couple of lines in this file are a bunch of requires.\n\nconst knockKnockJokes = require('knock-knock-jokes'); const cnApi = require('chuck-norris-api'); const talkify = require('talkify');\n\nThis is like an \u201cimport\u201d statement where its initialising libraries like the <span class=\"lang:default decode:true crayon-inline\">knock-knock-jokes<\/span>, <span class=\"lang:default decode:true crayon-inline\">chuck-norris-api<\/span> and <span class=\"lang:default decode:true crayon-inline\">talkify<\/span>.\n\nThe next couple of lines are importing template functions from Talkify for us to use later on in the code.\n\nconst Bot = talkify.Bot; const BotTypes = talkify.BotTypes; const SingleLineMessage = BotTypes.SingleLineMessage; const TrainingDocument = BotTypes.TrainingDocument; const Skill = BotTypes.Skill;\n\nIf the previous require statements were loading libraries, this is equivalent to loading specific things that we will be needing from those libraries.\n\nIn the next line we are initialising the bot itself using the previously loaded Bot function.\n\nconst bot = new Bot();\n\nThe constructor for Bot can accept an optional configuration object but to keep things simple, we\u2019re just going to use default configuration.\n\nIt might look like a lot is happening in the following lines but in essence, we\u2019re simply training the bot to learn a couple of words and associate them with topics. To do that, we are using the <span class=\"lang:default decode:true crayon-inline \">trainAll<\/span> function of the bot and are passing in an array of objects as well as a callback function. Each of these objects are of <span class=\"lang:default decode:true crayon-inline\">TrainingDocument<\/span> type.\n\nbot.trainAll([ new TrainingDocument('knock_joke', 'knock'), new TrainingDocument('knock_joke', 'knock knock'), new TrainingDocument('chuck_norris_joke', 'chuck norris'), new TrainingDocument('chuck_norris_joke', 'chuck'), new TrainingDocument('chuck_norris_joke', 'norris'), new TrainingDocument('chuck_norris_joke', 'chuck norris joke'), ], function () { console.log(' BOT> Ready.'); });\n\nAs you can see, each <span class=\"lang:default decode:true crayon-inline\">TrainingDocument<\/span> object accepts two parameters. The first parameter is the topic name and the second is text that the bot will learn to associate with the previously mentioned topic.\n\nThe callback function that is passed to the <span class=\"lang:default decode:true crayon-inline \">trainAll<\/span> function is executed when the training for the bot has finished. Here, we\u2019re just printing out a message indicating that the bot is ready.\n\nNext, we\u2019re defining two skill objects by utilising the <span class=\"lang:default decode:true crayon-inline \">Skill<\/span> constructor.\n\nconst kJokeSkill = new Skill('my_knock_knock_joke_skill', 'knock_joke', function (context, request, response) { if (!context.kJokes) { context.kJokes = []; } let newJoke = knockKnockJokes(); let counter = 0; while(counter < 11 && context.kJokes.indexOf(newJoke) !== -1) { newJoke = knockKnockJokes(); counter++; } if(counter === 11) { return response.send(new SingleLineMessage('Sorry I am out of knock knock jokes. :(')); } context.kJokes.push(newJoke); return response.send(new SingleLineMessage(newJoke)); }); const cJokeSkill = new Skill('my_chuck_norris_joke_skill', 'chuck_norris_joke', function(context, request, response) { return cnApi.getRandom().then(function(data) { return response.send(new SingleLineMessage(data.value.joke)); }); });\n\nAs you can see, this accepts three parameters, unique name of the skill, topic to associate the skill with and a function to execute as part of that skill.\n\nThis function is called an <span class=\"lang:default decode:true crayon-inline \">apply<\/span> function and it must accept at least three parameters. These are context, request and response.\n\nThe context parameter is the context that the bot loads for you automatically. This is request driven context and is associated with an end user.\u00a0 Think of this as how a web session works.\n\nThe request parameter contains metadata about the request like the actual raw sentence in the request, topic resolution confidence level etc.\n\nThe response parameter contains a bunch of methods that you can use to set states and respond to the user. More information on this entire section is available in the [Skills wiki page](https:\/\/github.com\/manthanhd\/talkify\/blob\/master\/wiki\/SKILLS.md).\n\nWithin the skill functions we are using the knock knock joke and the chuck norris library to get jokes that the bot can respond with.\n\nNext two lines is where we are making the bot aware of the skills by adding them to it.\n\nbot.addSkill(kJokeSkill); bot.addSkill(cJokeSkill); module.exports = bot;\n\nThe last line exports the bot so that it can be used by other files within this project. We are doing this so that we can separate the bot logic from the logic of getting the raw request from the user and then responding with it. The latter logic in this case can be found from <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span> file.\n\nThis means that you can quite easily replace the <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span> file with a web hook from Facebook messenger, Apple iMessage, Skype or even a web request from an iPhone app that works with Siri to enable voice.\n\nSo that\u2019s it. That was a quick whistle stop guide to writing your own first chat bot. Hope you enjoyed it.\n\nThe project is available on my GitHub page: [https:\/\/github.com\/manthanhd\/talkify](https:\/\/github.com\/manthanhd\/talkify)\n\nIf you want to request a feature or file a bug, please feel free to raise an issue on the GitHub repository link above. I am also open to contributors so feel free to fork the repository and contribute.\n\nJust for reference, here\u2019s the NPM package link: [https:\/\/www.npmjs.com\/package\/talkify](https:\/\/www.npmjs.com\/package\/talkify)\n\n\n","html":"<p>Chat bots have always seemed so complex. They process natural language from text so it must be hard right? After all, how can you make sense of loose words into computer instructions and then back? It must be hard.<\/p>\n<p>Well, it is hard. Kinda. A lot has already been solved around natural language processing so the amount you have to do to get started has reduced by significant amount. Tools are already there, you just have to use them.<\/p>\n<p>I used those tools and still found it to be difficult. I wanted to make the process of building chat bots as easy as getting started with web development. So, I built <a href=\"https:\/\/github.com\/manthanhd\/talkify\">Talkify<\/a>.<\/p>\n<p>Takify is an Open Source framework for building chat bots. It is written in node.js and here&#8217;s how you can build your very own chat bot in a couple of minutes.<\/p>\n<p>We&#8217;ll be building a chat bot that I like to call &#8220;sidekick&#8221;. This is a simple bot that tells you knock knock and chuck norris jokes.<!--more--><\/p>\n<p>The template for this already exists on my <a href=\"https:\/\/github.com\/manthanhd\/talkify-example-sidekick\">GitHub<\/a> so lets just clone it for now. Assuming you have git command line installed, run:<\/p>\n<pre class=\"lang:sh decode:true\">git clone https:\/\/github.com\/manthanhd\/talkify-example-sidekick.git<\/pre>\n<p>This will create a folder called <span class=\"lang:default decode:true crayon-inline \">talkify-example-sidekick<\/span> within your current working directory. Go into that folder using the <span class=\"lang:sh decode:true crayon-inline \">cd<\/span> command.<\/p>\n<p>You&#8217;ll now need to install module dependencies. This should take a couple of seconds depending on your internet connection. Run the following command to tell npm to do this for you.<\/p>\n<pre class=\"lang:sh decode:true\">npm install<\/pre>\n<p>Now if you list your current directory (using <span class=\"lang:sh decode:true crayon-inline\">ls<\/span> if using linux\/mac or <span class=\"lang:sh decode:true crayon-inline \">dir<\/span> for windows), you&#8217;ll notice two JavaScript files. These are <span class=\"lang:default decode:true crayon-inline\">index.js<\/span> and <span class=\"lang:default decode:true crayon-inline\">cli.js<\/span>. The <span class=\"lang:default decode:true crayon-inline \">index.js<\/span> file contains the main bot code while the <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span>\u00a0 provides an interface to talk the bot via the command line interface. For this tutorial, lets run the <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span> file using node.<\/p>\n<pre class=\"lang:sh decode:true\">node cli.js<\/pre>\n<p>This should give you a prompt.<\/p>\n<pre class=\"lang:sh decode:true\">C:\\Users\\manthanhd\\Documents\\projects\\sidekick&gt;node cli.js\n BOT&gt; Ready.\nYOU&gt;<\/pre>\n<p>Give it a try. Ask the bot to tell you a knock knock joke. Or ask it to tell you a Chuck Norris joke. You should get responses like so:<\/p>\n<pre class=\"lang:sh decode:true\">YOU&gt; tell me a knock knock joke\n BOT&gt;  Knock, knock.\n BOT&gt;  Who\u2019s there?\n BOT&gt;  Rufus.\n BOT&gt;  Rufus who?\n BOT&gt;  Rufus the most important part of your house.\nYOU&gt; tell me a chuck norris joke\n BOT&gt;  There are only two things that can cut diamonds: other diamonds, and Chuck Norris.<\/pre>\n<p>If you keep asking for knock knock jokes, it will eventually run out. Try it!<\/p>\n<p>Awesome work! You&#8217;ve setup the bot correctly!<\/p>\n<p>Now lets look under the hood inside the <span class=\"lang:default decode:true crayon-inline \">index.js<\/span> to better understand whats going on. First couple of lines in this file are a bunch of requires.<\/p>\n<pre class=\"lang:js decode:true\">const knockKnockJokes = require('knock-knock-jokes');\nconst cnApi = require('chuck-norris-api');\n\nconst talkify = require('talkify');<\/pre>\n<p>This is like an &#8220;import&#8221; statement where its initialising libraries like the <span class=\"lang:default decode:true crayon-inline\">knock-knock-jokes<\/span>, <span class=\"lang:default decode:true crayon-inline\">chuck-norris-api<\/span> and <span class=\"lang:default decode:true crayon-inline\">talkify<\/span>.<\/p>\n<p>The next couple of lines are importing template functions from Talkify for us to use later on in the code.<\/p>\n<pre class=\"lang:js decode:true \">const Bot = talkify.Bot;\n\nconst BotTypes = talkify.BotTypes;\n\nconst SingleLineMessage = BotTypes.SingleLineMessage;\n\nconst TrainingDocument = BotTypes.TrainingDocument;\nconst Skill = BotTypes.Skill;<\/pre>\n<p>If the previous require statements were loading libraries, this is equivalent to loading specific things that we will be needing from those libraries.<\/p>\n<p>In the next line we are initialising the bot itself using the previously loaded Bot function.<\/p>\n<pre class=\"lang:js decode:true \">const bot = new Bot();<\/pre>\n<p>The constructor for Bot can accept an optional configuration object but to keep things simple, we&#8217;re just going to use default configuration.<\/p>\n<p>It might look like a lot is happening in the following lines but in essence, we&#8217;re simply training the bot to learn a couple of words and associate them with topics. To do that, we are using the <span class=\"lang:default decode:true crayon-inline \">trainAll<\/span> function of the bot and are passing in an array of objects as well as a callback function. Each of these objects are of <span class=\"lang:default decode:true crayon-inline\">TrainingDocument<\/span> type.<\/p>\n<pre class=\"lang:js decode:true \">bot.trainAll([\n    new TrainingDocument('knock_joke', 'knock'),\n    new TrainingDocument('knock_joke', 'knock knock'),\n\n    new TrainingDocument('chuck_norris_joke', 'chuck norris'),\n    new TrainingDocument('chuck_norris_joke', 'chuck'),\n    new TrainingDocument('chuck_norris_joke', 'norris'),\n    new TrainingDocument('chuck_norris_joke', 'chuck norris joke'),\n], function () {\n    console.log(' BOT&gt; Ready.');\n});<\/pre>\n<p>As you can see, each <span class=\"lang:default decode:true crayon-inline\">TrainingDocument<\/span> object accepts two parameters. The first parameter is the topic name and the second is text that the bot will learn to associate with the previously mentioned topic.<\/p>\n<p>The callback function that is passed to the <span class=\"lang:default decode:true crayon-inline \">trainAll<\/span> function is executed when the training for the bot has finished. Here, we&#8217;re just printing out a message indicating that the bot is ready.<\/p>\n<p>Next, we&#8217;re defining two skill objects by utilising the <span class=\"lang:default decode:true crayon-inline \">Skill<\/span> constructor.<\/p>\n<pre class=\"lang:js decode:true\">const kJokeSkill = new Skill('my_knock_knock_joke_skill', 'knock_joke', function (context, request, response) {\n    if (!context.kJokes) {\n        context.kJokes = [];\n    }\n\n    let newJoke = knockKnockJokes();\n    let counter = 0;\n    while(counter &lt; 11 &amp;&amp; context.kJokes.indexOf(newJoke) !== -1) {\n        newJoke = knockKnockJokes();\n        counter++;\n    }\n\n    if(counter === 11) {\n        return response.send(new SingleLineMessage('Sorry I am out of knock knock jokes. :('));\n    }\n\n    context.kJokes.push(newJoke);\n    return response.send(new SingleLineMessage(newJoke));\n});\n\nconst cJokeSkill = new Skill('my_chuck_norris_joke_skill', 'chuck_norris_joke', function(context, request, response) {\n    return cnApi.getRandom().then(function(data) {\n        return response.send(new SingleLineMessage(data.value.joke));\n    });\n});<\/pre>\n<p>As you can see, this accepts three parameters, unique name of the skill, topic to associate the skill with and a function to execute as part of that skill.<\/p>\n<p>This function is called an <span class=\"lang:default decode:true crayon-inline \">apply<\/span> function and it must accept at least three parameters. These are context, request and response.<\/p>\n<p>The context parameter is the context that the bot loads for you automatically. This is request driven context and is associated with an end user.\u00a0 Think of this as how a web session works.<\/p>\n<p>The request parameter contains metadata about the request like the actual raw sentence in the request, topic resolution confidence level etc.<\/p>\n<p>The response parameter contains a bunch of methods that you can use to set states and respond to the user. More information on this entire section is available in the <a href=\"https:\/\/github.com\/manthanhd\/talkify\/blob\/master\/wiki\/SKILLS.md\">Skills wiki page<\/a>.<\/p>\n<p>Within the skill functions we are using the knock knock joke and the chuck norris library to get jokes that the bot can respond with.<\/p>\n<p>Next two lines is where we are making the bot aware of the skills by adding them to it.<\/p>\n<pre class=\"lang:js decode:true\">bot.addSkill(kJokeSkill);\nbot.addSkill(cJokeSkill);\n\nmodule.exports = bot;<\/pre>\n<p>The last line exports the bot so that it can be used by other files within this project. We are doing this so that we can separate the bot logic from the logic of getting the raw request from the user and then responding with it. The latter logic in this case can be found from <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span> file.<\/p>\n<p>This means that you can quite easily replace the <span class=\"lang:default decode:true crayon-inline \">cli.js<\/span> file with a web hook from Facebook messenger, Apple iMessage, Skype or even a web request from an iPhone app that works with Siri to enable voice.<\/p>\n<p>So that&#8217;s it. That was a quick whistle stop guide to writing your own first chat bot. Hope you enjoyed it.<\/p>\n<p>The project is available on my GitHub page: <a href=\"https:\/\/github.com\/manthanhd\/talkify\">https:\/\/github.com\/manthanhd\/talkify<\/a><\/p>\n<p>If you want to request a feature or file a bug, please feel free to raise an issue on the GitHub repository link above. I am also open to contributors so feel free to fork the repository and contribute.<\/p>\n<p>Just for reference, here&#8217;s the NPM package link: <a href=\"https:\/\/www.npmjs.com\/package\/talkify\">https:\/\/www.npmjs.com\/package\/talkify<\/a><\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2016\/11\/chat_bot-01.jpg","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Fri, 11 Nov 2016 12:27:46 +0000","created_by":1,"updated_at":"Fri, 11 Nov 2016 12:28:41 +0000","updated_by":1,"published_at":"Fri, 11 Nov 2016 12:27:46 +0000","published_by":1},{"id":509,"title":"Gradle multi module projects","slug":"gradle-multi-module-projects","markdown":"\nRecently I\u2019ve had to deal with a lot of gradle projects, specifically the multi-module ones. It\u2019s dead easy to setup, however, every time I do it, I have to refer to one of my old projects to see the layout. So, hopefully, while this document will certainly help future me, I hope it is of help to you too.\n\nFirst things first, create a project directory. We will refer this directory as \u2018project root directory\u2019 in future. For all the awesome things that I do, we\u2019ll call this one, ***drum roll please* \u2026** awesomeproject.\n\nmkdir awesomeproject cd awesomeproject\n\nOnce in that directory, lets create the <span class=\"lang:sh decode:true crayon-inline \">build.gradle<\/span>\u00a0file at the project root with the following content:\n\n\/\/ build.gradle apply plugin: 'distribution'\n\nAlongside this, create a <span class=\"lang:sh decode:true crayon-inline \">settings.gradle<\/span>\u00a0file but leave it blank for now.\n\ntouch settings.gradle\n\nNow create your sub-module project directories. In this example, we\u2019ll create two directories, awesomeproject-scripts which will hold our deployment scripts and awesomeproject-service which will contain our actual java code.\n\nmkdir awesomeproject-scripts awesomeproject-service\n\nTime to create gradle files for your sub-modules. We won\u2019t go into the detail of what the gradle files will actually contain, however, for semi-completeness, we\u2019ll just apply some plugins as a marker to indicate the type of sub-module projects.\n\n\/\/ awesomeproject-scripts\/build.gradle apply plugin: 'distribution'\n\n\/\/ awesomeproject-service\/build.gradle apply plugin: 'java'\n\nAs you can see from above, our <span class=\"lang:sh decode:true crayon-inline \">awesomeproject-scripts\/build.gradle<\/span>\u00a0file applies the distribution plugin to indicate that it will need to archive its contents while the <span class=\"lang:sh decode:true crayon-inline \">awesomeproject-service\/build.gradle<\/span>\u00a0applies the java plugin to indicate that it will be compiling its java source.\n\nAmazing! Now we just need to tell our root project to include these two sub-modules. This is simple. You know we left that <span class=\"lang:sh decode:true crayon-inline \">settings.gradle<\/span>\u00a0file in the project root blank? Well, lets change that.\n\n\/\/ settings.gradle include 'awesomeproject-scripts', 'awesomeproject-service'\n\nHere, we are telling the root gradle project to include the sub-modules. Gradle will then expect <span class=\"lang:sh decode:true crayon-inline\">build.gradle<\/span>\u00a0files within these directories and will execute jobs. It\u2019s pretty clever so if you\u2019ve got a module that needs to execute one of the other sub-modules first, it will run it in the correct required order.\n\nMore [detailed documentation](https:\/\/docs.gradle.org\/current\/userguide\/multi_project_builds.html) on this is available on gradle\u2019s website, however, this should get you up and running quickly.\n\n\n","html":"<p>Recently I&#8217;ve had to deal with a lot of gradle projects, specifically the multi-module ones. It&#8217;s dead easy to setup, however, every time I do it, I have to refer to one of my old projects to see the layout. So, hopefully, while this document will certainly help future me, I hope it is of help to you too.<\/p>\n<p>First things first, create a project directory. We will refer this directory as &#8216;project root directory&#8217; in future. For all the awesome things that I do, we&#8217;ll call this one, <strong><em>drum roll please<\/em> &#8230;<\/strong> awesomeproject.<\/p>\n<p><!--more--><\/p>\n<pre class=\"lang:sh decode:true\">mkdir awesomeproject\ncd awesomeproject<\/pre>\n<p>Once in that directory, lets create the <span class=\"lang:sh decode:true crayon-inline \">build.gradle<\/span>\u00a0file at the project root with the following content:<\/p>\n<pre class=\"lang:default decode:true\" title=\"build.gradle\">\/\/ build.gradle\napply plugin: 'distribution'<\/pre>\n<p>Alongside this, create a <span class=\"lang:sh decode:true crayon-inline \">settings.gradle<\/span>\u00a0file but leave it blank for now.<\/p>\n<pre class=\"lang:sh decode:true\">touch settings.gradle\n<\/pre>\n<p>Now create your sub-module project directories. In this example, we&#8217;ll create two directories, awesomeproject-scripts which will hold our deployment scripts and awesomeproject-service which will contain our actual java code.<\/p>\n<pre class=\"lang:sh decode:true\">mkdir awesomeproject-scripts awesomeproject-service<\/pre>\n<p>Time to create gradle files for your sub-modules. We won&#8217;t go into the detail of what the gradle files will actually contain, however, for semi-completeness, we&#8217;ll just apply some plugins as a marker to indicate the type of sub-module projects.<\/p>\n<pre class=\"lang:default decode:true\" title=\"awesomeproject-scripts\/build.gradle\">\/\/ awesomeproject-scripts\/build.gradle\napply plugin: 'distribution'<\/pre>\n<pre class=\"lang:default decode:true\" title=\"awesomeproject-service\/build.gradle\">\/\/ awesomeproject-service\/build.gradle\napply plugin: 'java'<\/pre>\n<p>As you can see from above, our <span class=\"lang:sh decode:true crayon-inline \">awesomeproject-scripts\/build.gradle<\/span>\u00a0file applies the distribution plugin to indicate that it will need to archive its contents while the <span class=\"lang:sh decode:true crayon-inline \">awesomeproject-service\/build.gradle<\/span>\u00a0applies the java plugin to indicate that it will be compiling its java source.<\/p>\n<p>Amazing! Now we just need to tell our root project to include these two sub-modules. This is simple. You know we left that <span class=\"lang:sh decode:true crayon-inline \">settings.gradle<\/span>\u00a0file in the project root blank? Well, lets change that.<\/p>\n<pre class=\"lang:default decode:true\" title=\"settings.gradle\">\/\/ settings.gradle\ninclude 'awesomeproject-scripts', 'awesomeproject-service'<\/pre>\n<p>Here, we are telling the root gradle project to include the sub-modules. Gradle will then expect <span class=\"lang:sh decode:true crayon-inline\">build.gradle<\/span>\u00a0files within these directories and will execute jobs. It&#8217;s pretty clever so if you&#8217;ve got a module that needs to execute one of the other sub-modules first, it will run it in the correct required order.<\/p>\n<p>More <a href=\"https:\/\/docs.gradle.org\/current\/userguide\/multi_project_builds.html\">detailed documentation<\/a> on this is available on gradle&#8217;s website, however, this should get you up and running quickly.<\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/Updated_logo_for_Gradle.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 05 Dec 2016 12:41:37 +0000","created_by":1,"updated_at":"Mon, 05 Dec 2016 12:44:21 +0000","updated_by":1,"published_at":"Mon, 05 Dec 2016 12:41:37 +0000","published_by":1},{"id":517,"title":"Blue-green deployments on AWS","slug":"blue-green-deployments-on-aws","markdown":"\nBefore we do this, make sure that you have a service that you want to deploy. To keep things simple, I followed the spring boot tutorial on making a restful web service. It was quick and the app worked like a charm. As usual, I went a bit extra and made my app return a stubbed list of users. You don\u2019t have to. Make sure you have a <span class=\"lang:default decode:true crayon-inline\">\/healthcheck<\/span>\u00a0endpoint and another endpoint that you can test with. In my case, I have <span class=\"lang:default decode:true crayon-inline \">\/users<\/span>\u00a0which returns a list of users.\n\nAll righty then. Lets get a high level overview of what things are and how they are going to work. But before we do that, lets go through a quick real-ish life scenario.\n\nSay you have a service that you have deployed onto AWS. Now you have a newer version of that service that you\u2019d like to test. Since you never know if something works without actually trying it out, normally, after exhaustive testing in staging and other environments, you\u2019d deploy that service into production to all your users. But ah ha! That one guy in your team forgot that one test case which made it blow up which means every single user of yours is now seeing error pages everywhere. This is bad so you roll it back to the previous version. Doesn\u2019t sound too bad yet but by the time you do this, you\u2019d have lost a couple of hours in time which would translate into actual money lost to the company which could eventually make\u00a0a dent in your end of the year bonus.\n\nBut what if there was a better way. What if you could\u2019ve deployed that change to only one of your hundred servers. This would mean that if it does blow up, it would\u2019ve only affected one percent of your users (or rather more accurately one percent of servers). This better way is the blue-green deployments.\n\nBlue-green deployments (a.k.a A\/B deployments) are used quite often as part of software development lifecycle to test out a certain piece of functionality in a live production environment for a select group of users. Basically what I said but in a fancier language.\n\n[![blue green deployments image representation](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-deployments.png?fit=349%2C328&ssl=1)](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-deployments.png?ssl=1)\n\nIn order to do this, we need to break down the AWS infrastructure our service depends on. In most cases, the following resources are used:\n\n- Elastic Load Balancing (ELB)\n- Autoscaling Groups (ASG)\n- Launch Configuration\n\nNote that I have not explicitly called out Elastic Compute (EC2) instances in the above list of resources. This is mainly because we are implicitly using them, via the launch configuration but are not explicitly creating a resource of that type. Also, at this level, blue-green does not have anything to do with databases which is why we\u2019ll leave it out this time.\n\nLike most people, you would have a single cloud formation script with all the above resources in it. Whilst this is great, to do blue-green deployments, we\u2019ll need to break it down. You could go as fine as one cloud formation script per resource, however, I have them down into two scripts; first for ELB and second for ASG as well as launch configuration. I tried breaking them down into three scripts but something about that didn\u2019t quite feel right as I felt that ASG and launch configuration are quite tightly coupled as is.\n\nThe way this is going to work is that we\u2019ll deploy an ELB using the ELB cloud formation scripts across two availability zones. In my case, this is eu-west-1a and eu-west-1b. This ELB will have a unique LoadBalancerName attribute. In our case, since we are only doing single region, this will be \u00a0something like UserServiceLB.\n\nOnce that is deployed, we will then deploy the blue service stack using the second cloud formation script which will deploy our launch configuration as well as a auto scaling group. This auto scaling group will need to be attached to a load balancer. Yep, you guessed it. We\u2019ll specify the previously defined unique LoadBalancerName here. So, say for instance UserServiceV1 (blue), this will look like following:\n\n[![aws blue green deployment image representation](https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment.png?fit=509%2C319&ssl=1)](https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment.png?ssl=1)\n\nIf, say now we want to deploy V2 (green) of the service, we\u2019ll have to deploy another stack with different launch configuration but still have the autoscaling group pointing at the same ELB. This might look like following:\n\n[![cf-deployment-2](https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2-700x294.png?fit=700%2C294&ssl=1)](https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2.png?ssl=1)\n\nNow the ELB will send some of the traffic to the new green (color not indicative of health) instances. Because this is only some of the traffic, if something does go wrong, it is only impacted to a small subset of customers. Also, if it all goes horribly wrong (murphy\u2019s law not intended), we can simply delete the green stack and it will all happily keep ticking.\n\nIf the new green stack is proving its worth, we can make changes to the autoscaling group to increase the number of instances while at the same time making changes to the blue stack to decrease the number of its instances. As you can see, all this can quite easily be automated.\n\nNow lets get on with the how. These are the scripts that I used.\n\nelb.json:\n\n{ \"Parameters\": { \"LoadBalancerName\": { \"Type\": \"String\", \"Description\": \"Name of this loadbalancer. This is the name that you must specify when attaching an autoscaling group.\" } }, \"Resources\": { \"UserServiceELB\": { \"Type\" : \"AWS::ElasticLoadBalancing::LoadBalancer\", \"Properties\": { \"LoadBalancerName\": {\"Ref\": \"LoadBalancerName\"}, \"SecurityGroups\": [\"sg-xxxxxxx\"], \"AvailabilityZones\": [\"eu-west-1a\", \"eu-west-1b\"], \"Listeners\" : [ { \"LoadBalancerPort\" : \"80\", \"InstancePort\" : \"8080\", \"Protocol\" : \"HTTP\" } ], \"HealthCheck\" : { \"Target\" : \"HTTP:8080\/healthcheck\", \"HealthyThreshold\" : \"3\", \"UnhealthyThreshold\" : \"5\", \"Interval\" : \"30\", \"Timeout\" : \"5\" }, \"Tags\": [ {\"Key\": \"Name\", \"Value\": {\"Ref\": \"LoadBalancerName\"}} ] } } } }\n\nThis is the one that actually deploys the load balancer. Make sure that you replace the security groups with one of yours or else the script won\u2019t work.\n\nSo lets quickly examine what this script is doing.\u00a0This one deploys a single resource of type LoadBalancer. This is a AWS Classic Load balancer. Why not V2? Well, this is what we need for now. I think we might need V2 when we do containerised deployments. I\u2019ll cover that some another time.\n\nListeners section indicates that the load balancer is listening on port 80 while forwarding requests to port 8080 on instances. If your sample app is listening on a different port, other than 8080, you will have to change the value for InstancePort attribute.\n\nThe HealthCheck section lets the loadbalancer know what to health check on. In this case, it is calling \/healthcheck endpoint on my app. This will let it know whether or not the instance is listening correctly. The URL doesn\u2019t need to be \/healthcheck, it can be anything as long as it returns 200.\n\nIf you upload this cloud formation to the AWS web console, you will be prompted to enter a value for the LoadBalancerName parameter (as defined in the Parameters section). Whatever value you enter, make a note of it.\n\nNext, we\u2019ll deploy the app itself.\n\napp.json:\n\n{ \"Parameters\": { \"ASGMinSize\": { \"Type\": \"Number\", \"Default\": \"1\", \"Description\": \"Minimum number of instances required for this app's autoscaling group\" }, \"ASGMaxSize\": { \"Type\": \"Number\", \"Default\": \"1\", \"Description\": \"Maximum number of instances required for this app's autoscaling group\" }, \"LoadBalancerName\": { \"Type\": \"String\", \"Description\": \"Name of the loadbalancer to attach this app's autoscaling group to\" } }, \"Resources\": { \"UserServiceScalingGroup\": { \"Type\" : \"AWS::AutoScaling::AutoScalingGroup\", \"Properties\" : { \"AvailabilityZones\": [\"eu-west-1a\", \"eu-west-1b\"], \"LaunchConfigurationName\" : { \"Ref\" : \"UserServiceLaunchConfig\" }, \"MinSize\" : {\"Ref\": \"ASGMinSize\"}, \"MaxSize\" : {\"Ref\": \"ASGMaxSize\"}, \"LoadBalancerNames\" : [ {\"Ref\": \"LoadBalancerName\"} ] } }, \"UserServiceLaunchConfig\": { \"Type\" : \"AWS::AutoScaling::LaunchConfiguration\", \"Properties\": { \"KeyName\" : \"aws-keypair\", \"ImageId\": \"ami-9398d3e0\", \"SecurityGroups\": [\"sg-xxxxxxx\"], \"InstanceType\": \"t2.nano\", \"IamInstanceProfile\": \"arn:aws:iam::999987541517:instance-profile\/UserServiceRoles-UserServiceInstanceProfile-4D8TE56SDVZM\", \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [\"\", [ \"#!\/bin\/bash -xe\\n\", \"yum update -y\\n\", \"yum remove -y java-1.7.0-openjdk\\n\", \"yum install -y java-1.8.0-openjdk-headless.x86_64\\n\", \"aws s3 cp s3:\/\/manthanhd-buildartifacts\/services\/userservice\/1.0.0\/user-service-1.0.0.jar .\\n\", \"\/bin\/env java -jar user-service-1.0.0.jar\\n\" ]]}} } } } }\n\nThis will deploy the launch configuration as well as the autoscaling group. Again, make sure you replace the SecurityGroups attribute with value of your own or else the script won\u2019t work. Also, note that the UserData is configured to deploy a java application because, as I mentioned in the beginning, I am deploying a Spring Boot service. If you are using something else other than a java based service, you might have to change it to suit your needs.\n\nLet\u2019s quickly go through that cloud formation script. As you can see, we are deploying one autoscaling group and one launch configuration. The auto scaling group defines the min size, max size, name of the load balancer as well as the name of the launch configuration. The first three parameters are referenced from the parameters section whereas the launch configuration is a live reference from the launch configuration resource below. Also, note that we are deploying the autoscaling group into two availability zones within eu-west region, same as the load balancer.\n\nThe launch configuration is rather simple. It basically instucts AWS as to how to launch the instance. The imageId attribute is the default AWS AMI. I\u2019ve made the KeyName generic as in aws-keypair. It doesn\u2019t need to have a value, however, it is useful when you need to debug something.\n\nThe launch configuration also attaches instance profile to the launched EC2 instance. In my case, this is a restricted instance profile that only allows the EC2 instance to get and list objects in a specific folder within my S3 bucket.\n\nIf you refer back to the architecture diagram, you\u2019ll notice that we are deploying the launch configuration and autoscaling group separately to the load balancer. During the deployment of the autoscaling group, we are specifying the load balancer to attach the autoscaling group to. This is where you need to use the LoadBalancerName attribute that you previously noted down.\n\nOnce you have roles and security groups in place with updated scripts, go ahead and deploy the elb.json as well as app.json. For app.json, make sure you suffix the stack name with V1 so that we know that this is the first deployment.\n\nThis will create three instances under the elb. Now deploy the app.json again but this time with only one instance. Lets pretend that this is a new version. Make sure you suffix the stack name with V2. Once deployed, you will see that your new instance has been attached to the same ELB. Really cool right? Now, you could just update that existing V1 stack. Go back to app.json and change the instance count to 2 and update the V1 stack. Now you should see AWS shrink that autoscaling group to 2 from 3 whilst V2 is still 1. You get the idea.\n\nAll this, can of course be quite easily automated using the AWS\u2019s CLI and APIs.\n\nHope this has helped drive your blue green deployments on AWS. I have been piecing together notes for a while and finally managed to drive a PoC out. You can find the code here:\n\n[https:\/\/github.com\/manthanhd\/aws-blue-green-java](https:\/\/github.com\/manthanhd\/aws-blue-green-java)\n\n\n","html":"<p style=\"text-align: justify;\">Before we do this, make sure that you have a service that you want to deploy. To keep things simple, I followed the spring boot tutorial on making a restful web service. It was quick and the app worked like a charm. As usual, I went a bit extra and made my app return a stubbed list of users. You don&#8217;t have to. Make sure you have a <span class=\"lang:default decode:true crayon-inline\">\/healthcheck<\/span>\u00a0endpoint and another endpoint that you can test with. In my case, I have <span class=\"lang:default decode:true crayon-inline \">\/users<\/span>\u00a0which returns a list of users.<\/p>\n<p style=\"text-align: justify;\">All righty then. Lets get a high level overview of what things are and how they are going to work. But before we do that, lets go through a quick real-ish life scenario.<\/p>\n<p style=\"text-align: justify;\">Say you have a service that you have deployed onto AWS. Now you have a newer version of that service that you&#8217;d like to test. Since you never know if something works without actually trying it out, normally, after exhaustive testing in staging and other environments, you&#8217;d deploy that service into production to all your users. But ah ha! That one guy in your team forgot that one test case which made it blow up which means every single user of yours is now seeing error pages everywhere. This is bad so you roll it back to the previous version. Doesn&#8217;t sound too bad yet but by the time you do this, you&#8217;d have lost a couple of hours in time which would translate into actual money lost to the company which could eventually make\u00a0a dent in your end of the year bonus.<\/p>\n<p style=\"text-align: justify;\"><!--more--><\/p>\n<p style=\"text-align: justify;\">But what if there was a better way. What if you could&#8217;ve deployed that change to only one of your hundred servers. This would mean that if it does blow up, it would&#8217;ve only affected one percent of your users (or rather more accurately one percent of servers). This better way is the blue-green deployments.<\/p>\n<p style=\"text-align: justify;\">Blue-green deployments (a.k.a A\/B deployments) are used quite often as part of software development lifecycle to test out a certain piece of functionality in a live production environment for a select group of users. Basically what I said but in a fancier language.<\/p>\n<p style=\"text-align: justify;\"><a href=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-deployments.png?ssl=1\"><img class=\"aligncenter size-full wp-image-518\" src=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-deployments.png?fit=349%2C328&#038;ssl=1\" alt=\"blue green deployments image representation\" srcset=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-deployments.png?w=349&amp;ssl=1 349w, https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-deployments.png?resize=300%2C282&amp;ssl=1 300w\" sizes=\"(max-width: 349px) 100vw, 349px\" data-recalc-dims=\"1\" \/><\/a><\/p>\n<p style=\"text-align: justify;\">In order to do this, we need to break down the AWS infrastructure our service depends on. In most cases, the following resources are used:<\/p>\n<ul style=\"text-align: justify;\">\n<li>Elastic Load Balancing (ELB)<\/li>\n<li>Autoscaling Groups (ASG)<\/li>\n<li>Launch Configuration<\/li>\n<\/ul>\n<p style=\"text-align: justify;\">Note that I have not explicitly called out Elastic Compute (EC2) instances in the above list of resources. This is mainly because we are implicitly using them, via the launch configuration but are not explicitly creating a resource of that type. Also, at this level, blue-green does not have anything to do with databases which is why we&#8217;ll leave it out this time.<\/p>\n<p style=\"text-align: justify;\">Like most people, you would have a single cloud formation script with all the above resources in it. Whilst this is great, to do blue-green deployments, we&#8217;ll need to break it down. You could go as fine as one cloud formation script per resource, however, I have them down into two scripts; first for ELB and second for ASG as well as launch configuration. I tried breaking them down into three scripts but something about that didn&#8217;t quite feel right as I felt that ASG and launch configuration are quite tightly coupled as is.<\/p>\n<p style=\"text-align: justify;\">The way this is going to work is that we&#8217;ll deploy an ELB using the ELB cloud formation scripts across two availability zones. In my case, this is eu-west-1a and eu-west-1b. This ELB will have a unique LoadBalancerName attribute. In our case, since we are only doing single region, this will be \u00a0something like UserServiceLB.<\/p>\n<p style=\"text-align: justify;\">Once that is deployed, we will then deploy the blue service stack using the second cloud formation script which will deploy our launch configuration as well as a auto scaling group. This auto scaling group will need to be attached to a load balancer. Yep, you guessed it. We&#8217;ll specify the previously defined unique LoadBalancerName here. So, say for instance UserServiceV1 (blue), this will look like following:<\/p>\n<p style=\"text-align: justify;\"><a href=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment.png?ssl=1\"><img class=\"aligncenter size-full wp-image-519\" src=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment.png?fit=509%2C319&#038;ssl=1\" alt=\"aws blue green deployment image representation\" srcset=\"https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment.png?w=509&amp;ssl=1 509w, https:\/\/i0.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment.png?resize=300%2C188&amp;ssl=1 300w\" sizes=\"(max-width: 509px) 100vw, 509px\" data-recalc-dims=\"1\" \/><\/a><\/p>\n<p style=\"text-align: justify;\">If, say now we want to deploy V2 (green) of the service, we&#8217;ll have to deploy another stack with different launch configuration but still have the autoscaling group pointing at the same ELB. This might look like following:<\/p>\n<p style=\"text-align: justify;\"><a href=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2.png?ssl=1\"><img class=\"aligncenter size-large wp-image-521\" src=\"https:\/\/i1.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2-700x294.png?fit=700%2C294&#038;ssl=1\" alt=\"cf-deployment-2\" srcset=\"https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2.png?resize=700%2C294&amp;ssl=1 700w, https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2.png?resize=300%2C126&amp;ssl=1 300w, https:\/\/i2.wp.com\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/cf-deployment-2.png?w=759&amp;ssl=1 759w\" sizes=\"(max-width: 700px) 100vw, 700px\" data-recalc-dims=\"1\" \/><\/a><\/p>\n<p style=\"text-align: justify;\">Now the ELB will send some of the traffic to the new green (color not indicative of health) instances. Because this is only some of the traffic, if something does go wrong, it is only impacted to a small subset of customers. Also, if it all goes horribly wrong (murphy&#8217;s law not intended), we can simply delete the green stack and it will all happily keep ticking.<\/p>\n<p style=\"text-align: justify;\">If the new green stack is proving its worth, we can make changes to the autoscaling group to increase the number of instances while at the same time making changes to the blue stack to decrease the number of its instances. As you can see, all this can quite easily be automated.<\/p>\n<p style=\"text-align: justify;\">Now lets get on with the how. These are the scripts that I used.<\/p>\n<p style=\"text-align: justify;\">elb.json:<\/p>\n<pre class=\"lang:js decode:true\" title=\"elb.json\">{\n  \"Parameters\": {\n    \"LoadBalancerName\": {\n      \"Type\": \"String\",\n      \"Description\": \"Name of this loadbalancer. This is the name that you must specify when attaching an autoscaling group.\"\n    }\n  },\n  \"Resources\": {\n    \"UserServiceELB\": {\n      \"Type\" : \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"LoadBalancerName\": {\"Ref\": \"LoadBalancerName\"},\n        \"SecurityGroups\": [\"sg-xxxxxxx\"],\n        \"AvailabilityZones\": [\"eu-west-1a\", \"eu-west-1b\"],\n        \"Listeners\" : [ {\n          \"LoadBalancerPort\" : \"80\",\n          \"InstancePort\" : \"8080\",\n          \"Protocol\" : \"HTTP\"\n        } ],\n        \"HealthCheck\" : {\n          \"Target\" : \"HTTP:8080\/healthcheck\",\n          \"HealthyThreshold\" : \"3\",\n          \"UnhealthyThreshold\" : \"5\",\n          \"Interval\" : \"30\",\n          \"Timeout\" : \"5\"\n        },\n        \"Tags\": [\n          {\"Key\": \"Name\", \"Value\": {\"Ref\": \"LoadBalancerName\"}}\n        ]\n      }\n    }\n  }\n}<\/pre>\n<p style=\"text-align: justify;\">This is the one that actually deploys the load balancer. Make sure that you replace the security groups with one of yours or else the script won&#8217;t work.<\/p>\n<p style=\"text-align: justify;\">So lets quickly examine what this script is doing.\u00a0This one deploys a single resource of type LoadBalancer. This is a AWS Classic Load balancer. Why not V2? Well, this is what we need for now. I think we might need V2 when we do containerised deployments. I&#8217;ll cover that some another time.<\/p>\n<p style=\"text-align: justify;\">Listeners section indicates that the load balancer is listening on port 80 while forwarding requests to port 8080 on instances. If your sample app is listening on a different port, other than 8080, you will have to change the value for InstancePort attribute.<\/p>\n<p style=\"text-align: justify;\">The HealthCheck section lets the loadbalancer know what to health check on. In this case, it is calling \/healthcheck endpoint on my app. This will let it know whether or not the instance is listening correctly. The URL doesn&#8217;t need to be \/healthcheck, it can be anything as long as it returns 200.<\/p>\n<p style=\"text-align: justify;\">If you upload this cloud formation to the AWS web console, you will be prompted to enter a value for the LoadBalancerName parameter (as defined in the Parameters section). Whatever value you enter, make a note of it.<\/p>\n<p style=\"text-align: justify;\">Next, we&#8217;ll deploy the app itself.<\/p>\n<p style=\"text-align: justify;\">app.json:<\/p>\n<pre class=\"lang:js decode:true\" title=\"app.json\">{\n  \"Parameters\": {\n    \"ASGMinSize\": {\n      \"Type\": \"Number\",\n      \"Default\": \"1\",\n      \"Description\": \"Minimum number of instances required for this app's autoscaling group\"\n    },\n    \"ASGMaxSize\": {\n      \"Type\": \"Number\",\n      \"Default\": \"1\",\n      \"Description\": \"Maximum number of instances required for this app's autoscaling group\"\n    },\n    \"LoadBalancerName\": {\n      \"Type\": \"String\",\n      \"Description\": \"Name of the loadbalancer to attach this app's autoscaling group to\"\n    }\n  },\n  \"Resources\": {\n    \"UserServiceScalingGroup\": {\n      \"Type\" : \"AWS::AutoScaling::AutoScalingGroup\",\n      \"Properties\" : {\n        \"AvailabilityZones\": [\"eu-west-1a\", \"eu-west-1b\"],\n        \"LaunchConfigurationName\" : { \"Ref\" : \"UserServiceLaunchConfig\" },\n        \"MinSize\" : {\"Ref\": \"ASGMinSize\"},\n        \"MaxSize\" : {\"Ref\": \"ASGMaxSize\"},\n        \"LoadBalancerNames\" : [ {\"Ref\": \"LoadBalancerName\"} ]\n      }\n    },\n    \"UserServiceLaunchConfig\": {\n      \"Type\" : \"AWS::AutoScaling::LaunchConfiguration\",\n      \"Properties\": {\n        \"KeyName\" : \"aws-keypair\",\n        \"ImageId\": \"ami-9398d3e0\",\n        \"SecurityGroups\": [\"sg-xxxxxxx\"],\n        \"InstanceType\": \"t2.nano\",\n        \"IamInstanceProfile\": \"arn:aws:iam::999987541517:instance-profile\/UserServiceRoles-UserServiceInstanceProfile-4D8TE56SDVZM\",\n        \"UserData\"       : { \"Fn::Base64\" : { \"Fn::Join\" : [\"\", [\n          \"#!\/bin\/bash -xe\\n\",\n          \"yum update -y\\n\",\n          \"yum remove -y java-1.7.0-openjdk\\n\",\n          \"yum install -y java-1.8.0-openjdk-headless.x86_64\\n\",\n          \"aws s3 cp s3:\/\/manthanhd-buildartifacts\/services\/userservice\/1.0.0\/user-service-1.0.0.jar .\\n\",\n          \"\/bin\/env java -jar user-service-1.0.0.jar\\n\"\n        ]]}}\n      }\n    }\n  }\n}<\/pre>\n<p style=\"text-align: justify;\">This will deploy the launch configuration as well as the autoscaling group. Again, make sure you replace the SecurityGroups attribute with value of your own or else the script won&#8217;t work. Also, note that the UserData is configured to deploy a java application because, as I mentioned in the beginning, I am deploying a Spring Boot service. If you are using something else other than a java based service, you might have to change it to suit your needs.<\/p>\n<p style=\"text-align: justify;\">Let&#8217;s quickly go through that cloud formation script. As you can see, we are deploying one autoscaling group and one launch configuration. The auto scaling group defines the min size, max size, name of the load balancer as well as the name of the launch configuration. The first three parameters are referenced from the parameters section whereas the launch configuration is a live reference from the launch configuration resource below. Also, note that we are deploying the autoscaling group into two availability zones within eu-west region, same as the load balancer.<\/p>\n<p style=\"text-align: justify;\">The launch configuration is rather simple. It basically instucts AWS as to how to launch the instance. The imageId attribute is the default AWS AMI. I&#8217;ve made the KeyName generic as in aws-keypair. It doesn&#8217;t need to have a value, however, it is useful when you need to debug something.<\/p>\n<p style=\"text-align: justify;\">The launch configuration also attaches instance profile to the launched EC2 instance. In my case, this is a restricted instance profile that only allows the EC2 instance to get and list objects in a specific folder within my S3 bucket.<\/p>\n<p style=\"text-align: justify;\">If you refer back to the architecture diagram, you&#8217;ll notice that we are deploying the launch configuration and autoscaling group separately to the load balancer. During the deployment of the autoscaling group, we are specifying the load balancer to attach the autoscaling group to. This is where you need to use the LoadBalancerName attribute that you previously noted down.<\/p>\n<p style=\"text-align: justify;\">Once you have roles and security groups in place with updated scripts, go ahead and deploy the elb.json as well as app.json. For app.json, make sure you suffix the stack name with V1 so that we know that this is the first deployment.<\/p>\n<p style=\"text-align: justify;\">This will create three instances under the elb. Now deploy the app.json again but this time with only one instance. Lets pretend that this is a new version. Make sure you suffix the stack name with V2. Once deployed, you will see that your new instance has been attached to the same ELB. Really cool right? Now, you could just update that existing V1 stack. Go back to app.json and change the instance count to 2 and update the V1 stack. Now you should see AWS shrink that autoscaling group to 2 from 3 whilst V2 is still 1. You get the idea.<\/p>\n<p style=\"text-align: justify;\">All this, can of course be quite easily automated using the AWS&#8217;s CLI and APIs.<\/p>\n<p style=\"text-align: justify;\">Hope this has helped drive your blue green deployments on AWS. I have been piecing together notes for a while and finally managed to drive a PoC out. You can find the code here:<\/p>\n<p style=\"text-align: justify;\"><a href=\"https:\/\/github.com\/manthanhd\/aws-blue-green-java\">https:\/\/github.com\/manthanhd\/aws-blue-green-java<\/a><\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2016\/12\/blue-green-banner.png","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 06 Dec 2016 10:53:47 +0000","created_by":1,"updated_at":"Tue, 06 Dec 2016 10:53:47 +0000","updated_by":1,"published_at":"Tue, 06 Dec 2016 10:53:47 +0000","published_by":1},{"id":528,"title":"Anatomy of a chat bot","slug":"temp-slug-99","markdown":"\nA basic chat bot consists of four components:\n\n1. Channel Interface\n2. Context\u00a0Resolution\n3. Classification\n4. Skill\u00a0Resolution\n\nLets go through them one by one.\n\n\n## Channel Interface\n\nSimply put, a channel interface is a point of interface used to interact with the bot. Since chat bots use text as an input, this could be anything from a simple command line interface to something more complex like Facebook messenger, slack, skype etc.\n\nA channel interface must be able to provide two things. Input message in text as well as a correspondence ID. The input message, without any doubt is the textual natural language input entered by the user. The correspondence ID on the other hand is the ID of the entire interaction that is happening between the chat bot and the user. Think of this like a user identifier but instead of being linked to the entirely of the user, is only linked to the correspondence that the user is having with the chat bot.\n\nThis is not a session identifier either! As sessions can be created and destroyed. A correspondence is something that outlives a user session as for the bot to functional correctly, it must remember all interactions that it has had with the user since the first conversation. This, of course is subject to change as your use case might require the bot to forget all previous interactions when the user closes the window, in which case, the correspondence ID becomes the same as a session ID.\n\nSo to summarise, a channel interface is a point of interaction that allows a user to interface with the chat bot. This interface, must supply two things: textual input and correspondence ID for that input.\n\n\n## Context\u00a0Resolution\n\nNow this is where things start to get interesting. Being aware of the context when responding is one of the better traits of a good chat bot. There are two types of contexts:\n\n1. Explicit\n2. Implicit\n\nExplicit context is where context is given within a supplied query. Lets look at the following statement:\n\n> What is the balance on my personal credit card?\n\nThe above statement provides explicit context containing four attributes, namely, object: credit card, name: personal, attribute: balance.\n\nExplicit context helps you establish context from a sentence. Once such a context has been set, the user shouldn\u2019t need to specify the same context again and again. This is where the sweet sweet implicit context comes in.\n\nImplicit context is context that has been set from previous interactions with the user. In other terms, when context is missing in a given query, it is derived from previous interactions. Say for instance the user has already said the above statement. That sets the context as follows:\n\n{ \u00a0 \u00a0 \"object\": \"creditcard\", \u00a0 \u00a0 \"name\": \"personal\", \u00a0 \u00a0 \"attribute\": \"balance\" }\n\nNow if the user says the following:\n\n> And remind me my limit on that?\n\nWith explicit context, the bot has no way of knowing what the customer is talking about except the attribute being limt. This is where the implicit context kicks in. From the previous conversation, it assumes that the conversation is still about the credit card, specifically the personal one. Using the explicit context derivation, it knows that the attribute this time is \u201climit\u201d. Viola! It should respond with the limit on your personal credit card without you specifying it explicitly!\n\n\n## Classification\n\nYou\u2019ve been given a piece of text. How do you know what topic it is for? How do you classify it into a topic? This is where a classifier comes in. A classifier takes in an input, analyses it and classifies it into a topic. Sounds like magic right? Well, it kinda is.\n\nMost classifiers have two kinds of input. Training and query. Training, as the name suggests is training data. It must comprise of two things. Query and a topic that the query classifies to. Consider the following example:\n\n> Tell me about an apple.\n\nYou and I\u00a0know that the sentence above is about an apple. Thus, here the topic is apple. When you train a classifier, you\u2019ll have to provide the above sentence (or merely \u2018about apple\u2019 as query should suffice most classifiers) as well as the topic that we know as \u2018apple\u2019. Obviously one is not enough so you\u2019ll have to make up as much training data as you can. These are known values.\n\nThe second kind of input is the query itself. This is where you genuinely don\u2019t know the topic for a given sentence and are asking the classifier to do the classification for you.\n\nThe most basic type of classification is logistic regression where for every line of text in training data set for a given topic, you extract features out of it, and then plot it on a graph. You then derive a line of regression for every topic. Whenever a given text input matches closest to one of the lines of regression, you return the topic for that regression line\u00a0as \u201cclassified\u201d topic. All the classifiers that I have seen so far provide two values when queried with a piece of text. First is the topic that the given piece of text has been classified to. Second is the level of confidence the classifier has in that classification. This normally ranges between 0 and 1 in decimals.\n\n\n## Skill Resolution\n\nUntil now, we\u2019ve accepted input from a channel, resolved the context and classified the text into a topic. The next thing to do for the bot is to take action on it. This is where skill resolution comes in. However, before we go into skill resolution, lets briefly cover some details about the skill itself.\n\nA skill is a piece of logic that can be executed in order to generate a response. This response can be a textual or in some cases (like Facebook messenger) html. Typically, a skill, when executed, must generate a response. In JavaScript terms, a skill is a simple object with an executable function. In Java, it could be an object that is an instance of a class implementing some Skill interface. Either way, there should be an explicit contract between the skill and the bot that states the callable logic that can be executed.\n\nThere are different levels of complexities associated with skill resolution. This could be something like \u201cWhat happens when no skill is found for a given topic?\u201d, \u201cHow do I map skills according to the confidence in topic provided by the classifier?\u201d, \u201cWhat happens when a skill doesn\u2019t respond at all?\u201d, \u201cHow do we manage requested responses from skills?\u201d etc.\n\n\n","html":"<p>A basic chat bot consists of four components:<\/p>\n<ol>\n<li>Channel Interface<\/li>\n<li>Context\u00a0Resolution<\/li>\n<li>Classification<\/li>\n<li>Skill\u00a0Resolution<\/li>\n<\/ol>\n<p>Lets go through them one by one.<\/p>\n<h2>Channel Interface<\/h2>\n<p>Simply put, a channel interface is a point of interface used to interact with the bot. Since chat bots use text as an input, this could be anything from a simple command line interface to something more complex like Facebook messenger, slack, skype etc.<\/p>\n<p>A channel interface must be able to provide two things. Input message in text as well as a correspondence ID. The input message, without any doubt is the textual natural language input entered by the user. The correspondence ID on the other hand is the ID of the entire interaction that is happening between the chat bot and the user. Think of this like a user identifier but instead of being linked to the entirely of the user, is only linked to the correspondence that the user is having with the chat bot.<\/p>\n<p>This is not a session identifier either! As sessions can be created and destroyed. A correspondence is something that outlives a user session as for the bot to functional correctly, it must remember all interactions that it has had with the user since the first conversation. This, of course is subject to change as your use case might require the bot to forget all previous interactions when the user closes the window, in which case, the correspondence ID becomes the same as a session ID.<\/p>\n<p>So to summarise, a channel interface is a point of interaction that allows a user to interface with the chat bot. This interface, must supply two things: textual input and correspondence ID for that input.<\/p>\n<h2>Context\u00a0Resolution<\/h2>\n<p>Now this is where things start to get interesting. Being aware of the context when responding is one of the better traits of a good chat bot. There are two types of contexts:<\/p>\n<ol>\n<li>Explicit<\/li>\n<li>Implicit<\/li>\n<\/ol>\n<p>Explicit context is where context is given within a supplied query. Lets look at the following statement:<\/p>\n<blockquote><p>What is the balance on my personal credit card?<\/p><\/blockquote>\n<p>The above statement provides explicit context containing four attributes, namely, object: credit card, name: personal, attribute: balance.<\/p>\n<p>Explicit context helps you establish context from a sentence. Once such a context has been set, the user shouldn&#8217;t need to specify the same context again and again. This is where the sweet sweet implicit context comes in.<\/p>\n<p>Implicit context is context that has been set from previous interactions with the user. In other terms, when context is missing in a given query, it is derived from previous interactions. Say for instance the user has already said the above statement. That sets the context as follows:<\/p>\n<pre class=\"lang:default decode:true\">{\n\u00a0 \u00a0 \"object\": \"creditcard\",\n\u00a0 \u00a0 \"name\": \"personal\",\n\u00a0 \u00a0 \"attribute\": \"balance\"\n}<\/pre>\n<p>Now if the user says the following:<\/p>\n<blockquote><p>And remind me my limit on that?<\/p><\/blockquote>\n<p>With explicit context, the bot has no way of knowing what the customer is talking about except the attribute being limt. This is where the implicit context kicks in. From the previous conversation, it assumes that the conversation is still about the credit card, specifically the personal one. Using the explicit context derivation, it knows that the attribute this time is &#8220;limit&#8221;. Viola! It should respond with the limit on your personal credit card without you specifying it explicitly!<\/p>\n<h2>Classification<\/h2>\n<p>You&#8217;ve been given a piece of text. How do you know what topic it is for? How do you classify it into a topic? This is where a classifier comes in. A classifier takes in an input, analyses it and classifies it into a topic. Sounds like magic right? Well, it kinda is.<\/p>\n<p>Most classifiers have two kinds of input. Training and query. Training, as the name suggests is training data. It must comprise of two things. Query and a topic that the query classifies to. Consider the following example:<\/p>\n<blockquote><p>Tell me about an apple.<\/p><\/blockquote>\n<p>You and I\u00a0know that the sentence above is about an apple. Thus, here the topic is apple. When you train a classifier, you&#8217;ll have to provide the above sentence (or merely &#8216;about apple&#8217; as query should suffice most classifiers) as well as the topic that we know as &#8216;apple&#8217;. Obviously one is not enough so you&#8217;ll have to make up as much training data as you can. These are known values.<\/p>\n<p>The second kind of input is the query itself. This is where you genuinely don&#8217;t know the topic for a given sentence and are asking the classifier to do the classification for you.<\/p>\n<p>The most basic type of classification is logistic regression where for every line of text in training data set for a given topic, you extract features out of it, and then plot it on a graph. You then derive a line of regression for every topic. Whenever a given text input matches closest to one of the lines of regression, you return the topic for that regression line\u00a0as &#8220;classified&#8221; topic. All the classifiers that I have seen so far provide two values when queried with a piece of text. First is the topic that the given piece of text has been classified to. Second is the level of confidence the classifier has in that classification. This normally ranges between 0 and 1 in decimals.<\/p>\n<h2>Skill Resolution<\/h2>\n<p>Until now, we&#8217;ve accepted input from a channel, resolved the context and classified the text into a topic. The next thing to do for the bot is to take action on it. This is where skill resolution comes in. However, before we go into skill resolution, lets briefly cover some details about the skill itself.<\/p>\n<p>A skill is a piece of logic that can be executed in order to generate a response. This response can be a textual or in some cases (like Facebook messenger) html. Typically, a skill, when executed, must generate a response. In JavaScript terms, a skill is a simple object with an executable function. In Java, it could be an object that is an instance of a class implementing some Skill interface. Either way, there should be an explicit contract between the skill and the bot that states the callable logic that can be executed.<\/p>\n<p>There are different levels of complexities associated with skill resolution. This could be something like &#8220;What happens when no skill is found for a given topic?&#8221;, &#8220;How do I map skills according to the confidence in topic provided by the classifier?&#8221;, &#8220;What happens when a skill doesn&#8217;t respond at all?&#8221;, &#8220;How do we manage requested responses from skills?&#8221; etc.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 12 Dec 2016 15:40:34 +0000","created_by":1,"updated_at":"Mon, 12 Dec 2016 15:40:34 +0000","updated_by":1,"published_at":"","published_by":1},{"id":539,"title":"CodeEval: Fizz Buzz (Java)","slug":"codeeval-fizz-buzz-java","markdown":"\nFinally managed to get some spare time in order to do this. Helped be clear my head a bit. Here\u2019s my quick 1 minute solution to CodeEval\u2019s fizz buzz problem in Java:\n\n\/* Sample code to read in test cases:*\/ import java.io.*; public class Main { public static void main (String[] args) throws IOException { File file = new File(args[0]); BufferedReader buffer = new BufferedReader(new FileReader(file)); String line; while ((line = buffer.readLine()) != null) { line = line.trim(); final String[] inputStringArray = line.split(\" \"); final int divisor1 = Integer.parseInt(inputStringArray[0]); final int divisor2 = Integer.parseInt(inputStringArray[1]); final int countingLength = Integer.parseInt(inputStringArray[2]); for(int i = 1; i <= countingLength; i++) { final StringBuilder responseBuilder = new StringBuilder(); if(i % divisor1 == 0) responseBuilder.append(\"F\"); if(i % divisor2 == 0) responseBuilder.append(\"B\"); if(responseBuilder.length() == 0) responseBuilder.append(i); System.out.print(responseBuilder.toString() + \" \"); } System.out.println(); } } }\n\nUpon submitting, CodeEval marked my solution as unique (go figure!) which was a bit weird. However, I thought I could do better. So, here\u2019s a second approach using lambdas:\n\n\/* Sample code to read in test cases:*\/ import java.io.*; import java.util.Arrays; public class Main { public static void main (String[] args) throws IOException { final File file = new File(args[0]); final BufferedReader buffer = new BufferedReader(new FileReader(file)); String line; while ((line = buffer.readLine()) != null) { line = line.trim(); final int[] inputs = Arrays.stream(line.split(\" \")).mapToInt(Integer::parseInt).toArray(); final int divisor1 = inputs[0]; final int divisor2 = inputs[1]; final int countingLength = inputs[2]; for(int i = 1; i <= countingLength; i++) { final StringBuilder responseBuilder = new StringBuilder(); if(i % divisor1 == 0) responseBuilder.append(\"F\"); if(i % divisor2 == 0) responseBuilder.append(\"B\"); if(responseBuilder.length() == 0) responseBuilder.append(i); responseBuilder.append(\" \"); System.out.print(responseBuilder.toString()); } System.out.println(); } } }\n\nThis takes slightly longer (3-5 ms) and uses more memory.\n\n\n","html":"<p>Finally managed to get some spare time in order to do this. Helped be clear my head a bit. Here&#8217;s my quick 1 minute solution to CodeEval&#8217;s fizz buzz problem in Java:<\/p>\n<pre class=\"lang:java decode:true \">\/* Sample code to read in test cases:*\/\nimport java.io.*;\npublic class Main {\n    public static void main (String[] args) throws IOException {\n        File file = new File(args[0]);\n        BufferedReader buffer = new BufferedReader(new FileReader(file));\n        String line;\n        while ((line = buffer.readLine()) != null) {\n            line = line.trim();\n            final String[] inputStringArray = line.split(\" \");\n            final int divisor1 = Integer.parseInt(inputStringArray[0]);\n            final int divisor2 = Integer.parseInt(inputStringArray[1]);\n            final int countingLength = Integer.parseInt(inputStringArray[2]);\n            \n            for(int i = 1; i &lt;= countingLength; i++) {\n                final StringBuilder responseBuilder = new StringBuilder();\n                if(i % divisor1 == 0) responseBuilder.append(\"F\");\n                if(i % divisor2 == 0) responseBuilder.append(\"B\");\n                if(responseBuilder.length() == 0) responseBuilder.append(i);\n                System.out.print(responseBuilder.toString() + \" \");\n            }\n            \n            System.out.println();\n        }\n    }\n}<\/pre>\n<p><!--more-->Upon submitting, CodeEval marked my solution as unique (go figure!) which was a bit weird. However, I thought I could do better. So, here&#8217;s a second approach using lambdas:<\/p>\n<pre class=\"lang:java decode:true \">\/* Sample code to read in test cases:*\/\nimport java.io.*;\nimport java.util.Arrays;\n\npublic class Main {\n    public static void main (String[] args) throws IOException {\n        final File file = new File(args[0]);\n        final BufferedReader buffer = new BufferedReader(new FileReader(file));\n        String line;\n        while ((line = buffer.readLine()) != null) {\n            line = line.trim();\n            final int[] inputs = Arrays.stream(line.split(\" \")).mapToInt(Integer::parseInt).toArray();\n            final int divisor1 = inputs[0];\n            final int divisor2 = inputs[1];\n            final int countingLength = inputs[2];\n            \n            for(int i = 1; i &lt;= countingLength; i++) {\n                final StringBuilder responseBuilder = new StringBuilder();\n                if(i % divisor1 == 0) responseBuilder.append(\"F\");\n                if(i % divisor2 == 0) responseBuilder.append(\"B\");\n                if(responseBuilder.length() == 0) responseBuilder.append(i);\n                \n                responseBuilder.append(\" \");\n                System.out.print(responseBuilder.toString());\n            }\n            \n            System.out.println();\n        }\n    }\n}<\/pre>\n<p>This takes slightly longer (3-5 ms) and uses more memory.<\/p>\n","image":null,"featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 02 Jan 2017 08:32:16 +0000","created_by":1,"updated_at":"Mon, 02 Jan 2017 08:32:16 +0000","updated_by":1,"published_at":"Mon, 02 Jan 2017 08:32:16 +0000","published_by":1},{"id":542,"title":"CodeEval: Sum of first 1000 primes","slug":"temp-slug-101","markdown":"\nSum of first 1000 primes\n\n\/* Sample code to read in test cases:*\/ import java.io.*; public class Main { private static final int MAX_PRIME_COUNT = 1000; public static void main (String[] args) throws IOException { int primeSum = 2; int primeCount = 1; int number = 3; while(primeCount < MAX_PRIME_COUNT) { boolean prime = true; for(int i = 2; i < number; i++) { if(number % i == 0) prime = false; } if(prime){ primeSum += number; primeCount++; } number++; } System.out.println(primeSum + \"\"); } }\n\n\u00a0\n\n\u00a0\n\n\n","html":"<p>Sum of first 1000 primes<\/p>\n<pre class=\"lang:java decode:true\">\/* Sample code to read in test cases:*\/\nimport java.io.*;\npublic class Main {\n    private static final int MAX_PRIME_COUNT = 1000;\n    public static void main (String[] args) throws IOException {\n        int primeSum = 2;\n        int primeCount = 1;\n        int number = 3;\n        while(primeCount &lt; MAX_PRIME_COUNT) {\n            boolean prime = true;\n            for(int i = 2; i &lt; number; i++) {\n                if(number % i == 0) prime = false;\n            }\n            \n            if(prime){\n              primeSum += number;\n              primeCount++;\n            }\n            \n            number++;\n        }\n        System.out.println(primeSum + \"\");\n    }\n}\n<\/pre>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 02 Jan 2017 08:53:22 +0000","created_by":1,"updated_at":"Mon, 02 Jan 2017 08:53:22 +0000","updated_by":1,"published_at":"","published_by":1},{"id":544,"title":"Setting up corporate proxy on Docker for mac","slug":"setting-up-corporate-proxy-on-docker-for-mac","markdown":"\nWorking with Docker on corporate proxy is a painful experience. Mainly because there aren\u2019t many guides available to do it. Finally after banging my head on the desk for a long time, my friend and colleague at [https:\/\/nextmetaphor.io](https:\/\/nextmetaphor.io) showed me how to do it.\n\nFirst of all, fire up your terminal and open up docker tty in screen.\n\nscreen ~\/Library\/Containers\/com.docker.docker\/Data\/com.docker.driver.amd64-linux\/tty\n\nIf you see a blank screen, press enter. You should see a prompt.\n\nMake sure you are in the docker VM by typing the hostname command. You should see the response as moby. If your response is other than that, try that screen command again.\n\n\/ # hostname moby\n\nNow we want to view docker\u2019s routing table. This is because we\u2019ll need to find out the IP address of the host machine that is running the proxy. This is specific to my setup where I have a charles proxy server running on my machine which proxies to the remote corporate proxy.\n\n\/ # netstat -rn Kernel IP routing table Destination Gateway Genmask Flags MSS Window irtt Iface 0.0.0.0 192.168.65.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.65.0 0.0.0.0 255.255.255.240 U 0 0 0 eth0\n\nGet the gateway entry for 0.0.0.0. In my case this is 192.168.65.1. That\u2019s the IP for the host machine running docker. For the proxy running on your local machine, just map it to the port. My charles server is running on port 8099 so my proxy will be:\n\nhttp:\/\/192.168.65.1:8099\n\nClose screen by pressing **Ctrl + a \\ **key. Once you\u2019ve exited, open up docker for mac preferences.\n\nGo to the \u201cadvanced\u201d tab and fill out your proxy settings.\n\nHit apply and restart when done!\n\nYour Docker for Mac should now work harmoniously with your proxy!\n\n\n","html":"<p>Working with Docker on corporate proxy is a painful experience. Mainly because there aren&#8217;t many guides available to do it. Finally after banging my head on the desk for a long time, my friend and colleague at <a href=\"https:\/\/nextmetaphor.io\">https:\/\/nextmetaphor.io<\/a> showed me how to do it.<\/p>\n<p>First of all, fire up your terminal and open up docker tty in screen.<\/p>\n<pre class=\"lang:sh decode:true\">screen ~\/Library\/Containers\/com.docker.docker\/Data\/com.docker.driver.amd64-linux\/tty<\/pre>\n<p>If you see a blank screen, press enter. You should see a prompt.<\/p>\n<p>Make sure you are in the docker VM by typing the hostname command. You should see the response as moby. If your response is other than that, try that screen command again.<\/p>\n<pre class=\"lang:sh decode:true\">\/ # hostname\nmoby<\/pre>\n<p>Now we want to view docker&#8217;s routing table. This is because we&#8217;ll need to find out the IP address of the host machine that is running the proxy. This is specific to my setup where I have a charles proxy server running on my machine which proxies to the remote corporate proxy.<\/p>\n<pre class=\"lang:sh decode:true\">\/ # netstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n0.0.0.0         192.168.65.1    0.0.0.0         UG        0 0          0 eth0\n172.17.0.0      0.0.0.0         255.255.0.0     U         0 0          0 docker0\n192.168.65.0    0.0.0.0         255.255.255.240 U         0 0          0 eth0<\/pre>\n<p>Get the gateway entry for 0.0.0.0. In my case this is 192.168.65.1. That&#8217;s the IP for the host machine running docker. For the proxy running on your local machine, just map it to the port. My charles server is running on port 8099 so my proxy will be:<\/p>\n<p>http:\/\/192.168.65.1:8099<\/p>\n<p>Close screen by pressing <strong>Ctrl + a \\ <\/strong>key. Once you&#8217;ve exited, open up docker for mac preferences.<\/p>\n<p>Go to the &#8220;advanced&#8221; tab and fill out your proxy settings.<\/p>\n<p>Hit apply and restart when done!<\/p>\n<p>Your Docker for Mac should now work harmoniously with your proxy!<\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2017\/01\/docker-turtles-banner-network-copy.jpg","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Thu, 19 Jan 2017 09:42:16 +0000","created_by":1,"updated_at":"Thu, 19 Jan 2017 09:42:46 +0000","updated_by":1,"published_at":"Thu, 19 Jan 2017 09:42:16 +0000","published_by":1},{"id":546,"title":"Generating random passwords from linux command line","slug":"generating-random-passwords-from-linux-command-line","markdown":"\nManaging production passwords\u00a0isn\u2019t a trivial task. I was trying to deploy a containerized app the other day that had a database deployed with it. During the deployment, I was trying to find an easy way to set a secure password. I didn\u2019t want anyone to know the password because I wanted only the application to know it and no one else. Also, the container was setup in a way that the database cannot be accessed from the outside world.\n\nSo instead of hard-coding the password, after doing some research, I used the following command:\n\n< \/dev\/urandom tr -dc _A-Z-a-z-0-9+= | head -c${1:-32};echo;\n\nThis might look like some gibberish but it uses linux\u2019s <span class=\"lang:default decode:true crayon-inline \">\/dev\/urandom<\/span>\u00a0to generate some randomness and extracts human readable characters like alphabets, numbers and common symbols like underscores, hyphens, pluses and equals.\n\nTry it! it works!\n\nNow, using clever bash interpolation syntax you can embed this password throughout your script in a secure way.\n\n\n","html":"<p>Managing production passwords\u00a0isn&#8217;t a trivial task. I was trying to deploy a containerized app the other day that had a database deployed with it. During the deployment, I was trying to find an easy way to set a secure password. I didn&#8217;t want anyone to know the password because I wanted only the application to know it and no one else. Also, the container was setup in a way that the database cannot be accessed from the outside world.<\/p>\n<p>So instead of hard-coding the password, after doing some research, I used the following command:<!--more--><\/p>\n<pre class=\"lang:sh decode:true\">&lt; \/dev\/urandom tr -dc _A-Z-a-z-0-9+= | head -c${1:-32};echo;<\/pre>\n<p>This might look like some gibberish but it uses linux&#8217;s <span class=\"lang:default decode:true crayon-inline \">\/dev\/urandom<\/span>\u00a0to generate some randomness and extracts human readable characters like alphabets, numbers and common symbols like underscores, hyphens, pluses and equals.<\/p>\n<p>Try it! it works!<\/p>\n<p>Now, using clever bash interpolation syntax you can embed this password throughout your script in a secure way.<\/p>\n","image":"https:\/\/www.manthanhd.com\/wp-content\/uploads\/2017\/01\/password-info-sec-banner.jpg","featured":0,"page":0,"status":"published","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Mon, 23 Jan 2017 16:29:08 +0000","created_by":1,"updated_at":"Mon, 23 Jan 2017 16:32:33 +0000","updated_by":1,"published_at":"Mon, 23 Jan 2017 16:29:08 +0000","published_by":1},{"id":551,"title":"Fixing docker service startup after using an alternative graph driver","slug":"temp-slug-104","markdown":"\nSo I was playing around with devicemapper docker storage driver the other day. It was quite nice but when I stopped the modified docker daemon and tried to start the docker service in \u201cnormal\u201d way, I received the following error:\n\nroot@carbon:~# service docker start  \n Job for docker.service failed because the control process exited with error code. See \u201csystemctl status docker.service\u201d and \u201cjournalctl -xe\u201d for details.\n\nClearly something had gone wrong. Upon running:\n\nsystemctl status docker.service\n\nI found the following line most helpful in fixing the error:\n\nJan 24 22:33:17 carbon dockerd[3753]: Error starting daemon: error initializing graphdriver: \/var\/lib\/docker contains several valid graphdrivers: devicemapper, aufs; Please cleanup or explicitly choose storage driver (-s\n\nSo I ran the following command in order to remove my devicemapper driver:\n\nrm -rf \/var\/lib\/docker\/devicemapper\n\nBoom it worked!\n\nAlternatively, I could\u2019ve backed it up to tmp instead by running:\n\nmv \/var\/lib\/docker\/devicemapper \/tmp\/\n\nBut I didn\u2019t need any of my images anyway so I removed it.\n\nHope this helps.\n\n\n","html":"<p>So I was playing around with devicemapper docker storage driver the other day. It was quite nice but when I stopped the modified docker daemon and tried to start the docker service in &#8220;normal&#8221; way, I received the following error:<\/p>\n<p>root@carbon:~# service docker start<br \/>\nJob for docker.service failed because the control process exited with error code. See &#8220;systemctl status docker.service&#8221; and &#8220;journalctl -xe&#8221; for details.<\/p>\n<p>Clearly something had gone wrong. Upon running:<\/p>\n<p>systemctl status docker.service<\/p>\n<p>I found the following line most helpful in fixing the error:<\/p>\n<p>Jan 24 22:33:17 carbon dockerd[3753]: Error starting daemon: error initializing graphdriver: \/var\/lib\/docker contains several valid graphdrivers: devicemapper, aufs; Please cleanup or explicitly choose storage driver (-s<\/p>\n<p>So I ran the following command in order to remove my devicemapper driver:<\/p>\n<p>rm -rf \/var\/lib\/docker\/devicemapper<\/p>\n<p>Boom it worked!<\/p>\n<p>Alternatively, I could&#8217;ve backed it up to tmp instead by running:<\/p>\n<p>mv \/var\/lib\/docker\/devicemapper \/tmp\/<\/p>\n<p>But I didn&#8217;t need any of my images anyway so I removed it.<\/p>\n<p>Hope this helps.<\/p>\n","image":null,"featured":0,"page":0,"status":"draft","language":"en_US","meta_title":null,"meta_description":null,"author_id":1,"created_at":"Tue, 24 Jan 2017 22:40:56 +0000","created_by":1,"updated_at":"Tue, 24 Jan 2017 22:40:56 +0000","updated_by":1,"published_at":"","published_by":1}],"tags":[{"id":58,"name":".net","slug":"net","description":""},{"id":85,"name":"agile","slug":"agile","description":""},{"id":82,"name":"android","slug":"android","description":""},{"id":92,"name":"Angular","slug":"angular","description":""},{"id":93,"name":"Angular Connect","slug":"angular-connect","description":""},{"id":97,"name":"api","slug":"api","description":""},{"id":117,"name":"appdynamics","slug":"appdynamics","description":""},{"id":52,"name":"apple","slug":"apple","description":""},{"id":55,"name":"arduino","slug":"arduino","description":""},{"id":57,"name":"asp","slug":"asp","description":""},{"id":112,"name":"auto scaling","slug":"auto-scaling","description":""},{"id":104,"name":"aws","slug":"aws","description":""},{"id":124,"name":"bash","slug":"bash","description":""},{"id":139,"name":"bot","slug":"bot","description":""},{"id":59,"name":"build system","slug":"build-system","description":""},{"id":56,"name":"c","slug":"c","description":""},{"id":138,"name":"chat","slug":"chat","description":""},{"id":46,"name":"chrome","slug":"chrome","description":""},{"id":122,"name":"ci","slug":"ci","description":""},{"id":105,"name":"cloud","slug":"cloud","description":""},{"id":134,"name":"cloud formation","slug":"cloud-formation","description":""},{"id":61,"name":"code efficiency","slug":"code-efficiency","description":""},{"id":102,"name":"community of practice","slug":"community-of-practice","description":""},{"id":96,"name":"conference","slug":"conference","description":""},{"id":133,"name":"containers","slug":"containers","description":""},{"id":125,"name":"coreos","slug":"coreos","description":""},{"id":60,"name":"database","slug":"database","description":""},{"id":126,"name":"deployment","slug":"deployment","description":""},{"id":84,"name":"dilbert","slug":"dilbert","description":""},{"id":116,"name":"docker","slug":"docker","description":""},{"id":111,"name":"ec2","slug":"ec2","description":""},{"id":62,"name":"eclipse","slug":"eclipse","description":""},{"id":113,"name":"elasticache","slug":"elasticache","description":""},{"id":137,"name":"expectjs","slug":"expectjs","description":""},{"id":89,"name":"express","slug":"express","description":""},{"id":47,"name":"firefox","slug":"firefox","description":""},{"id":119,"name":"fork","slug":"fork","description":""},{"id":141,"name":"framework","slug":"framework","description":""},{"id":65,"name":"google","slug":"google","description":""},{"id":121,"name":"gradle","slug":"gradle","description":""},{"id":99,"name":"gulp","slug":"gulp","description":""},{"id":114,"name":"iam","slug":"iam","description":""},{"id":66,"name":"ibm","slug":"ibm","description":""},{"id":68,"name":"java","slug":"java","description":""},{"id":135,"name":"javascript","slug":"javascript","description":""},{"id":110,"name":"kinesis","slug":"kinesis","description":""},{"id":106,"name":"lambda","slug":"lambda","description":""},{"id":49,"name":"linux","slug":"linux","description":""},{"id":76,"name":"linux.solaris","slug":"linux-solaris","description":""},{"id":54,"name":"mac","slug":"mac","description":""},{"id":101,"name":"machine learning","slug":"machine-learning","description":""},{"id":100,"name":"mahout","slug":"mahout","description":""},{"id":118,"name":"monitoring","slug":"monitoring","description":""},{"id":120,"name":"multer","slug":"multer","description":""},{"id":70,"name":"mysql","slug":"mysql","description":""},{"id":73,"name":"networking","slug":"networking","description":""},{"id":88,"name":"node","slug":"node","description":""},{"id":64,"name":"nosql","slug":"nosql","description":""},{"id":91,"name":"notes","slug":"notes","description":""},{"id":90,"name":"npm","slug":"npm","description":""},{"id":143,"name":"passwords","slug":"passwords","description":""},{"id":71,"name":"php","slug":"php","description":""},{"id":69,"name":"phpmyadmin","slug":"phpmyadmin","description":""},{"id":142,"name":"problem solving","slug":"problem-solving","description":""},{"id":127,"name":"production","slug":"production","description":""},{"id":123,"name":"proxy","slug":"proxy","description":""},{"id":72,"name":"raspberry pi","slug":"raspberry-pi","description":""},{"id":98,"name":"rest","slug":"rest","description":""},{"id":86,"name":"scrum","slug":"scrum","description":""},{"id":115,"name":"security","slug":"security","description":""},{"id":51,"name":"shell","slug":"shell","description":""},{"id":107,"name":"sns","slug":"sns","description":""},{"id":63,"name":"software engineering","slug":"software-engineering","description":""},{"id":75,"name":"solaris","slug":"solaris","description":""},{"id":108,"name":"sqs","slug":"sqs","description":""},{"id":78,"name":"steam","slug":"steam","description":""},{"id":109,"name":"swf","slug":"swf","description":""},{"id":140,"name":"talkify","slug":"talkify","description":""},{"id":95,"name":"TechCrunch Disrupt","slug":"techcrunch-disrupt","description":""},{"id":136,"name":"testing","slug":"testing","description":""},{"id":48,"name":"tomcat","slug":"tomcat","description":""},{"id":53,"name":"troubleshooting","slug":"troubleshooting","description":""},{"id":81,"name":"tutorial","slug":"tutorial","description":""},{"id":74,"name":"ubuntu","slug":"ubuntu","description":""},{"id":50,"name":"unix","slug":"unix","description":""},{"id":45,"name":"web browser","slug":"web-browser","description":""},{"id":77,"name":"windows","slug":"windows","description":""},{"id":128,"name":"wordpress","slug":"wordpress","description":""}],"posts_tags":[{"tag_id":61,"post_id":54},{"tag_id":58,"post_id":53},{"tag_id":56,"post_id":53},{"tag_id":58,"post_id":52},{"tag_id":56,"post_id":52},{"tag_id":60,"post_id":52},{"tag_id":58,"post_id":51},{"tag_id":57,"post_id":51},{"tag_id":56,"post_id":51},{"tag_id":58,"post_id":50},{"tag_id":57,"post_id":50},{"tag_id":56,"post_id":50},{"tag_id":68,"post_id":49},{"tag_id":58,"post_id":48},{"tag_id":56,"post_id":48},{"tag_id":46,"post_id":47},{"tag_id":47,"post_id":47},{"tag_id":45,"post_id":47},{"tag_id":58,"post_id":46},{"tag_id":56,"post_id":46},{"tag_id":49,"post_id":44},{"tag_id":51,"post_id":44},{"tag_id":49,"post_id":43},{"tag_id":51,"post_id":43},{"tag_id":68,"post_id":41},{"tag_id":68,"post_id":39},{"tag_id":68,"post_id":38},{"tag_id":82,"post_id":35},{"tag_id":68,"post_id":35},{"tag_id":68,"post_id":34},{"tag_id":68,"post_id":33},{"tag_id":81,"post_id":33},{"tag_id":49,"post_id":32},{"tag_id":51,"post_id":32},{"tag_id":56,"post_id":31},{"tag_id":70,"post_id":29},{"tag_id":71,"post_id":29},{"tag_id":69,"post_id":29},{"tag_id":59,"post_id":28},{"tag_id":56,"post_id":28},{"tag_id":49,"post_id":28},{"tag_id":76,"post_id":27},{"tag_id":51,"post_id":27},{"tag_id":50,"post_id":27},{"tag_id":68,"post_id":26},{"tag_id":81,"post_id":26},{"tag_id":75,"post_id":25},{"tag_id":50,"post_id":25},{"tag_id":49,"post_id":24},{"tag_id":51,"post_id":24},{"tag_id":50,"post_id":24},{"tag_id":66,"post_id":23},{"tag_id":53,"post_id":21},{"tag_id":77,"post_id":21},{"tag_id":49,"post_id":20},{"tag_id":51,"post_id":20},{"tag_id":49,"post_id":19},{"tag_id":51,"post_id":19},{"tag_id":55,"post_id":18},{"tag_id":56,"post_id":18},{"tag_id":49,"post_id":17},{"tag_id":51,"post_id":17},{"tag_id":65,"post_id":16},{"tag_id":53,"post_id":15},{"tag_id":49,"post_id":14},{"tag_id":73,"post_id":14},{"tag_id":72,"post_id":14},{"tag_id":49,"post_id":13},{"tag_id":74,"post_id":13},{"tag_id":53,"post_id":12},{"tag_id":77,"post_id":12},{"tag_id":64,"post_id":11},{"tag_id":63,"post_id":11},{"tag_id":52,"post_id":10},{"tag_id":54,"post_id":10},{"tag_id":53,"post_id":10},{"tag_id":78,"post_id":9},{"tag_id":53,"post_id":9},{"tag_id":77,"post_id":9},{"tag_id":62,"post_id":8},{"tag_id":53,"post_id":8},{"tag_id":49,"post_id":7},{"tag_id":51,"post_id":7},{"tag_id":48,"post_id":7},{"tag_id":50,"post_id":7},{"tag_id":51,"post_id":6},{"tag_id":68,"post_id":5},{"tag_id":73,"post_id":5},{"tag_id":49,"post_id":116},{"tag_id":51,"post_id":116},{"tag_id":74,"post_id":116},{"tag_id":50,"post_id":116},{"tag_id":85,"post_id":122},{"tag_id":86,"post_id":122},{"tag_id":49,"post_id":118},{"tag_id":51,"post_id":118},{"tag_id":50,"post_id":118},{"tag_id":85,"post_id":198},{"tag_id":84,"post_id":198},{"tag_id":49,"post_id":134},{"tag_id":51,"post_id":134},{"tag_id":50,"post_id":134},{"tag_id":49,"post_id":204},{"tag_id":51,"post_id":204},{"tag_id":50,"post_id":204},{"tag_id":68,"post_id":226},{"tag_id":89,"post_id":233},{"tag_id":88,"post_id":233},{"tag_id":90,"post_id":233},{"tag_id":85,"post_id":206},{"tag_id":86,"post_id":206},{"tag_id":89,"post_id":238},{"tag_id":88,"post_id":238},{"tag_id":89,"post_id":244},{"tag_id":88,"post_id":244},{"tag_id":92,"post_id":255},{"tag_id":93,"post_id":255},{"tag_id":91,"post_id":255},{"tag_id":92,"post_id":257},{"tag_id":93,"post_id":257},{"tag_id":91,"post_id":257},{"tag_id":92,"post_id":259},{"tag_id":93,"post_id":259},{"tag_id":91,"post_id":259},{"tag_id":92,"post_id":261},{"tag_id":93,"post_id":261},{"tag_id":91,"post_id":261},{"tag_id":92,"post_id":263},{"tag_id":93,"post_id":263},{"tag_id":91,"post_id":263},{"tag_id":92,"post_id":266},{"tag_id":93,"post_id":266},{"tag_id":91,"post_id":266},{"tag_id":92,"post_id":268},{"tag_id":93,"post_id":268},{"tag_id":91,"post_id":268},{"tag_id":96,"post_id":272},{"tag_id":91,"post_id":272},{"tag_id":95,"post_id":272},{"tag_id":97,"post_id":280},{"tag_id":98,"post_id":280},{"tag_id":49,"post_id":301},{"tag_id":48,"post_id":301},{"tag_id":50,"post_id":301},{"tag_id":99,"post_id":306},{"tag_id":88,"post_id":306},{"tag_id":90,"post_id":306},{"tag_id":89,"post_id":310},{"tag_id":88,"post_id":310},{"tag_id":68,"post_id":313},{"tag_id":101,"post_id":313},{"tag_id":100,"post_id":313},{"tag_id":102,"post_id":326},{"tag_id":63,"post_id":326},{"tag_id":104,"post_id":336},{"tag_id":105,"post_id":336},{"tag_id":91,"post_id":336},{"tag_id":104,"post_id":340},{"tag_id":105,"post_id":340},{"tag_id":110,"post_id":340},{"tag_id":106,"post_id":340},{"tag_id":91,"post_id":340},{"tag_id":107,"post_id":340},{"tag_id":108,"post_id":340},{"tag_id":109,"post_id":340},{"tag_id":112,"post_id":346},{"tag_id":104,"post_id":346},{"tag_id":105,"post_id":346},{"tag_id":111,"post_id":346},{"tag_id":113,"post_id":346},{"tag_id":114,"post_id":346},{"tag_id":91,"post_id":346},{"tag_id":115,"post_id":346},{"tag_id":116,"post_id":350},{"tag_id":49,"post_id":350},{"tag_id":50,"post_id":350},{"tag_id":68,"post_id":349},{"tag_id":49,"post_id":349},{"tag_id":48,"post_id":349},{"tag_id":50,"post_id":349},{"tag_id":117,"post_id":359},{"tag_id":118,"post_id":359},{"tag_id":91,"post_id":359},{"tag_id":117,"post_id":361},{"tag_id":118,"post_id":361},{"tag_id":91,"post_id":361},{"tag_id":117,"post_id":364},{"tag_id":118,"post_id":364},{"tag_id":91,"post_id":364},{"tag_id":119,"post_id":367},{"tag_id":88,"post_id":367},{"tag_id":90,"post_id":367},{"tag_id":49,"post_id":376},{"tag_id":50,"post_id":376},{"tag_id":89,"post_id":380},{"tag_id":120,"post_id":380},{"tag_id":88,"post_id":380},{"tag_id":90,"post_id":380},{"tag_id":122,"post_id":386},{"tag_id":61,"post_id":386},{"tag_id":121,"post_id":386},{"tag_id":68,"post_id":386},{"tag_id":116,"post_id":398},{"tag_id":123,"post_id":398},{"tag_id":124,"post_id":402},{"tag_id":49,"post_id":402},{"tag_id":51,"post_id":402},{"tag_id":50,"post_id":402},{"tag_id":116,"post_id":404},{"tag_id":49,"post_id":404},{"tag_id":51,"post_id":404},{"tag_id":50,"post_id":404},{"tag_id":124,"post_id":426},{"tag_id":49,"post_id":426},{"tag_id":51,"post_id":426},{"tag_id":50,"post_id":426},{"tag_id":125,"post_id":430},{"tag_id":126,"post_id":430},{"tag_id":116,"post_id":430},{"tag_id":70,"post_id":430},{"tag_id":127,"post_id":430},{"tag_id":128,"post_id":430},{"tag_id":130,"post_id":439},{"tag_id":132,"post_id":439},{"tag_id":131,"post_id":439},{"tag_id":129,"post_id":439},{"tag_id":133,"post_id":441},{"tag_id":126,"post_id":441},{"tag_id":116,"post_id":441},{"tag_id":49,"post_id":441},{"tag_id":104,"post_id":465},{"tag_id":105,"post_id":465},{"tag_id":134,"post_id":465},{"tag_id":114,"post_id":465},{"tag_id":137,"post_id":492},{"tag_id":135,"post_id":492},{"tag_id":88,"post_id":492},{"tag_id":136,"post_id":492},{"tag_id":139,"post_id":496},{"tag_id":138,"post_id":496},{"tag_id":141,"post_id":496},{"tag_id":88,"post_id":496},{"tag_id":90,"post_id":496},{"tag_id":140,"post_id":496},{"tag_id":121,"post_id":509},{"tag_id":68,"post_id":509},{"tag_id":104,"post_id":517},{"tag_id":105,"post_id":517},{"tag_id":134,"post_id":517},{"tag_id":126,"post_id":517},{"tag_id":68,"post_id":517},{"tag_id":139,"post_id":528},{"tag_id":138,"post_id":528},{"tag_id":68,"post_id":539},{"tag_id":142,"post_id":539},{"tag_id":68,"post_id":542},{"tag_id":142,"post_id":542},{"tag_id":116,"post_id":544},{"tag_id":124,"post_id":546},{"tag_id":49,"post_id":546},{"tag_id":143,"post_id":546},{"tag_id":115,"post_id":546},{"tag_id":50,"post_id":546}],"users":[{"id":1,"slug":"manthanhd","bio":"I like awesome things. \r\nCurrently transforming the world bit by bit. \r\nI'm a Software Engineer.\r\n(All views my own)","website":"https:\/\/www.manthanhd.com","created_at":"Mon, 24 Aug 2015 18:05:26 +0000","created_by":1,"email":"manthanhd@live.com","name":"manthanhd"}]},"meta":{"exported_on":"Fri, 10 Feb 2017 17:27:20 +0000","version":"000"}}